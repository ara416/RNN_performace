{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate\n",
    "import control as ctrl\n",
    "\n",
    "\n",
    "def vdp1(t, x):\n",
    "    return np.array([-x[1], - .3*x[1] + x[0]])\n",
    "\n",
    "res=500\n",
    "seq_len=20\n",
    "t0, t1 = 0, 20                # start and end\n",
    "t = np.linspace(t0, t1, res+seq_len)  # the points of evaluation of solution\n",
    "x0 = [2, 0]                   # initial value\n",
    "x = np.zeros((len(t), len(x0)))   # arrax for solution\n",
    "x[0, :] = x0\n",
    "r = integrate.ode(vdp1).set_integrator(\"dopri5\")  # choice of method\n",
    "r.set_initial_value(x0, t0)   # initial values\n",
    "for i in range(1, t.size):\n",
    "    x[i, :] = r.integrate(t[i])+np.random.normal(0,0.03,(2,)) # get one more value, add it to the arrax\n",
    "    if not r.successful():\n",
    "        raise RuntimeError(\"Could not integrate\")\n",
    "plt.plot(t, x)\n",
    "plt.show()\n",
    "\n",
    "def createInputs(x,res,seq_len):\n",
    "    data_list=[]\n",
    "#     target_list=[]\n",
    "    for i in range(res-1):\n",
    "        data_list.append([x[i:seq_len+i,:],x[seq_len+i+1,:]])\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "\n",
    "# ================================\n",
    "\n",
    "class RNN:\n",
    "  # A many-to-one Vanilla Recurrent Neural Network.\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights\n",
    "        self.Whh = random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wxh = random.randn(hidden_size, input_size) / 1000\n",
    "        self.Why = random.randn(output_size, hidden_size) / 1000\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "    \n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = { 0: h }\n",
    "\n",
    "        # Perform each step of the RNN\n",
    "        for i, xin in enumerate(inputs):\n",
    "            xin=xin[...,np.newaxis]\n",
    "            h = (self.Wxh @ xin + np.tanh(self.Whh @ h) + self.bh)\n",
    "            self.last_hs[i + 1] = h\n",
    "\n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "\n",
    "    def backprop(self, d_y,epoch,g0, learn_rate=2e-3):\n",
    "        n = len(self.last_inputs)\n",
    "\n",
    "        # Calculate dL/dWhy and dL/dby.\n",
    "        d_Why = d_y @ self.last_hs[n].T\n",
    "        d_by = d_y\n",
    "\n",
    "        # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n",
    "        d_Whh = np.zeros(self.Whh.shape)\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "\n",
    "        # Calculate dL/dh for the last h.\n",
    "        # dL/dh = dL/dy * dy/dh\n",
    "        d_h = self.Why.T @ d_y\n",
    "\n",
    "        # Backpropagate through time.\n",
    "        for t in reversed(range(n)):\n",
    "            # An intermediate value: dL/dh * (1 - h^2)\n",
    "            temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n",
    "#             temp = d_h\n",
    "            # dL/db = dL/dh * (1 - h^2)\n",
    "            d_bh += d_h\n",
    "            # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
    "            d_Whh += d_h @ np.tanh(self.last_hs[t].T)\n",
    "\n",
    "            # dL/dWxh = dL/dh * (1 - h^2) * x\n",
    "            last_x=self.last_inputs[t]\n",
    "            last_x=last_x[...,np.newaxis]\n",
    "            d_Wxh += d_h @ last_x.T\n",
    "\n",
    "            # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "            d_h = self.Whh @ temp\n",
    "\n",
    "        # Clip to prevent exploding gradients.\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # Update weights and biases using gradient descent.\n",
    "        # adding the regulization terms -----------------------------------------\n",
    "        dA,dB,dC=self.Rbstdiff()\n",
    "#         gamma=g0\n",
    "#         gamma=g0/np.log(epoch/10+2)\n",
    "        gamma=g0/((epoch/20)+1)\n",
    "        learn_rate2=learn_rate*gamma\n",
    "#         learn_rate=learn_rate*gamma\n",
    "        self.Whh -= learn_rate * d_Whh+learn_rate2 *dA\n",
    "        self.Wxh -= learn_rate * d_Wxh+learn_rate2 *dB\n",
    "        self.Why -= learn_rate * d_Why+learn_rate2 *dC\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by\n",
    "    def Weights(self):\n",
    "        return self.Whh,self.Wxh,self.Why,self.bh,self.by\n",
    "    \n",
    "    def Rbstdiff(self):\n",
    "        A,B,C,b,c=self.Weights()\n",
    "        At=np.transpose(A)\n",
    "        Q=np.transpose(C)@C\n",
    "        Bt=np.transpose(B)\n",
    "        P=ctrl.dlyap(A,Q)\n",
    "        I=np.identity(len(A))\n",
    "        dB=2*P@B\n",
    "        dC=-2*C@B@Bt@ np.linalg.pinv(I-At@A)\n",
    "        dA=2*B@Bt@P@A @np.linalg.pinv(I-At@A)\n",
    "        return dA,dB,dC\n",
    "\n",
    "    def Prediction(self,x,sig,res,seq_len):\n",
    "        y_pre=np.zeros([res-1,2])\n",
    "        i=0\n",
    "        x_n=x+np.random.normal(0,sig,x.shape)\n",
    "        X=createInputs(x_n,res,seq_len)\n",
    "        for xd in X:\n",
    "            inputs = xd[0]\n",
    "            target = xd[1].reshape(-1,1)\n",
    "            out, _ = rnn.forward(inputs)\n",
    "            y_pre[i]=out.reshape(-1,2)\n",
    "            i+=1\n",
    "        return y_pre\n",
    "\n",
    "    def processData(self,x, backprop=True,batch_size=20,res=500,seq_len=20,g0=.1):\n",
    "        X_train=createInputs(x,res,seq_len)\n",
    "        loss = 0\n",
    "        d_L_d_y=np.zeros([2,1])\n",
    "        random.shuffle(X_train)\n",
    "        for xd in X_train[0:batch_size]:\n",
    "            inputs = xd[0]\n",
    "            target = xd[1].reshape(-1,1)\n",
    "\n",
    "            out, _ = self.forward(inputs)\n",
    "            diff=(out - target)\n",
    "            loss += (diff**2).mean(axis=0)\n",
    "\n",
    "            if backprop:\n",
    "                d_L_d_y += 2*(diff)\n",
    "                self.backprop(d_L_d_y,epoch+1,g0)\n",
    "        return loss/batch_size\n",
    "    def Perf_bound(self):\n",
    "        A,B,C,b,c=self.Weights()\n",
    "        At=np.transpose(A)\n",
    "        Q=np.transpose(C)@C\n",
    "        Bt=np.transpose(B)\n",
    "        P=ctrl.dlyap(A,Q)\n",
    "        return np.trace(Bt@P@B)\n",
    "\n",
    "\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0014044433452161933\n",
      "Train:\t Perf B \t 0.002642869288836263\n",
      "Train:\t Perf I \t 0.0013055917824429234\n",
      "Train:\t L2 norm of A \t  0.35862507312707065\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0015854690722182532\n",
      "Train:\t Perf B \t 0.002599225132881582\n",
      "Train:\t Perf I \t 0.0012676616088079366\n",
      "Train:\t L2 norm of A \t  0.34572525739330967\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.001917362614817562\n",
      "Train:\t Perf B \t 0.0025584386406660876\n",
      "Train:\t Perf I \t 0.0011895139402402668\n",
      "Train:\t L2 norm of A \t  0.3436404524339006\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0014999281275885285\n",
      "Train:\t Perf B \t 0.0025370871921107834\n",
      "Train:\t Perf I \t 0.0012433125635375067\n",
      "Train:\t L2 norm of A \t  0.336829264707494\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0014002055854050635\n",
      "Train:\t Perf B \t 0.002486868425622239\n",
      "Train:\t Perf I \t 0.001284635734360176\n",
      "Train:\t L2 norm of A \t  0.3632166088472302\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0015355893425586032\n",
      "Train:\t Perf B \t 0.002558640755396972\n",
      "Train:\t Perf I \t 0.0012161690912836523\n",
      "Train:\t L2 norm of A \t  0.34503903165228406\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0015258427471800007\n",
      "Train:\t Perf B \t 0.0023499873821706783\n",
      "Train:\t Perf I \t 0.0011773302231021761\n",
      "Train:\t L2 norm of A \t  0.40076713716501705\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0015950473485935375\n",
      "Train:\t Perf B \t 0.0025018977163502785\n",
      "Train:\t Perf I \t 0.0012483771972919664\n",
      "Train:\t L2 norm of A \t  0.3544776878970329\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.001955655665125636\n",
      "Train:\t Perf B \t 0.0025886747883460044\n",
      "Train:\t Perf I \t 0.0012441024621870223\n",
      "Train:\t L2 norm of A \t  0.3459756267825857\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0014557570213240467\n",
      "Train:\t Perf B \t 0.002568999145412419\n",
      "Train:\t Perf I \t 0.0013723791102337156\n",
      "Train:\t L2 norm of A \t  0.3476281187942572\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.001773135083179235\n",
      "Train:\t Perf B \t 0.0024667286479708624\n",
      "Train:\t Perf I \t 0.0012244264680725445\n",
      "Train:\t L2 norm of A \t  0.35176297375135607\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0014486483230263815\n",
      "Train:\t Perf B \t 0.0025963280239831143\n",
      "Train:\t Perf I \t 0.001271673484945979\n",
      "Train:\t L2 norm of A \t  0.3320110139259607\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0016758472696184123\n",
      "Train:\t Perf B \t 0.0025986359287315615\n",
      "Train:\t Perf I \t 0.0012971824781086512\n",
      "Train:\t L2 norm of A \t  0.3179749906664567\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0017477560415626728\n",
      "Train:\t Perf B \t 0.0025240205107255895\n",
      "Train:\t Perf I \t 0.0013083947115428093\n",
      "Train:\t L2 norm of A \t  0.364147087869417\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0017803324527030737\n",
      "Train:\t Perf B \t 0.0026190116523230117\n",
      "Train:\t Perf I \t 0.001303965390646668\n",
      "Train:\t L2 norm of A \t  0.3230131505546051\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0020966484799505017\n",
      "Train:\t Perf B \t 0.0025157491962179597\n",
      "Train:\t Perf I \t 0.0011726892179060627\n",
      "Train:\t L2 norm of A \t  0.3687819850984488\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0014593898355399094\n",
      "Train:\t Perf B \t 0.0024811428390528983\n",
      "Train:\t Perf I \t 0.0012074891077057883\n",
      "Train:\t L2 norm of A \t  0.34991111005745645\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0016885383738821835\n",
      "Train:\t Perf B \t 0.0025788795565443038\n",
      "Train:\t Perf I \t 0.001349388535939775\n",
      "Train:\t L2 norm of A \t  0.34520927638828725\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0017257683470752528\n",
      "Train:\t Perf B \t 0.0025616455082703186\n",
      "Train:\t Perf I \t 0.0012537299122522088\n",
      "Train:\t L2 norm of A \t  0.348928260118557\n",
      "-------------- The gamma 0.000000\n",
      "Train:\tLoss \t  0.0014142633860761468\n",
      "Train:\t Perf B \t 0.0025258801560683413\n",
      "Train:\t Perf I \t 0.001294946836849857\n",
      "Train:\t L2 norm of A \t  0.34784967421567636\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0018084947770435993\n",
      "Train:\t Perf B \t 0.002502613909802025\n",
      "Train:\t Perf I \t 0.0011894099077402076\n",
      "Train:\t L2 norm of A \t  0.3358621357432374\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0013175035995696938\n",
      "Train:\t Perf B \t 0.0025353710425510137\n",
      "Train:\t Perf I \t 0.0012927892405506613\n",
      "Train:\t L2 norm of A \t  0.3509020773958602\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.001488285710445696\n",
      "Train:\t Perf B \t 0.0025586804572129557\n",
      "Train:\t Perf I \t 0.0011122299328357752\n",
      "Train:\t L2 norm of A \t  0.35414441567476657\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.001311792757622091\n",
      "Train:\t Perf B \t 0.0025183337182218784\n",
      "Train:\t Perf I \t 0.0013185070589526527\n",
      "Train:\t L2 norm of A \t  0.34944595846376253\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0015092117022591199\n",
      "Train:\t Perf B \t 0.002467502477597445\n",
      "Train:\t Perf I \t 0.0012346787308956777\n",
      "Train:\t L2 norm of A \t  0.3608050990665753\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.001421158426107826\n",
      "Train:\t Perf B \t 0.002658250503965348\n",
      "Train:\t Perf I \t 0.0012765646044683554\n",
      "Train:\t L2 norm of A \t  0.3044913876333084\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0019205596021413553\n",
      "Train:\t Perf B \t 0.0025093109036766377\n",
      "Train:\t Perf I \t 0.001270993939805387\n",
      "Train:\t L2 norm of A \t  0.3490175198380266\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.002183394219226405\n",
      "Train:\t Perf B \t 0.002620743688327792\n",
      "Train:\t Perf I \t 0.001291098931646254\n",
      "Train:\t L2 norm of A \t  0.343584752160955\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0017140806901266438\n",
      "Train:\t Perf B \t 0.0026210697512682613\n",
      "Train:\t Perf I \t 0.0013592549834196626\n",
      "Train:\t L2 norm of A \t  0.35094590878886506\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0013167421842233522\n",
      "Train:\t Perf B \t 0.00247785936004094\n",
      "Train:\t Perf I \t 0.0012963359994173649\n",
      "Train:\t L2 norm of A \t  0.3445141789763901\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0013752744289232703\n",
      "Train:\t Perf B \t 0.002589292762274802\n",
      "Train:\t Perf I \t 0.0013032545035206054\n",
      "Train:\t L2 norm of A \t  0.33415435224283646\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0016122854315883406\n",
      "Train:\t Perf B \t 0.0025015260086827234\n",
      "Train:\t Perf I \t 0.0012097443482109893\n",
      "Train:\t L2 norm of A \t  0.3304650387067645\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0014003381504763001\n",
      "Train:\t Perf B \t 0.002546941350888328\n",
      "Train:\t Perf I \t 0.001301571656049959\n",
      "Train:\t L2 norm of A \t  0.35200562001957936\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.00150281424331934\n",
      "Train:\t Perf B \t 0.0024115199159561074\n",
      "Train:\t Perf I \t 0.0012487394717258845\n",
      "Train:\t L2 norm of A \t  0.3595414754981264\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.001437279050327381\n",
      "Train:\t Perf B \t 0.002650415798330964\n",
      "Train:\t Perf I \t 0.001308470950203753\n",
      "Train:\t L2 norm of A \t  0.33448230341835217\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0016553478081194296\n",
      "Train:\t Perf B \t 0.0024257517883339006\n",
      "Train:\t Perf I \t 0.001123875903202668\n",
      "Train:\t L2 norm of A \t  0.3526576558008007\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0015057635055762382\n",
      "Train:\t Perf B \t 0.002552783913356753\n",
      "Train:\t Perf I \t 0.001292805874781308\n",
      "Train:\t L2 norm of A \t  0.34819353102745937\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0014910113519554348\n",
      "Train:\t Perf B \t 0.0024617103795119243\n",
      "Train:\t Perf I \t 0.0012773741665281094\n",
      "Train:\t L2 norm of A \t  0.3467600672449136\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0017319683433751158\n",
      "Train:\t Perf B \t 0.0025932296664939593\n",
      "Train:\t Perf I \t 0.001300371642772286\n",
      "Train:\t L2 norm of A \t  0.3397519675902502\n",
      "-------------- The gamma 0.010000\n",
      "Train:\tLoss \t  0.0013193919760000302\n",
      "Train:\t Perf B \t 0.002489563971434444\n",
      "Train:\t Perf I \t 0.0011656795337433167\n",
      "Train:\t L2 norm of A \t  0.34699273124198116\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0014351336492299783\n",
      "Train:\t Perf B \t 0.0026437124396164417\n",
      "Train:\t Perf I \t 0.0013016649603304765\n",
      "Train:\t L2 norm of A \t  0.3360857323044993\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0016120895036341482\n",
      "Train:\t Perf B \t 0.0024929878143159093\n",
      "Train:\t Perf I \t 0.0012429192290357145\n",
      "Train:\t L2 norm of A \t  0.34599123355833683\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.001397684940228005\n",
      "Train:\t Perf B \t 0.002466976868066446\n",
      "Train:\t Perf I \t 0.0012972275382015185\n",
      "Train:\t L2 norm of A \t  0.3467419348224496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0018109146562191353\n",
      "Train:\t Perf B \t 0.002437926840531417\n",
      "Train:\t Perf I \t 0.001193187727780589\n",
      "Train:\t L2 norm of A \t  0.3401160392914732\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0013579043399955097\n",
      "Train:\t Perf B \t 0.002484116176756762\n",
      "Train:\t Perf I \t 0.0012080294719235367\n",
      "Train:\t L2 norm of A \t  0.35371761057600903\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0016646031368262057\n",
      "Train:\t Perf B \t 0.0025465039668876813\n",
      "Train:\t Perf I \t 0.0011527642276042457\n",
      "Train:\t L2 norm of A \t  0.3337289397586237\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0013758083140987037\n",
      "Train:\t Perf B \t 0.002592006048244236\n",
      "Train:\t Perf I \t 0.0014743448581668907\n",
      "Train:\t L2 norm of A \t  0.32587654981710995\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0018186178739887419\n",
      "Train:\t Perf B \t 0.0024693256300388367\n",
      "Train:\t Perf I \t 0.0013107958911896824\n",
      "Train:\t L2 norm of A \t  0.345098546643842\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0014839021536237047\n",
      "Train:\t Perf B \t 0.002601559983497253\n",
      "Train:\t Perf I \t 0.0012049896289799077\n",
      "Train:\t L2 norm of A \t  0.3436764417956708\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.001347407017222617\n",
      "Train:\t Perf B \t 0.0024905978353386097\n",
      "Train:\t Perf I \t 0.0012573158349743082\n",
      "Train:\t L2 norm of A \t  0.34422602542819586\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0013547610567332935\n",
      "Train:\t Perf B \t 0.002534934225566265\n",
      "Train:\t Perf I \t 0.0012340296212583293\n",
      "Train:\t L2 norm of A \t  0.33540281221184554\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.001543556944109026\n",
      "Train:\t Perf B \t 0.0024981785059318765\n",
      "Train:\t Perf I \t 0.0011294990923690568\n",
      "Train:\t L2 norm of A \t  0.32964250339011064\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.001954276336973268\n",
      "Train:\t Perf B \t 0.0025171244482781823\n",
      "Train:\t Perf I \t 0.0011791330014563409\n",
      "Train:\t L2 norm of A \t  0.3520927009552449\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0014706378089120588\n",
      "Train:\t Perf B \t 0.0025544304730210437\n",
      "Train:\t Perf I \t 0.0012536535479856425\n",
      "Train:\t L2 norm of A \t  0.3357323101359731\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.001997979174479331\n",
      "Train:\t Perf B \t 0.002528548477401231\n",
      "Train:\t Perf I \t 0.0011466249555752471\n",
      "Train:\t L2 norm of A \t  0.36321468354323494\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0016310059758614126\n",
      "Train:\t Perf B \t 0.002586429247559582\n",
      "Train:\t Perf I \t 0.00136238103805739\n",
      "Train:\t L2 norm of A \t  0.35476749054239065\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0013764767608352743\n",
      "Train:\t Perf B \t 0.0025455819503133466\n",
      "Train:\t Perf I \t 0.0012495949297869598\n",
      "Train:\t L2 norm of A \t  0.34969574429381844\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0015511175982363748\n",
      "Train:\t Perf B \t 0.002520557285020016\n",
      "Train:\t Perf I \t 0.001338919498985835\n",
      "Train:\t L2 norm of A \t  0.36174099625077627\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0013882401826123395\n",
      "Train:\t Perf B \t 0.002598815863440829\n",
      "Train:\t Perf I \t 0.0012921553547839129\n",
      "Train:\t L2 norm of A \t  0.35052826646040103\n",
      "-------------- The gamma 0.020000\n",
      "Train:\tLoss \t  0.0015212112921753893\n",
      "Train:\t Perf B \t 0.0025935186027974465\n",
      "Train:\t Perf I \t 0.0012926770465450075\n",
      "Train:\t L2 norm of A \t  0.36042346499246075\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.001340556415699495\n",
      "Train:\t Perf B \t 0.0024812045547171536\n",
      "Train:\t Perf I \t 0.001162140134930875\n",
      "Train:\t L2 norm of A \t  0.34744342433909187\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.0016382222106933024\n",
      "Train:\t Perf B \t 0.0025654343238231065\n",
      "Train:\t Perf I \t 0.0012895041373720305\n",
      "Train:\t L2 norm of A \t  0.34313139362913814\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.001656556587288044\n",
      "Train:\t Perf B \t 0.00256282482394856\n",
      "Train:\t Perf I \t 0.001222223693869576\n",
      "Train:\t L2 norm of A \t  0.34824994698640255\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.001915362432507267\n",
      "Train:\t Perf B \t 0.0024696128121511874\n",
      "Train:\t Perf I \t 0.0011982294987945836\n",
      "Train:\t L2 norm of A \t  0.3155686757324677\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.0015355882971300363\n",
      "Train:\t Perf B \t 0.002525878449568068\n",
      "Train:\t Perf I \t 0.0011825276947156466\n",
      "Train:\t L2 norm of A \t  0.33978127932986124\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.0015009964370771489\n",
      "Train:\t Perf B \t 0.00248158882919519\n",
      "Train:\t Perf I \t 0.0011881554023748452\n",
      "Train:\t L2 norm of A \t  0.3617426985698003\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.001447810068735644\n",
      "Train:\t Perf B \t 0.0025657332905012317\n",
      "Train:\t Perf I \t 0.001349270651868275\n",
      "Train:\t L2 norm of A \t  0.367448037298762\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.0015594341216359313\n",
      "Train:\t Perf B \t 0.0025285759608658677\n",
      "Train:\t Perf I \t 0.0011704069588509352\n",
      "Train:\t L2 norm of A \t  0.34936650942075637\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.001399616434897234\n",
      "Train:\t Perf B \t 0.002505768182973816\n",
      "Train:\t Perf I \t 0.0012774765568257399\n",
      "Train:\t L2 norm of A \t  0.36665377171710045\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.0014063712449325823\n",
      "Train:\t Perf B \t 0.0026216350900657576\n",
      "Train:\t Perf I \t 0.0012615912242227047\n",
      "Train:\t L2 norm of A \t  0.3267410317793539\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.0017052307684102383\n",
      "Train:\t Perf B \t 0.00244470018663864\n",
      "Train:\t Perf I \t 0.0011369189132720752\n",
      "Train:\t L2 norm of A \t  0.34899169396196855\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.001436544428768295\n",
      "Train:\t Perf B \t 0.0026233326122068216\n",
      "Train:\t Perf I \t 0.001306839981550656\n",
      "Train:\t L2 norm of A \t  0.34850933259654654\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.0015855445183134457\n",
      "Train:\t Perf B \t 0.0024516796105871316\n",
      "Train:\t Perf I \t 0.0011296028138610566\n",
      "Train:\t L2 norm of A \t  0.3474110147455749\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.0013405965992801688\n",
      "Train:\t Perf B \t 0.002490637073302474\n",
      "Train:\t Perf I \t 0.0011905336987954294\n",
      "Train:\t L2 norm of A \t  0.350671652059659\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.00268850744747821\n",
      "Train:\t Perf B \t 0.002682991094512021\n",
      "Train:\t Perf I \t 0.0013176808670535868\n",
      "Train:\t L2 norm of A \t  0.3495091174749254\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.0014217567793143038\n",
      "Train:\t Perf B \t 0.0024399115165779322\n",
      "Train:\t Perf I \t 0.0011903456539618363\n",
      "Train:\t L2 norm of A \t  0.3563143105064681\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.0019407522914994938\n",
      "Train:\t Perf B \t 0.0024518399844143434\n",
      "Train:\t Perf I \t 0.0010869199519228246\n",
      "Train:\t L2 norm of A \t  0.3595584638242228\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.001623427161717359\n",
      "Train:\t Perf B \t 0.0024787288503483086\n",
      "Train:\t Perf I \t 0.0012406639752813658\n",
      "Train:\t L2 norm of A \t  0.3389934560134515\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.001563015566496528\n",
      "Train:\t Perf B \t 0.0025734326667223007\n",
      "Train:\t Perf I \t 0.0013065206241153972\n",
      "Train:\t L2 norm of A \t  0.3404262807658525\n",
      "-------------- The gamma 0.030000\n",
      "Train:\tLoss \t  0.0014050386362760689\n",
      "Train:\t Perf B \t 0.0026381665369742797\n",
      "Train:\t Perf I \t 0.0013618139471216462\n",
      "Train:\t L2 norm of A \t  0.31520046289539105\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0016497554192575262\n",
      "Train:\t Perf B \t 0.0025449633541567767\n",
      "Train:\t Perf I \t 0.001226404051003962\n",
      "Train:\t L2 norm of A \t  0.3601106728476851\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0020772320777735095\n",
      "Train:\t Perf B \t 0.002558466878144638\n",
      "Train:\t Perf I \t 0.0011934588149474597\n",
      "Train:\t L2 norm of A \t  0.3426232142568157\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0015374668290032828\n",
      "Train:\t Perf B \t 0.0025768206403372616\n",
      "Train:\t Perf I \t 0.001166663609139947\n",
      "Train:\t L2 norm of A \t  0.3467907370163931\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0013348729845152733\n",
      "Train:\t Perf B \t 0.002496597397504468\n",
      "Train:\t Perf I \t 0.0013010931660096296\n",
      "Train:\t L2 norm of A \t  0.33141376413006174\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.00138465705796152\n",
      "Train:\t Perf B \t 0.002545078606529008\n",
      "Train:\t Perf I \t 0.0012746571143696108\n",
      "Train:\t L2 norm of A \t  0.3515190415233639\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0015600302405050055\n",
      "Train:\t Perf B \t 0.0025501986268319974\n",
      "Train:\t Perf I \t 0.0011568981254207972\n",
      "Train:\t L2 norm of A \t  0.34349973496816166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0013550564699814176\n",
      "Train:\t Perf B \t 0.002485786443179446\n",
      "Train:\t Perf I \t 0.0012293544254234196\n",
      "Train:\t L2 norm of A \t  0.34721303133246406\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.001647116744439617\n",
      "Train:\t Perf B \t 0.00249684709399672\n",
      "Train:\t Perf I \t 0.0011640221632334296\n",
      "Train:\t L2 norm of A \t  0.3543550263816786\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0016208111890627201\n",
      "Train:\t Perf B \t 0.0025122820933079796\n",
      "Train:\t Perf I \t 0.0012923081954850475\n",
      "Train:\t L2 norm of A \t  0.36662432953761076\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0015843035995478955\n",
      "Train:\t Perf B \t 0.0025429552399150526\n",
      "Train:\t Perf I \t 0.0011502516412761896\n",
      "Train:\t L2 norm of A \t  0.3588512667559147\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0015100874281284069\n",
      "Train:\t Perf B \t 0.002549337771691281\n",
      "Train:\t Perf I \t 0.001181492591827176\n",
      "Train:\t L2 norm of A \t  0.3478555040308951\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.001627479164347505\n",
      "Train:\t Perf B \t 0.0025849138652492094\n",
      "Train:\t Perf I \t 0.0014133102141430467\n",
      "Train:\t L2 norm of A \t  0.3647721167364276\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0017974640396268315\n",
      "Train:\t Perf B \t 0.0024241809610283565\n",
      "Train:\t Perf I \t 0.0011557941423139693\n",
      "Train:\t L2 norm of A \t  0.3373877025844456\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0013593436426464717\n",
      "Train:\t Perf B \t 0.0025429652765545955\n",
      "Train:\t Perf I \t 0.0013470555222124825\n",
      "Train:\t L2 norm of A \t  0.3417522069001348\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0013846431146327584\n",
      "Train:\t Perf B \t 0.0024615145050977593\n",
      "Train:\t Perf I \t 0.001250780382748108\n",
      "Train:\t L2 norm of A \t  0.3374232944868053\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0021313242518931166\n",
      "Train:\t Perf B \t 0.002536594493586788\n",
      "Train:\t Perf I \t 0.0012601325554365877\n",
      "Train:\t L2 norm of A \t  0.3471557448447733\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.002199129443898333\n",
      "Train:\t Perf B \t 0.002481363852906728\n",
      "Train:\t Perf I \t 0.0011690899736044374\n",
      "Train:\t L2 norm of A \t  0.3577377249244901\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.001629364823371345\n",
      "Train:\t Perf B \t 0.002487341491524932\n",
      "Train:\t Perf I \t 0.0011439522671237755\n",
      "Train:\t L2 norm of A \t  0.35028943038998495\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.001522872598597297\n",
      "Train:\t Perf B \t 0.0024730094625174796\n",
      "Train:\t Perf I \t 0.0012690002548352087\n",
      "Train:\t L2 norm of A \t  0.35549657710713645\n",
      "-------------- The gamma 0.040000\n",
      "Train:\tLoss \t  0.0014837612770522185\n",
      "Train:\t Perf B \t 0.0025675057072068537\n",
      "Train:\t Perf I \t 0.0011937661732347562\n",
      "Train:\t L2 norm of A \t  0.34360253424869464\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0016565302624203763\n",
      "Train:\t Perf B \t 0.00254679825309533\n",
      "Train:\t Perf I \t 0.0012443388939792526\n",
      "Train:\t L2 norm of A \t  0.34087591132452\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0023092752322954897\n",
      "Train:\t Perf B \t 0.0025233712105633783\n",
      "Train:\t Perf I \t 0.0013139470605144823\n",
      "Train:\t L2 norm of A \t  0.3246395335304936\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0018468629669903902\n",
      "Train:\t Perf B \t 0.0025672217119231226\n",
      "Train:\t Perf I \t 0.0011606182364520784\n",
      "Train:\t L2 norm of A \t  0.33782329094486757\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.002193381916857726\n",
      "Train:\t Perf B \t 0.0024963500776879937\n",
      "Train:\t Perf I \t 0.001307723217722106\n",
      "Train:\t L2 norm of A \t  0.33486194546616255\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0016809347572749897\n",
      "Train:\t Perf B \t 0.0025938013319255254\n",
      "Train:\t Perf I \t 0.0012643568440661084\n",
      "Train:\t L2 norm of A \t  0.3352464267746977\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0016453036434847565\n",
      "Train:\t Perf B \t 0.002491350202357449\n",
      "Train:\t Perf I \t 0.001259793538571942\n",
      "Train:\t L2 norm of A \t  0.3425793685928855\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0017739181937168054\n",
      "Train:\t Perf B \t 0.0025606397503550034\n",
      "Train:\t Perf I \t 0.0013430537596059982\n",
      "Train:\t L2 norm of A \t  0.3649132879054577\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0013317999534215707\n",
      "Train:\t Perf B \t 0.0025520373942231845\n",
      "Train:\t Perf I \t 0.0012432254957680554\n",
      "Train:\t L2 norm of A \t  0.3477905643154301\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0015223570343167621\n",
      "Train:\t Perf B \t 0.002547369420717507\n",
      "Train:\t Perf I \t 0.0013245822972214809\n",
      "Train:\t L2 norm of A \t  0.34926667580271936\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0016671229114078995\n",
      "Train:\t Perf B \t 0.0026488983155145223\n",
      "Train:\t Perf I \t 0.0012846872675833438\n",
      "Train:\t L2 norm of A \t  0.32862118743879726\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0013438605037223315\n",
      "Train:\t Perf B \t 0.0024558985773238066\n",
      "Train:\t Perf I \t 0.0013583473865642428\n",
      "Train:\t L2 norm of A \t  0.3464768103722683\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0013271140697944741\n",
      "Train:\t Perf B \t 0.002511771432948198\n",
      "Train:\t Perf I \t 0.0012515530086306612\n",
      "Train:\t L2 norm of A \t  0.3466528814771212\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.00154061565192499\n",
      "Train:\t Perf B \t 0.0025493653543988476\n",
      "Train:\t Perf I \t 0.0012652117684860467\n",
      "Train:\t L2 norm of A \t  0.3402755022841212\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0015249422395557167\n",
      "Train:\t Perf B \t 0.0025465816376465536\n",
      "Train:\t Perf I \t 0.0012851449382396613\n",
      "Train:\t L2 norm of A \t  0.3511333380574855\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0014703454839975396\n",
      "Train:\t Perf B \t 0.002620870002777123\n",
      "Train:\t Perf I \t 0.0012900851728622558\n",
      "Train:\t L2 norm of A \t  0.33748736274868235\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0017622752639441966\n",
      "Train:\t Perf B \t 0.0026544670680579197\n",
      "Train:\t Perf I \t 0.0013250124493356117\n",
      "Train:\t L2 norm of A \t  0.3487548169434561\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0020662538406863106\n",
      "Train:\t Perf B \t 0.002550823613457768\n",
      "Train:\t Perf I \t 0.0013546578089091664\n",
      "Train:\t L2 norm of A \t  0.3408995510732819\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0014867485917033248\n",
      "Train:\t Perf B \t 0.002618895042759594\n",
      "Train:\t Perf I \t 0.0012055301339869635\n",
      "Train:\t L2 norm of A \t  0.32178601680217056\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.001662742262713893\n",
      "Train:\t Perf B \t 0.002454927066998987\n",
      "Train:\t Perf I \t 0.0011588808415156672\n",
      "Train:\t L2 norm of A \t  0.3490186963341918\n",
      "-------------- The gamma 0.050000\n",
      "Train:\tLoss \t  0.0014551162137203622\n",
      "Train:\t Perf B \t 0.0025107885549930558\n",
      "Train:\t Perf I \t 0.0012087315175367383\n",
      "Train:\t L2 norm of A \t  0.3647272227648622\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0017206478353576603\n",
      "Train:\t Perf B \t 0.0024637431884129393\n",
      "Train:\t Perf I \t 0.0012634717633678027\n",
      "Train:\t L2 norm of A \t  0.3458643079264355\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0022992291976918155\n",
      "Train:\t Perf B \t 0.0025750096609304333\n",
      "Train:\t Perf I \t 0.001191640052168339\n",
      "Train:\t L2 norm of A \t  0.3587872254052495\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0015573424721328777\n",
      "Train:\t Perf B \t 0.0025294695234744408\n",
      "Train:\t Perf I \t 0.0013140657265788248\n",
      "Train:\t L2 norm of A \t  0.3413867311043664\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0014661122572821588\n",
      "Train:\t Perf B \t 0.0024338233338816765\n",
      "Train:\t Perf I \t 0.0012554641968843807\n",
      "Train:\t L2 norm of A \t  0.3582393011043027\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0019018054017642123\n",
      "Train:\t Perf B \t 0.002408146304641986\n",
      "Train:\t Perf I \t 0.0011854739941068479\n",
      "Train:\t L2 norm of A \t  0.34590859187885664\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0019021131653385807\n",
      "Train:\t Perf B \t 0.002551263737910035\n",
      "Train:\t Perf I \t 0.0012125118855270687\n",
      "Train:\t L2 norm of A \t  0.3413074086069652\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0016868661872743512\n",
      "Train:\t Perf B \t 0.002519506283242192\n",
      "Train:\t Perf I \t 0.0012633195360104662\n",
      "Train:\t L2 norm of A \t  0.3339008242702485\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.001535852698837284\n",
      "Train:\t Perf B \t 0.002440317087452543\n",
      "Train:\t Perf I \t 0.0012663646372781984\n",
      "Train:\t L2 norm of A \t  0.34813951519827857\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0024539940961678516\n",
      "Train:\t Perf B \t 0.0025611994898669475\n",
      "Train:\t Perf I \t 0.0011217537721460518\n",
      "Train:\t L2 norm of A \t  0.3697421546093279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.00214121812061298\n",
      "Train:\t Perf B \t 0.002516222061326836\n",
      "Train:\t Perf I \t 0.0012854308660800481\n",
      "Train:\t L2 norm of A \t  0.3469304532708465\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0014827717253539964\n",
      "Train:\t Perf B \t 0.0024623536044588207\n",
      "Train:\t Perf I \t 0.001212120475789695\n",
      "Train:\t L2 norm of A \t  0.3506546963561049\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0013210086697322445\n",
      "Train:\t Perf B \t 0.002535885393473454\n",
      "Train:\t Perf I \t 0.0013382825560325758\n",
      "Train:\t L2 norm of A \t  0.35883052257579756\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0014331813312519958\n",
      "Train:\t Perf B \t 0.0025478101673121526\n",
      "Train:\t Perf I \t 0.001222585695266197\n",
      "Train:\t L2 norm of A \t  0.3496264163238679\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.001617839766383807\n",
      "Train:\t Perf B \t 0.0026158620502523427\n",
      "Train:\t Perf I \t 0.0012761078658779746\n",
      "Train:\t L2 norm of A \t  0.3538639797580705\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0013351638569050752\n",
      "Train:\t Perf B \t 0.0024752813158582755\n",
      "Train:\t Perf I \t 0.0012598365649266424\n",
      "Train:\t L2 norm of A \t  0.35185691753913667\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0019854673932856136\n",
      "Train:\t Perf B \t 0.0025704351495101684\n",
      "Train:\t Perf I \t 0.0013776409384793963\n",
      "Train:\t L2 norm of A \t  0.3314690857020382\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.001961954764301153\n",
      "Train:\t Perf B \t 0.0025220084589573713\n",
      "Train:\t Perf I \t 0.0013639037885438527\n",
      "Train:\t L2 norm of A \t  0.3405700802051212\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0015564287305201644\n",
      "Train:\t Perf B \t 0.0025468817236967006\n",
      "Train:\t Perf I \t 0.001237564355468604\n",
      "Train:\t L2 norm of A \t  0.3341431423121864\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0013628132003576333\n",
      "Train:\t Perf B \t 0.002498681792172336\n",
      "Train:\t Perf I \t 0.0011188896388956483\n",
      "Train:\t L2 norm of A \t  0.3444409834284035\n",
      "-------------- The gamma 0.060000\n",
      "Train:\tLoss \t  0.0018861198732081177\n",
      "Train:\t Perf B \t 0.002570471412971247\n",
      "Train:\t Perf I \t 0.0013302322991284535\n",
      "Train:\t L2 norm of A \t  0.33431314146334207\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0017147568979335697\n",
      "Train:\t Perf B \t 0.002521556496808277\n",
      "Train:\t Perf I \t 0.001187303971846541\n",
      "Train:\t L2 norm of A \t  0.35185699462442355\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.001496569590935232\n",
      "Train:\t Perf B \t 0.002488913687203441\n",
      "Train:\t Perf I \t 0.0012689842697316783\n",
      "Train:\t L2 norm of A \t  0.35888813538038206\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0015459628444735188\n",
      "Train:\t Perf B \t 0.0025427360967510194\n",
      "Train:\t Perf I \t 0.0012834293287220786\n",
      "Train:\t L2 norm of A \t  0.3534240860485124\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0017858438899659387\n",
      "Train:\t Perf B \t 0.0024434540797382024\n",
      "Train:\t Perf I \t 0.0013286290930431767\n",
      "Train:\t L2 norm of A \t  0.36082852777968255\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.001782380131630782\n",
      "Train:\t Perf B \t 0.0024794433292571216\n",
      "Train:\t Perf I \t 0.001246863100049418\n",
      "Train:\t L2 norm of A \t  0.34749241089014477\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0018380768041659805\n",
      "Train:\t Perf B \t 0.002547730856602612\n",
      "Train:\t Perf I \t 0.001358955349088768\n",
      "Train:\t L2 norm of A \t  0.35985750225718194\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0017921619596884169\n",
      "Train:\t Perf B \t 0.0024547555615513686\n",
      "Train:\t Perf I \t 0.0012916027234188738\n",
      "Train:\t L2 norm of A \t  0.3531370610475129\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0014681127347611368\n",
      "Train:\t Perf B \t 0.0025019598132956893\n",
      "Train:\t Perf I \t 0.0011874200060585027\n",
      "Train:\t L2 norm of A \t  0.3408855440242742\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0014799840481089016\n",
      "Train:\t Perf B \t 0.002520531749747906\n",
      "Train:\t Perf I \t 0.0011927911981708958\n",
      "Train:\t L2 norm of A \t  0.35008289525054026\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0018028914577807673\n",
      "Train:\t Perf B \t 0.0025315468345452766\n",
      "Train:\t Perf I \t 0.0012343463073359454\n",
      "Train:\t L2 norm of A \t  0.36864601332688995\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0015129427816189902\n",
      "Train:\t Perf B \t 0.002458991758579366\n",
      "Train:\t Perf I \t 0.0012730247630496207\n",
      "Train:\t L2 norm of A \t  0.3563161513094132\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0014467128504278026\n",
      "Train:\t Perf B \t 0.002552431569455816\n",
      "Train:\t Perf I \t 0.001198606952208414\n",
      "Train:\t L2 norm of A \t  0.35817178473347516\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0018607612305735549\n",
      "Train:\t Perf B \t 0.0024972739748226336\n",
      "Train:\t Perf I \t 0.0011883003444358793\n",
      "Train:\t L2 norm of A \t  0.3515745758756841\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0016247400169237516\n",
      "Train:\t Perf B \t 0.0024810345072838707\n",
      "Train:\t Perf I \t 0.0012138730415904419\n",
      "Train:\t L2 norm of A \t  0.3488349781149971\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0014537756756282142\n",
      "Train:\t Perf B \t 0.002667835384672837\n",
      "Train:\t Perf I \t 0.0012966557783893528\n",
      "Train:\t L2 norm of A \t  0.3399904146446469\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0018324241059253909\n",
      "Train:\t Perf B \t 0.002488083096063567\n",
      "Train:\t Perf I \t 0.001337864458979589\n",
      "Train:\t L2 norm of A \t  0.35255537757589234\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0015146039615485003\n",
      "Train:\t Perf B \t 0.0025769288635537927\n",
      "Train:\t Perf I \t 0.0012910679196149303\n",
      "Train:\t L2 norm of A \t  0.33749491915730934\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0014313784137894925\n",
      "Train:\t Perf B \t 0.002518126544905062\n",
      "Train:\t Perf I \t 0.0012760760790754338\n",
      "Train:\t L2 norm of A \t  0.34656352010405\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.002259090757373498\n",
      "Train:\t Perf B \t 0.0025571528867422834\n",
      "Train:\t Perf I \t 0.0013520832877897185\n",
      "Train:\t L2 norm of A \t  0.35415793506122434\n",
      "-------------- The gamma 0.070000\n",
      "Train:\tLoss \t  0.0016759010396864076\n",
      "Train:\t Perf B \t 0.002425209528163319\n",
      "Train:\t Perf I \t 0.0011414975744809726\n",
      "Train:\t L2 norm of A \t  0.3446887318545272\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0015377418544016601\n",
      "Train:\t Perf B \t 0.002512443503389496\n",
      "Train:\t Perf I \t 0.0012020695332165038\n",
      "Train:\t L2 norm of A \t  0.3595180779156279\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0017465645562617738\n",
      "Train:\t Perf B \t 0.00241222737440451\n",
      "Train:\t Perf I \t 0.0011929870223272205\n",
      "Train:\t L2 norm of A \t  0.3708690961269715\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.00143039200178324\n",
      "Train:\t Perf B \t 0.0024637804566912122\n",
      "Train:\t Perf I \t 0.0013057117846612739\n",
      "Train:\t L2 norm of A \t  0.3516849998040726\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.001576500500144353\n",
      "Train:\t Perf B \t 0.002519611432955106\n",
      "Train:\t Perf I \t 0.0013192075353901523\n",
      "Train:\t L2 norm of A \t  0.3485621554009035\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.001460867961434591\n",
      "Train:\t Perf B \t 0.0024606313894094903\n",
      "Train:\t Perf I \t 0.001193717038884856\n",
      "Train:\t L2 norm of A \t  0.345568695737518\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0022684525081572223\n",
      "Train:\t Perf B \t 0.002364097710232544\n",
      "Train:\t Perf I \t 0.0012065431975153361\n",
      "Train:\t L2 norm of A \t  0.3360590839709059\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.001386619111384096\n",
      "Train:\t Perf B \t 0.0024717290098531667\n",
      "Train:\t Perf I \t 0.001318738113087631\n",
      "Train:\t L2 norm of A \t  0.349054654760113\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.001609062925926388\n",
      "Train:\t Perf B \t 0.002481383437363475\n",
      "Train:\t Perf I \t 0.0012922457382458974\n",
      "Train:\t L2 norm of A \t  0.3453158022732913\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0014999819147469746\n",
      "Train:\t Perf B \t 0.0024733053166361724\n",
      "Train:\t Perf I \t 0.0011910597855965429\n",
      "Train:\t L2 norm of A \t  0.3456333280174968\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.002048735297282679\n",
      "Train:\t Perf B \t 0.0024865307327886087\n",
      "Train:\t Perf I \t 0.0011729263504464214\n",
      "Train:\t L2 norm of A \t  0.3370441667938694\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0014376289673734117\n",
      "Train:\t Perf B \t 0.0024931492989406934\n",
      "Train:\t Perf I \t 0.0012254777026018283\n",
      "Train:\t L2 norm of A \t  0.35209079999932424\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0018327029608420512\n",
      "Train:\t Perf B \t 0.0024753397416336373\n",
      "Train:\t Perf I \t 0.0012427676481810218\n",
      "Train:\t L2 norm of A \t  0.3600296457845884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0013567318860479385\n",
      "Train:\t Perf B \t 0.0024323319204403765\n",
      "Train:\t Perf I \t 0.00112329649314925\n",
      "Train:\t L2 norm of A \t  0.3428051073784036\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0013380252597416954\n",
      "Train:\t Perf B \t 0.0025105018564315447\n",
      "Train:\t Perf I \t 0.0012154495085948864\n",
      "Train:\t L2 norm of A \t  0.3408010916431545\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0013317829107101155\n",
      "Train:\t Perf B \t 0.0025701775675135603\n",
      "Train:\t Perf I \t 0.0012687942580275515\n",
      "Train:\t L2 norm of A \t  0.34267059256909366\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0023499065360861474\n",
      "Train:\t Perf B \t 0.0025649713640720617\n",
      "Train:\t Perf I \t 0.0012376498651185113\n",
      "Train:\t L2 norm of A \t  0.33161870986954617\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0017758357305410616\n",
      "Train:\t Perf B \t 0.002433511377799874\n",
      "Train:\t Perf I \t 0.0012456166283058327\n",
      "Train:\t L2 norm of A \t  0.33327253266020596\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0013655127445750892\n",
      "Train:\t Perf B \t 0.0024582207076002433\n",
      "Train:\t Perf I \t 0.0011516704757663714\n",
      "Train:\t L2 norm of A \t  0.35118691609656894\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.0023633166210332585\n",
      "Train:\t Perf B \t 0.0025261316121881123\n",
      "Train:\t Perf I \t 0.0012274160165531263\n",
      "Train:\t L2 norm of A \t  0.3524369115387283\n",
      "-------------- The gamma 0.080000\n",
      "Train:\tLoss \t  0.001650995579579495\n",
      "Train:\t Perf B \t 0.002522310092015455\n",
      "Train:\t Perf I \t 0.0012521801779768092\n",
      "Train:\t L2 norm of A \t  0.3383120167692175\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.00136217901459816\n",
      "Train:\t Perf B \t 0.0024723400218426158\n",
      "Train:\t Perf I \t 0.0012727807520347936\n",
      "Train:\t L2 norm of A \t  0.34975743064893144\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.002018011505778928\n",
      "Train:\t Perf B \t 0.0024638494416099367\n",
      "Train:\t Perf I \t 0.001206977556423361\n",
      "Train:\t L2 norm of A \t  0.3414337770483597\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.002013848005560118\n",
      "Train:\t Perf B \t 0.002506594201982426\n",
      "Train:\t Perf I \t 0.0012032802813732187\n",
      "Train:\t L2 norm of A \t  0.36518138210406786\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.001518703614742364\n",
      "Train:\t Perf B \t 0.0024539327419381917\n",
      "Train:\t Perf I \t 0.0013092468497552094\n",
      "Train:\t L2 norm of A \t  0.3718065103029377\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0015069336003808737\n",
      "Train:\t Perf B \t 0.002523396821682823\n",
      "Train:\t Perf I \t 0.0012431610311123508\n",
      "Train:\t L2 norm of A \t  0.34914666695781726\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0024005243859643796\n",
      "Train:\t Perf B \t 0.002493675119996853\n",
      "Train:\t Perf I \t 0.001161119090860036\n",
      "Train:\t L2 norm of A \t  0.3625069788644892\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0028113263732566737\n",
      "Train:\t Perf B \t 0.0025549597161528157\n",
      "Train:\t Perf I \t 0.0012307767988711058\n",
      "Train:\t L2 norm of A \t  0.34223149140033515\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.002361794661898906\n",
      "Train:\t Perf B \t 0.0025546297617244093\n",
      "Train:\t Perf I \t 0.0012798189449632087\n",
      "Train:\t L2 norm of A \t  0.3383474295131702\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0018223594811735597\n",
      "Train:\t Perf B \t 0.002467565586995088\n",
      "Train:\t Perf I \t 0.0011800544446567418\n",
      "Train:\t L2 norm of A \t  0.36005163146595115\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0022907637465744667\n",
      "Train:\t Perf B \t 0.0024799197276580667\n",
      "Train:\t Perf I \t 0.0013271142535660847\n",
      "Train:\t L2 norm of A \t  0.3436948573506713\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0014495199999372528\n",
      "Train:\t Perf B \t 0.0025622465559331895\n",
      "Train:\t Perf I \t 0.00122293753054328\n",
      "Train:\t L2 norm of A \t  0.34006655888302245\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0019103646957656882\n",
      "Train:\t Perf B \t 0.0024093034273571907\n",
      "Train:\t Perf I \t 0.0012064517956910025\n",
      "Train:\t L2 norm of A \t  0.3559222775578423\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0021026407570827596\n",
      "Train:\t Perf B \t 0.0024392886842003613\n",
      "Train:\t Perf I \t 0.001178898566303015\n",
      "Train:\t L2 norm of A \t  0.34779130534875236\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0016157607949367852\n",
      "Train:\t Perf B \t 0.0025396931794776584\n",
      "Train:\t Perf I \t 0.0013000840488411778\n",
      "Train:\t L2 norm of A \t  0.3377992415209819\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0014003960728348663\n",
      "Train:\t Perf B \t 0.002554069835846457\n",
      "Train:\t Perf I \t 0.0013642662797787313\n",
      "Train:\t L2 norm of A \t  0.3459449227024132\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0019708074386870444\n",
      "Train:\t Perf B \t 0.0025679872356666844\n",
      "Train:\t Perf I \t 0.0011847630731058136\n",
      "Train:\t L2 norm of A \t  0.3425601627887476\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0016037966751423585\n",
      "Train:\t Perf B \t 0.002468659926121986\n",
      "Train:\t Perf I \t 0.0012512102507911055\n",
      "Train:\t L2 norm of A \t  0.35698814834148174\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0017017001281908384\n",
      "Train:\t Perf B \t 0.002609934112905546\n",
      "Train:\t Perf I \t 0.0012794261711541579\n",
      "Train:\t L2 norm of A \t  0.34401529043195905\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.001575605289130249\n",
      "Train:\t Perf B \t 0.0024614403133995755\n",
      "Train:\t Perf I \t 0.0013043489239849247\n",
      "Train:\t L2 norm of A \t  0.3368853472026191\n",
      "-------------- The gamma 0.090000\n",
      "Train:\tLoss \t  0.0019293699482417217\n",
      "Train:\t Perf B \t 0.0025418270057717047\n",
      "Train:\t Perf I \t 0.0012876975024818892\n",
      "Train:\t L2 norm of A \t  0.34837907896331277\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.004017907061024882\n",
      "Train:\t Perf B \t 0.0024285667703693305\n",
      "Train:\t Perf I \t 0.0011438648825200031\n",
      "Train:\t L2 norm of A \t  0.3505812299885671\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.002128660104249416\n",
      "Train:\t Perf B \t 0.0024591271055185195\n",
      "Train:\t Perf I \t 0.0012229953826190698\n",
      "Train:\t L2 norm of A \t  0.3503363572433607\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.00221206795488092\n",
      "Train:\t Perf B \t 0.0024558681207696824\n",
      "Train:\t Perf I \t 0.001232344697909987\n",
      "Train:\t L2 norm of A \t  0.3652818986067046\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.0029156858649994556\n",
      "Train:\t Perf B \t 0.002468694899278653\n",
      "Train:\t Perf I \t 0.0012898658653302095\n",
      "Train:\t L2 norm of A \t  0.35285978177314936\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.0014836135720346464\n",
      "Train:\t Perf B \t 0.0024656188324971846\n",
      "Train:\t Perf I \t 0.0011381865227983313\n",
      "Train:\t L2 norm of A \t  0.3562040444129632\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.0018523475704556966\n",
      "Train:\t Perf B \t 0.0025200780730942067\n",
      "Train:\t Perf I \t 0.001225211420478145\n",
      "Train:\t L2 norm of A \t  0.35151420512671805\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.001531712125807083\n",
      "Train:\t Perf B \t 0.002510759185593831\n",
      "Train:\t Perf I \t 0.00129374584382879\n",
      "Train:\t L2 norm of A \t  0.3413626917801466\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.0018427742137222096\n",
      "Train:\t Perf B \t 0.0024060422792209035\n",
      "Train:\t Perf I \t 0.0012445433642743376\n",
      "Train:\t L2 norm of A \t  0.35570226506373237\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.0025048478537344136\n",
      "Train:\t Perf B \t 0.0024508437927608126\n",
      "Train:\t Perf I \t 0.0012149523774207295\n",
      "Train:\t L2 norm of A \t  0.34584710741609886\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.001693317369221856\n",
      "Train:\t Perf B \t 0.002495402928666446\n",
      "Train:\t Perf I \t 0.0011833438174576219\n",
      "Train:\t L2 norm of A \t  0.3541062500260909\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.001455086170935767\n",
      "Train:\t Perf B \t 0.002433160961511484\n",
      "Train:\t Perf I \t 0.0012107584105745025\n",
      "Train:\t L2 norm of A \t  0.3669520560148331\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.0014980028338951745\n",
      "Train:\t Perf B \t 0.002406717147192128\n",
      "Train:\t Perf I \t 0.001133199912039458\n",
      "Train:\t L2 norm of A \t  0.35827480799313655\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.0017693248763528116\n",
      "Train:\t Perf B \t 0.0025131766225978953\n",
      "Train:\t Perf I \t 0.00114667981678364\n",
      "Train:\t L2 norm of A \t  0.3294413861447275\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.002010649776373777\n",
      "Train:\t Perf B \t 0.0025077547386396547\n",
      "Train:\t Perf I \t 0.0012020832962551277\n",
      "Train:\t L2 norm of A \t  0.33654161612018324\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.0013709916597019157\n",
      "Train:\t Perf B \t 0.0024472718504049197\n",
      "Train:\t Perf I \t 0.0012635644753004928\n",
      "Train:\t L2 norm of A \t  0.3474920787914002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.002177414985263376\n",
      "Train:\t Perf B \t 0.0024384378859968903\n",
      "Train:\t Perf I \t 0.0011943197111845588\n",
      "Train:\t L2 norm of A \t  0.35628697338017196\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.0015187793655478266\n",
      "Train:\t Perf B \t 0.002467220579015398\n",
      "Train:\t Perf I \t 0.0012931849465331144\n",
      "Train:\t L2 norm of A \t  0.347050979897485\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.0023542465115701555\n",
      "Train:\t Perf B \t 0.0024932216720754494\n",
      "Train:\t Perf I \t 0.0013497033887299756\n",
      "Train:\t L2 norm of A \t  0.3480512886207983\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.0017543266905604238\n",
      "Train:\t Perf B \t 0.002419303453490274\n",
      "Train:\t Perf I \t 0.0012252226547167947\n",
      "Train:\t L2 norm of A \t  0.34508024017887984\n",
      "-------------- The gamma 0.100000\n",
      "Train:\tLoss \t  0.0023142962140911107\n",
      "Train:\t Perf B \t 0.0025269834769808424\n",
      "Train:\t Perf I \t 0.0012850055533057865\n",
      "Train:\t L2 norm of A \t  0.3671180667315951\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0014907112558107494\n",
      "Train:\t Perf B \t 0.0025214109556529396\n",
      "Train:\t Perf I \t 0.0012122483353421887\n",
      "Train:\t L2 norm of A \t  0.3542786885090649\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0014465322937433358\n",
      "Train:\t Perf B \t 0.0023892030507512633\n",
      "Train:\t Perf I \t 0.0012954622602407006\n",
      "Train:\t L2 norm of A \t  0.3606456389830422\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0014221364268665589\n",
      "Train:\t Perf B \t 0.0025918803867835123\n",
      "Train:\t Perf I \t 0.001248543556948482\n",
      "Train:\t L2 norm of A \t  0.360051930485421\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0022848325571435046\n",
      "Train:\t Perf B \t 0.002503530641878015\n",
      "Train:\t Perf I \t 0.001114440840705827\n",
      "Train:\t L2 norm of A \t  0.34685859115935697\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.001676395118848109\n",
      "Train:\t Perf B \t 0.0025208405435485047\n",
      "Train:\t Perf I \t 0.0012459848774052805\n",
      "Train:\t L2 norm of A \t  0.3578738800857432\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0016303523357157977\n",
      "Train:\t Perf B \t 0.002412298636573657\n",
      "Train:\t Perf I \t 0.001169143526033197\n",
      "Train:\t L2 norm of A \t  0.3645514751016107\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0013758745350218448\n",
      "Train:\t Perf B \t 0.0024723982786766727\n",
      "Train:\t Perf I \t 0.001325094616877408\n",
      "Train:\t L2 norm of A \t  0.3689912640489436\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0012957177300590228\n",
      "Train:\t Perf B \t 0.002441540161544125\n",
      "Train:\t Perf I \t 0.001215480339880312\n",
      "Train:\t L2 norm of A \t  0.3464983731351064\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0016310552124311568\n",
      "Train:\t Perf B \t 0.0024044522866841205\n",
      "Train:\t Perf I \t 0.0011988481747972457\n",
      "Train:\t L2 norm of A \t  0.35244819734757193\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0018334162292712672\n",
      "Train:\t Perf B \t 0.0024983242713751894\n",
      "Train:\t Perf I \t 0.001198156140059733\n",
      "Train:\t L2 norm of A \t  0.3674656168802162\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0025659141548924824\n",
      "Train:\t Perf B \t 0.0023406739541047164\n",
      "Train:\t Perf I \t 0.0010945574201143271\n",
      "Train:\t L2 norm of A \t  0.3533874931559404\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.00185684912394313\n",
      "Train:\t Perf B \t 0.002505041016513611\n",
      "Train:\t Perf I \t 0.0012896235835021374\n",
      "Train:\t L2 norm of A \t  0.33874595720501444\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.00243027888853877\n",
      "Train:\t Perf B \t 0.002500380100741115\n",
      "Train:\t Perf I \t 0.001283515615188355\n",
      "Train:\t L2 norm of A \t  0.3542335446653786\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0015982890855631548\n",
      "Train:\t Perf B \t 0.0023240910809455566\n",
      "Train:\t Perf I \t 0.0011000460516909284\n",
      "Train:\t L2 norm of A \t  0.36925761877185137\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0017346840970014095\n",
      "Train:\t Perf B \t 0.0025201506143932117\n",
      "Train:\t Perf I \t 0.0013399355162711857\n",
      "Train:\t L2 norm of A \t  0.3477498504909808\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.001494699231757749\n",
      "Train:\t Perf B \t 0.0024765694339215637\n",
      "Train:\t Perf I \t 0.0012108574152780967\n",
      "Train:\t L2 norm of A \t  0.36384109024544486\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0016726748846867948\n",
      "Train:\t Perf B \t 0.0026210904563247206\n",
      "Train:\t Perf I \t 0.0013441791222106416\n",
      "Train:\t L2 norm of A \t  0.35114379083687397\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.001487467355337802\n",
      "Train:\t Perf B \t 0.0024924345259364675\n",
      "Train:\t Perf I \t 0.0011414704826059059\n",
      "Train:\t L2 norm of A \t  0.35280984729134146\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0019478398869227592\n",
      "Train:\t Perf B \t 0.002347742858858312\n",
      "Train:\t Perf I \t 0.001154175011929756\n",
      "Train:\t L2 norm of A \t  0.34304785006539257\n",
      "-------------- The gamma 0.110000\n",
      "Train:\tLoss \t  0.0017783170876420724\n",
      "Train:\t Perf B \t 0.002519574736901461\n",
      "Train:\t Perf I \t 0.00124503752553649\n",
      "Train:\t L2 norm of A \t  0.3493162210814656\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.002040986190702171\n",
      "Train:\t Perf B \t 0.0023659455557618046\n",
      "Train:\t Perf I \t 0.001169180854970324\n",
      "Train:\t L2 norm of A \t  0.3615244321341143\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.0019851641330841325\n",
      "Train:\t Perf B \t 0.00239963821850732\n",
      "Train:\t Perf I \t 0.001124524597136645\n",
      "Train:\t L2 norm of A \t  0.37126372834044524\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.0019099712733456516\n",
      "Train:\t Perf B \t 0.0024676953025830383\n",
      "Train:\t Perf I \t 0.0011769588042192245\n",
      "Train:\t L2 norm of A \t  0.3563768820626775\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.0014967994802959558\n",
      "Train:\t Perf B \t 0.0024289032232904574\n",
      "Train:\t Perf I \t 0.0011783891474611257\n",
      "Train:\t L2 norm of A \t  0.35269737836461446\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.001562000981215957\n",
      "Train:\t Perf B \t 0.002509894319333617\n",
      "Train:\t Perf I \t 0.0012820546636643899\n",
      "Train:\t L2 norm of A \t  0.36307199164356807\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.002779207373033351\n",
      "Train:\t Perf B \t 0.002381700470073342\n",
      "Train:\t Perf I \t 0.001164592610550269\n",
      "Train:\t L2 norm of A \t  0.36567351961835404\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.0015795163181725397\n",
      "Train:\t Perf B \t 0.0024957114757699675\n",
      "Train:\t Perf I \t 0.001271972694395198\n",
      "Train:\t L2 norm of A \t  0.3668660839451387\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.002705144623336364\n",
      "Train:\t Perf B \t 0.0025360531890193877\n",
      "Train:\t Perf I \t 0.0012274638632892474\n",
      "Train:\t L2 norm of A \t  0.36314970867083557\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.001653291580920965\n",
      "Train:\t Perf B \t 0.002368554436951994\n",
      "Train:\t Perf I \t 0.0010817717105211441\n",
      "Train:\t L2 norm of A \t  0.3462864016943957\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.0019018990979529846\n",
      "Train:\t Perf B \t 0.0024778260661551107\n",
      "Train:\t Perf I \t 0.0012828057163253319\n",
      "Train:\t L2 norm of A \t  0.36182022082464926\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.0016458403472242018\n",
      "Train:\t Perf B \t 0.0024683274090499794\n",
      "Train:\t Perf I \t 0.0013180516031663962\n",
      "Train:\t L2 norm of A \t  0.3523329030476963\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.002608531056151798\n",
      "Train:\t Perf B \t 0.002370849140737236\n",
      "Train:\t Perf I \t 0.0012576722376895096\n",
      "Train:\t L2 norm of A \t  0.3598561530344499\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.002502593210442831\n",
      "Train:\t Perf B \t 0.0023997247610375054\n",
      "Train:\t Perf I \t 0.001150685868909056\n",
      "Train:\t L2 norm of A \t  0.35026426998124904\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.0023646482163499688\n",
      "Train:\t Perf B \t 0.0024049597238550323\n",
      "Train:\t Perf I \t 0.0011743813279965756\n",
      "Train:\t L2 norm of A \t  0.3591956900461397\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.0022217013046791764\n",
      "Train:\t Perf B \t 0.002520912832465631\n",
      "Train:\t Perf I \t 0.0011244898605127429\n",
      "Train:\t L2 norm of A \t  0.3508924863394551\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.0017710304815924723\n",
      "Train:\t Perf B \t 0.0024301226372468536\n",
      "Train:\t Perf I \t 0.00107735342978661\n",
      "Train:\t L2 norm of A \t  0.34650437425253783\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.001449537403681553\n",
      "Train:\t Perf B \t 0.002517437993827816\n",
      "Train:\t Perf I \t 0.0012274343271128374\n",
      "Train:\t L2 norm of A \t  0.33796245443852524\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.002434254597632918\n",
      "Train:\t Perf B \t 0.0024346288279392518\n",
      "Train:\t Perf I \t 0.0012484031652785898\n",
      "Train:\t L2 norm of A \t  0.3580861340191239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.0019282109739651335\n",
      "Train:\t Perf B \t 0.0024125053211750793\n",
      "Train:\t Perf I \t 0.0011980896516989724\n",
      "Train:\t L2 norm of A \t  0.35082813839263916\n",
      "-------------- The gamma 0.120000\n",
      "Train:\tLoss \t  0.002324254055956126\n",
      "Train:\t Perf B \t 0.00250980313026306\n",
      "Train:\t Perf I \t 0.0012328572886897897\n",
      "Train:\t L2 norm of A \t  0.349345107928202\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.001723306511814107\n",
      "Train:\t Perf B \t 0.0024813738734326663\n",
      "Train:\t Perf I \t 0.0012526160833210205\n",
      "Train:\t L2 norm of A \t  0.3602492216212418\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0017962285347705045\n",
      "Train:\t Perf B \t 0.002338075431846366\n",
      "Train:\t Perf I \t 0.0011369233736287822\n",
      "Train:\t L2 norm of A \t  0.3601570974946744\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.001392246014886898\n",
      "Train:\t Perf B \t 0.002473933768352135\n",
      "Train:\t Perf I \t 0.0012154488677454725\n",
      "Train:\t L2 norm of A \t  0.35619467318010506\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.00203974360990989\n",
      "Train:\t Perf B \t 0.0024650925126209347\n",
      "Train:\t Perf I \t 0.0011908934236308226\n",
      "Train:\t L2 norm of A \t  0.3528832981414136\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0019932148638860557\n",
      "Train:\t Perf B \t 0.002341060850398443\n",
      "Train:\t Perf I \t 0.0011732479567656859\n",
      "Train:\t L2 norm of A \t  0.33908590235217745\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.002588252830949088\n",
      "Train:\t Perf B \t 0.0023387113240380354\n",
      "Train:\t Perf I \t 0.0011341469635332804\n",
      "Train:\t L2 norm of A \t  0.37617085642778214\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0024732359844826407\n",
      "Train:\t Perf B \t 0.002450833899558092\n",
      "Train:\t Perf I \t 0.001256559992889962\n",
      "Train:\t L2 norm of A \t  0.353574239768561\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0018601091924301107\n",
      "Train:\t Perf B \t 0.002388588376797328\n",
      "Train:\t Perf I \t 0.0012965457849624861\n",
      "Train:\t L2 norm of A \t  0.35939576516482963\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0013925322837335542\n",
      "Train:\t Perf B \t 0.0024407524218173775\n",
      "Train:\t Perf I \t 0.0011350313907508049\n",
      "Train:\t L2 norm of A \t  0.3536850835312706\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0016159906160539055\n",
      "Train:\t Perf B \t 0.0023359212829880908\n",
      "Train:\t Perf I \t 0.0011240480825556395\n",
      "Train:\t L2 norm of A \t  0.36632696559259154\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0023246367660435924\n",
      "Train:\t Perf B \t 0.0025653970608565555\n",
      "Train:\t Perf I \t 0.0012449673272209247\n",
      "Train:\t L2 norm of A \t  0.3664013139358743\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0019361023709273222\n",
      "Train:\t Perf B \t 0.002424901721324415\n",
      "Train:\t Perf I \t 0.0012685503743028708\n",
      "Train:\t L2 norm of A \t  0.35036283056030365\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0018905876298289544\n",
      "Train:\t Perf B \t 0.002459315731449519\n",
      "Train:\t Perf I \t 0.0012253005569161782\n",
      "Train:\t L2 norm of A \t  0.34079653884154776\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0019468799285453139\n",
      "Train:\t Perf B \t 0.0024522969153492474\n",
      "Train:\t Perf I \t 0.0012860399397435947\n",
      "Train:\t L2 norm of A \t  0.3537164170760892\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0013343142129343148\n",
      "Train:\t Perf B \t 0.0024500726245331784\n",
      "Train:\t Perf I \t 0.0012563238592881664\n",
      "Train:\t L2 norm of A \t  0.3402096491517413\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0014682084197772532\n",
      "Train:\t Perf B \t 0.0024908170051965266\n",
      "Train:\t Perf I \t 0.0011626455800065661\n",
      "Train:\t L2 norm of A \t  0.366957963863991\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0015275669063133331\n",
      "Train:\t Perf B \t 0.0023800246638649653\n",
      "Train:\t Perf I \t 0.0011522076887390239\n",
      "Train:\t L2 norm of A \t  0.35368773423266914\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0014106488289404712\n",
      "Train:\t Perf B \t 0.002457650558941116\n",
      "Train:\t Perf I \t 0.0012147986659016463\n",
      "Train:\t L2 norm of A \t  0.34699017928235626\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0022198999672716838\n",
      "Train:\t Perf B \t 0.0025546787878620546\n",
      "Train:\t Perf I \t 0.001314287321831739\n",
      "Train:\t L2 norm of A \t  0.35645201689876727\n",
      "-------------- The gamma 0.130000\n",
      "Train:\tLoss \t  0.0013778145824093774\n",
      "Train:\t Perf B \t 0.0024027264083285307\n",
      "Train:\t Perf I \t 0.0012168971766011154\n",
      "Train:\t L2 norm of A \t  0.36281653594058516\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.002068623005992947\n",
      "Train:\t Perf B \t 0.002415966842629539\n",
      "Train:\t Perf I \t 0.0011759551331428735\n",
      "Train:\t L2 norm of A \t  0.3682974423385503\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0017085935150731473\n",
      "Train:\t Perf B \t 0.002435382952681461\n",
      "Train:\t Perf I \t 0.0011487359248926303\n",
      "Train:\t L2 norm of A \t  0.3524793364926227\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.003077202263700485\n",
      "Train:\t Perf B \t 0.002375325937846587\n",
      "Train:\t Perf I \t 0.0010509661788380869\n",
      "Train:\t L2 norm of A \t  0.3816882369327726\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0025182036459525303\n",
      "Train:\t Perf B \t 0.0023564253168255125\n",
      "Train:\t Perf I \t 0.0012028762735177556\n",
      "Train:\t L2 norm of A \t  0.35307426603427366\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.001420980781713671\n",
      "Train:\t Perf B \t 0.0024209570416697406\n",
      "Train:\t Perf I \t 0.0012156839400514967\n",
      "Train:\t L2 norm of A \t  0.3579883200143192\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0017939114672316325\n",
      "Train:\t Perf B \t 0.0023730854353696176\n",
      "Train:\t Perf I \t 0.0012965950212982461\n",
      "Train:\t L2 norm of A \t  0.36952175102195667\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0014764735928007194\n",
      "Train:\t Perf B \t 0.002417729295018494\n",
      "Train:\t Perf I \t 0.0012327607603485624\n",
      "Train:\t L2 norm of A \t  0.36226257066531475\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0016875471730670945\n",
      "Train:\t Perf B \t 0.0024523149412068288\n",
      "Train:\t Perf I \t 0.0011876185002584478\n",
      "Train:\t L2 norm of A \t  0.36271982803887975\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0019592445395383044\n",
      "Train:\t Perf B \t 0.0024332635677312285\n",
      "Train:\t Perf I \t 0.001256166108907657\n",
      "Train:\t L2 norm of A \t  0.37132447882796854\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0018219619306716625\n",
      "Train:\t Perf B \t 0.002376356111063366\n",
      "Train:\t Perf I \t 0.0011101315725779203\n",
      "Train:\t L2 norm of A \t  0.37543520656564927\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0013963192068149532\n",
      "Train:\t Perf B \t 0.002331599473543657\n",
      "Train:\t Perf I \t 0.0011946741473966424\n",
      "Train:\t L2 norm of A \t  0.3733706203624689\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0029933202427426266\n",
      "Train:\t Perf B \t 0.0023657220495297013\n",
      "Train:\t Perf I \t 0.001156639637179725\n",
      "Train:\t L2 norm of A \t  0.38072646221324197\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0018242674341205193\n",
      "Train:\t Perf B \t 0.002472384712695329\n",
      "Train:\t Perf I \t 0.0012320800735437056\n",
      "Train:\t L2 norm of A \t  0.34709883293207366\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0014553198222982074\n",
      "Train:\t Perf B \t 0.0024396254447267758\n",
      "Train:\t Perf I \t 0.0011056190442678693\n",
      "Train:\t L2 norm of A \t  0.3589255762808235\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0015132977464786597\n",
      "Train:\t Perf B \t 0.0023954186448490204\n",
      "Train:\t Perf I \t 0.0012379491515449811\n",
      "Train:\t L2 norm of A \t  0.350453975650256\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0020255659248528696\n",
      "Train:\t Perf B \t 0.0023089129538543538\n",
      "Train:\t Perf I \t 0.0011526199762578809\n",
      "Train:\t L2 norm of A \t  0.35930945083228355\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0021197938542725577\n",
      "Train:\t Perf B \t 0.002463813690752391\n",
      "Train:\t Perf I \t 0.0012992866211084992\n",
      "Train:\t L2 norm of A \t  0.35463338423304974\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0020614318770000102\n",
      "Train:\t Perf B \t 0.00251892895612387\n",
      "Train:\t Perf I \t 0.0012504916753730804\n",
      "Train:\t L2 norm of A \t  0.3496350375846055\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0024116845681260287\n",
      "Train:\t Perf B \t 0.0023709760726683027\n",
      "Train:\t Perf I \t 0.0012246485447384452\n",
      "Train:\t L2 norm of A \t  0.3622757734587585\n",
      "-------------- The gamma 0.140000\n",
      "Train:\tLoss \t  0.0022810331912545487\n",
      "Train:\t Perf B \t 0.0024442606827490524\n",
      "Train:\t Perf I \t 0.0012191620971459087\n",
      "Train:\t L2 norm of A \t  0.3576637030512111\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0019922266690104305\n",
      "Train:\t Perf B \t 0.002376168340918038\n",
      "Train:\t Perf I \t 0.0012537294816582557\n",
      "Train:\t L2 norm of A \t  0.3582070712672715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0028381656959022036\n",
      "Train:\t Perf B \t 0.0024878432499867175\n",
      "Train:\t Perf I \t 0.001294095247084988\n",
      "Train:\t L2 norm of A \t  0.3715820055463174\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0019901101622876436\n",
      "Train:\t Perf B \t 0.0023367789133887868\n",
      "Train:\t Perf I \t 0.001299641245199714\n",
      "Train:\t L2 norm of A \t  0.374041950091177\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.002217716890634723\n",
      "Train:\t Perf B \t 0.0024156074315497445\n",
      "Train:\t Perf I \t 0.0012583320781500223\n",
      "Train:\t L2 norm of A \t  0.3769959767042504\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0020509462722525643\n",
      "Train:\t Perf B \t 0.0023601345131842133\n",
      "Train:\t Perf I \t 0.001159229771071299\n",
      "Train:\t L2 norm of A \t  0.37820903365740555\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.001994044885345062\n",
      "Train:\t Perf B \t 0.002425185191045148\n",
      "Train:\t Perf I \t 0.001164098201036826\n",
      "Train:\t L2 norm of A \t  0.3575460292354676\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0021290279555959335\n",
      "Train:\t Perf B \t 0.0024406516126163234\n",
      "Train:\t Perf I \t 0.0011836987169263449\n",
      "Train:\t L2 norm of A \t  0.34215913630392397\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.003201407618699503\n",
      "Train:\t Perf B \t 0.002352832685557806\n",
      "Train:\t Perf I \t 0.0012200166936845627\n",
      "Train:\t L2 norm of A \t  0.38270020708802954\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0032777594124485205\n",
      "Train:\t Perf B \t 0.0024115516503731204\n",
      "Train:\t Perf I \t 0.0012287525735464923\n",
      "Train:\t L2 norm of A \t  0.3788467148459044\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.002140056096780281\n",
      "Train:\t Perf B \t 0.0024683640560604165\n",
      "Train:\t Perf I \t 0.0012794779177678363\n",
      "Train:\t L2 norm of A \t  0.3645485563182325\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0018177033396353562\n",
      "Train:\t Perf B \t 0.00237801353771601\n",
      "Train:\t Perf I \t 0.0012168705514314802\n",
      "Train:\t L2 norm of A \t  0.37585114235166656\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0026995361821278135\n",
      "Train:\t Perf B \t 0.0024538394435922686\n",
      "Train:\t Perf I \t 0.001209336683919262\n",
      "Train:\t L2 norm of A \t  0.37424183519250576\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0022092991972036125\n",
      "Train:\t Perf B \t 0.0024570909435108093\n",
      "Train:\t Perf I \t 0.0012308240351883047\n",
      "Train:\t L2 norm of A \t  0.3785968313219816\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0019274921331555981\n",
      "Train:\t Perf B \t 0.002492501319297454\n",
      "Train:\t Perf I \t 0.0012535301218177277\n",
      "Train:\t L2 norm of A \t  0.3711149224222871\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0015954257554173443\n",
      "Train:\t Perf B \t 0.002292868977154226\n",
      "Train:\t Perf I \t 0.0012517190822555764\n",
      "Train:\t L2 norm of A \t  0.3684935510964008\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.002039307102781172\n",
      "Train:\t Perf B \t 0.002467703663262756\n",
      "Train:\t Perf I \t 0.0011350233059930197\n",
      "Train:\t L2 norm of A \t  0.3550377102006754\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0023078693637263673\n",
      "Train:\t Perf B \t 0.002459136279821936\n",
      "Train:\t Perf I \t 0.0011613843723482528\n",
      "Train:\t L2 norm of A \t  0.33382497076386397\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.002361568600795735\n",
      "Train:\t Perf B \t 0.0024120144779367085\n",
      "Train:\t Perf I \t 0.0012695195659337994\n",
      "Train:\t L2 norm of A \t  0.35849472635323165\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0036691226025860214\n",
      "Train:\t Perf B \t 0.002340898631036213\n",
      "Train:\t Perf I \t 0.0011100884279608963\n",
      "Train:\t L2 norm of A \t  0.35597940231074227\n",
      "-------------- The gamma 0.150000\n",
      "Train:\tLoss \t  0.0015908106401942169\n",
      "Train:\t Perf B \t 0.0024871487410433723\n",
      "Train:\t Perf I \t 0.0012669651161850996\n",
      "Train:\t L2 norm of A \t  0.3626289999767776\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0017207895845356806\n",
      "Train:\t Perf B \t 0.0023816901046527085\n",
      "Train:\t Perf I \t 0.0011400193256858304\n",
      "Train:\t L2 norm of A \t  0.34407801191480236\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.00148731799654863\n",
      "Train:\t Perf B \t 0.002537747805321241\n",
      "Train:\t Perf I \t 0.001286170997876806\n",
      "Train:\t L2 norm of A \t  0.3604674168621826\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0034484910519936946\n",
      "Train:\t Perf B \t 0.002353026495665065\n",
      "Train:\t Perf I \t 0.0011565573038471435\n",
      "Train:\t L2 norm of A \t  0.3584011738431741\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.002267839368085533\n",
      "Train:\t Perf B \t 0.0023453014200137783\n",
      "Train:\t Perf I \t 0.0012530283333314757\n",
      "Train:\t L2 norm of A \t  0.3724742362721953\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.002918371006968994\n",
      "Train:\t Perf B \t 0.0023785032417668215\n",
      "Train:\t Perf I \t 0.0012041982160913391\n",
      "Train:\t L2 norm of A \t  0.3941549745507383\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0029693013099382935\n",
      "Train:\t Perf B \t 0.0022790768963408745\n",
      "Train:\t Perf I \t 0.0011536413384484936\n",
      "Train:\t L2 norm of A \t  0.3665198234371002\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0014387590256903847\n",
      "Train:\t Perf B \t 0.0024891722906794445\n",
      "Train:\t Perf I \t 0.001224127236893967\n",
      "Train:\t L2 norm of A \t  0.36325321267922545\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0018381910055743978\n",
      "Train:\t Perf B \t 0.002331137504255421\n",
      "Train:\t Perf I \t 0.0011177228281258654\n",
      "Train:\t L2 norm of A \t  0.36398551071312973\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0015878592782389016\n",
      "Train:\t Perf B \t 0.0024117762788683048\n",
      "Train:\t Perf I \t 0.001116155447041287\n",
      "Train:\t L2 norm of A \t  0.37417376733964125\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.001457492317876586\n",
      "Train:\t Perf B \t 0.0023899728889398516\n",
      "Train:\t Perf I \t 0.0012802116405402637\n",
      "Train:\t L2 norm of A \t  0.3597876757855352\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0019941195578948518\n",
      "Train:\t Perf B \t 0.0023749979604466144\n",
      "Train:\t Perf I \t 0.001277926234790589\n",
      "Train:\t L2 norm of A \t  0.35718579933784744\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0020279177163034286\n",
      "Train:\t Perf B \t 0.0024235918230498697\n",
      "Train:\t Perf I \t 0.0011358342471416527\n",
      "Train:\t L2 norm of A \t  0.3782299201664269\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0029280855213765066\n",
      "Train:\t Perf B \t 0.0023332149067980504\n",
      "Train:\t Perf I \t 0.001119068883868063\n",
      "Train:\t L2 norm of A \t  0.35622485330480375\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0021004652926611884\n",
      "Train:\t Perf B \t 0.0023191933693228286\n",
      "Train:\t Perf I \t 0.0011339519919051368\n",
      "Train:\t L2 norm of A \t  0.36029351861934017\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0016003779133598158\n",
      "Train:\t Perf B \t 0.0024582940799655712\n",
      "Train:\t Perf I \t 0.0012392699708426726\n",
      "Train:\t L2 norm of A \t  0.3636574858593176\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0017092711551518118\n",
      "Train:\t Perf B \t 0.0023414546313973747\n",
      "Train:\t Perf I \t 0.0011942216136092401\n",
      "Train:\t L2 norm of A \t  0.36924308506292436\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0024932702896183377\n",
      "Train:\t Perf B \t 0.0024713481041537676\n",
      "Train:\t Perf I \t 0.0012988384887194927\n",
      "Train:\t L2 norm of A \t  0.36560033932230646\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.001672028513312642\n",
      "Train:\t Perf B \t 0.0023482625519346853\n",
      "Train:\t Perf I \t 0.000987147211882815\n",
      "Train:\t L2 norm of A \t  0.3554270953539541\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0014799075082776222\n",
      "Train:\t Perf B \t 0.0023456114227227076\n",
      "Train:\t Perf I \t 0.001189624377454415\n",
      "Train:\t L2 norm of A \t  0.37065410141419997\n",
      "-------------- The gamma 0.160000\n",
      "Train:\tLoss \t  0.0018209319147039378\n",
      "Train:\t Perf B \t 0.0022833095174870197\n",
      "Train:\t Perf I \t 0.0012279480125488168\n",
      "Train:\t L2 norm of A \t  0.3689497600380192\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0041775669522598185\n",
      "Train:\t Perf B \t 0.002260612741266042\n",
      "Train:\t Perf I \t 0.0011262467974299513\n",
      "Train:\t L2 norm of A \t  0.35135528529219073\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.001489756358634422\n",
      "Train:\t Perf B \t 0.0023841321072559578\n",
      "Train:\t Perf I \t 0.0011643161680668266\n",
      "Train:\t L2 norm of A \t  0.36916322070232643\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0019593074015656514\n",
      "Train:\t Perf B \t 0.002408829460041568\n",
      "Train:\t Perf I \t 0.0011990023795125026\n",
      "Train:\t L2 norm of A \t  0.3565175476722008\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0027578128619855278\n",
      "Train:\t Perf B \t 0.002333340640894518\n",
      "Train:\t Perf I \t 0.0012215313224801757\n",
      "Train:\t L2 norm of A \t  0.3676503555776626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0035338367625143675\n",
      "Train:\t Perf B \t 0.0024507623013587543\n",
      "Train:\t Perf I \t 0.0012627918665737417\n",
      "Train:\t L2 norm of A \t  0.3501427946724039\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0031053598953396508\n",
      "Train:\t Perf B \t 0.0023182280003669806\n",
      "Train:\t Perf I \t 0.001205445167425597\n",
      "Train:\t L2 norm of A \t  0.3883046970556494\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0031105944042523342\n",
      "Train:\t Perf B \t 0.0024208821458638337\n",
      "Train:\t Perf I \t 0.001221696148016136\n",
      "Train:\t L2 norm of A \t  0.3516979654247608\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0014068688983677512\n",
      "Train:\t Perf B \t 0.0024031094111485185\n",
      "Train:\t Perf I \t 0.0011686937801323263\n",
      "Train:\t L2 norm of A \t  0.3634793366014811\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.002538053430577932\n",
      "Train:\t Perf B \t 0.002205934901721062\n",
      "Train:\t Perf I \t 0.0010346411120738918\n",
      "Train:\t L2 norm of A \t  0.37402135829044175\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0022992715882513277\n",
      "Train:\t Perf B \t 0.0022896079697074176\n",
      "Train:\t Perf I \t 0.0011805101238148786\n",
      "Train:\t L2 norm of A \t  0.3900440631297682\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0028390545709363496\n",
      "Train:\t Perf B \t 0.0023206583150536917\n",
      "Train:\t Perf I \t 0.0011822313876440874\n",
      "Train:\t L2 norm of A \t  0.384537401420978\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.001977364842506314\n",
      "Train:\t Perf B \t 0.0022587598375863206\n",
      "Train:\t Perf I \t 0.0011738665011266276\n",
      "Train:\t L2 norm of A \t  0.3857829440656503\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0014997727065381057\n",
      "Train:\t Perf B \t 0.002441062067986702\n",
      "Train:\t Perf I \t 0.0011804859906622926\n",
      "Train:\t L2 norm of A \t  0.3579004445547128\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0023419873407495456\n",
      "Train:\t Perf B \t 0.0024308536187732294\n",
      "Train:\t Perf I \t 0.0011426376467799117\n",
      "Train:\t L2 norm of A \t  0.38409016374001304\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0033266620790571256\n",
      "Train:\t Perf B \t 0.002412652419795935\n",
      "Train:\t Perf I \t 0.0012338623528396547\n",
      "Train:\t L2 norm of A \t  0.3722006291732301\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.002225360350166021\n",
      "Train:\t Perf B \t 0.0023104950938394138\n",
      "Train:\t Perf I \t 0.0012166200311885126\n",
      "Train:\t L2 norm of A \t  0.36363302096836275\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0015450479272665788\n",
      "Train:\t Perf B \t 0.0023268062423533304\n",
      "Train:\t Perf I \t 0.001092517110794276\n",
      "Train:\t L2 norm of A \t  0.3720882398570053\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0013852263141016289\n",
      "Train:\t Perf B \t 0.0023372707285135657\n",
      "Train:\t Perf I \t 0.0011137626961862787\n",
      "Train:\t L2 norm of A \t  0.36517309598675235\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.003779839282191398\n",
      "Train:\t Perf B \t 0.0022701631697692247\n",
      "Train:\t Perf I \t 0.0011145624973970432\n",
      "Train:\t L2 norm of A \t  0.3694677993392876\n",
      "-------------- The gamma 0.170000\n",
      "Train:\tLoss \t  0.0014899627454766906\n",
      "Train:\t Perf B \t 0.0024052272160031444\n",
      "Train:\t Perf I \t 0.001221585626691162\n",
      "Train:\t L2 norm of A \t  0.3491026942062477\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0019402803995543097\n",
      "Train:\t Perf B \t 0.0022127736183274963\n",
      "Train:\t Perf I \t 0.001128114501744105\n",
      "Train:\t L2 norm of A \t  0.36997590238528527\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0019544519319319425\n",
      "Train:\t Perf B \t 0.002263651746580847\n",
      "Train:\t Perf I \t 0.001194116553383717\n",
      "Train:\t L2 norm of A \t  0.39942773936315407\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.001816749957647454\n",
      "Train:\t Perf B \t 0.0024275887327452625\n",
      "Train:\t Perf I \t 0.0011773294007564336\n",
      "Train:\t L2 norm of A \t  0.3590351610375654\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.002891101091026089\n",
      "Train:\t Perf B \t 0.002134850188733464\n",
      "Train:\t Perf I \t 0.0011108661022326501\n",
      "Train:\t L2 norm of A \t  0.3968711652079836\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0023928882551913387\n",
      "Train:\t Perf B \t 0.0023959167556265254\n",
      "Train:\t Perf I \t 0.0011384950238419263\n",
      "Train:\t L2 norm of A \t  0.3585564735817035\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.003038219609087241\n",
      "Train:\t Perf B \t 0.002229537171385788\n",
      "Train:\t Perf I \t 0.0010970807818433922\n",
      "Train:\t L2 norm of A \t  0.3731347689683426\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0022554710725891203\n",
      "Train:\t Perf B \t 0.00221466557320213\n",
      "Train:\t Perf I \t 0.0011302732661624293\n",
      "Train:\t L2 norm of A \t  0.3804684366141086\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0019083281839560049\n",
      "Train:\t Perf B \t 0.0022700178188549366\n",
      "Train:\t Perf I \t 0.0012399500199511539\n",
      "Train:\t L2 norm of A \t  0.3937151483087648\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0015276143204353192\n",
      "Train:\t Perf B \t 0.002448538582852521\n",
      "Train:\t Perf I \t 0.0011458029491923274\n",
      "Train:\t L2 norm of A \t  0.36222651162097\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0014405860455132282\n",
      "Train:\t Perf B \t 0.002306508434892786\n",
      "Train:\t Perf I \t 0.0010643947034005828\n",
      "Train:\t L2 norm of A \t  0.38262802598645873\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0026497683245324958\n",
      "Train:\t Perf B \t 0.0024007048113689196\n",
      "Train:\t Perf I \t 0.0011670167593336577\n",
      "Train:\t L2 norm of A \t  0.3766632637748517\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0019121392516384262\n",
      "Train:\t Perf B \t 0.002408948222934739\n",
      "Train:\t Perf I \t 0.0012106212501697322\n",
      "Train:\t L2 norm of A \t  0.3713743354000076\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0035978821555638254\n",
      "Train:\t Perf B \t 0.0022777945940347113\n",
      "Train:\t Perf I \t 0.0010443495132893203\n",
      "Train:\t L2 norm of A \t  0.36702851281895926\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0025696682440900552\n",
      "Train:\t Perf B \t 0.0024028402574592335\n",
      "Train:\t Perf I \t 0.0012436159522127646\n",
      "Train:\t L2 norm of A \t  0.3506105660537089\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.002376469025688453\n",
      "Train:\t Perf B \t 0.0024215648410131907\n",
      "Train:\t Perf I \t 0.0011934880448841577\n",
      "Train:\t L2 norm of A \t  0.3669440977415505\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.002184830654188005\n",
      "Train:\t Perf B \t 0.0022250314147698427\n",
      "Train:\t Perf I \t 0.0011778633653332334\n",
      "Train:\t L2 norm of A \t  0.3699046994267884\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0018980485421204855\n",
      "Train:\t Perf B \t 0.0024581615199776364\n",
      "Train:\t Perf I \t 0.001271965686830403\n",
      "Train:\t L2 norm of A \t  0.3685021144821805\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.002492956064934618\n",
      "Train:\t Perf B \t 0.002237586465346606\n",
      "Train:\t Perf I \t 0.0011196909876809305\n",
      "Train:\t L2 norm of A \t  0.38979806043454934\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0017286512371795697\n",
      "Train:\t Perf B \t 0.0024160923752962983\n",
      "Train:\t Perf I \t 0.0012181531093523198\n",
      "Train:\t L2 norm of A \t  0.3687634658836406\n",
      "-------------- The gamma 0.180000\n",
      "Train:\tLoss \t  0.0015644528082008585\n",
      "Train:\t Perf B \t 0.002340033258864591\n",
      "Train:\t Perf I \t 0.0010894207968979682\n",
      "Train:\t L2 norm of A \t  0.38763476466128\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0019028769022024878\n",
      "Train:\t Perf B \t 0.002413379944874944\n",
      "Train:\t Perf I \t 0.001232209724780896\n",
      "Train:\t L2 norm of A \t  0.36491674929253937\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0025268189375891197\n",
      "Train:\t Perf B \t 0.0023410140142537373\n",
      "Train:\t Perf I \t 0.0012832114331785176\n",
      "Train:\t L2 norm of A \t  0.36478373902718547\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0015764126765742134\n",
      "Train:\t Perf B \t 0.002267146896091168\n",
      "Train:\t Perf I \t 0.001117060657443677\n",
      "Train:\t L2 norm of A \t  0.38722694790847156\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0017991394653337379\n",
      "Train:\t Perf B \t 0.0022398542894227617\n",
      "Train:\t Perf I \t 0.0010697669705351806\n",
      "Train:\t L2 norm of A \t  0.37975160951154674\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.005148249349538246\n",
      "Train:\t Perf B \t 0.0021889732997348726\n",
      "Train:\t Perf I \t 0.0009890049454153167\n",
      "Train:\t L2 norm of A \t  0.3862444564865242\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0014976759462245845\n",
      "Train:\t Perf B \t 0.002356587983698279\n",
      "Train:\t Perf I \t 0.0011745213859615537\n",
      "Train:\t L2 norm of A \t  0.37680787410192296\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.001539716174233573\n",
      "Train:\t Perf B \t 0.0023992232297779556\n",
      "Train:\t Perf I \t 0.0011767015799095813\n",
      "Train:\t L2 norm of A \t  0.3682020924328522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0017802206232031665\n",
      "Train:\t Perf B \t 0.0023950231797773653\n",
      "Train:\t Perf I \t 0.0011742216735186935\n",
      "Train:\t L2 norm of A \t  0.39624221831996576\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0024903646965044116\n",
      "Train:\t Perf B \t 0.0022695387530466417\n",
      "Train:\t Perf I \t 0.0011488482934747491\n",
      "Train:\t L2 norm of A \t  0.3661913638443949\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.003032150219132213\n",
      "Train:\t Perf B \t 0.002350967413333984\n",
      "Train:\t Perf I \t 0.0012728950015442049\n",
      "Train:\t L2 norm of A \t  0.38504850129119556\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0020490687010883755\n",
      "Train:\t Perf B \t 0.002329357482079639\n",
      "Train:\t Perf I \t 0.0010955418033201858\n",
      "Train:\t L2 norm of A \t  0.38367135300108446\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.002784703432488451\n",
      "Train:\t Perf B \t 0.002312510978238833\n",
      "Train:\t Perf I \t 0.0011269700745850256\n",
      "Train:\t L2 norm of A \t  0.3872473269477754\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0038583836286674553\n",
      "Train:\t Perf B \t 0.0024492618078484616\n",
      "Train:\t Perf I \t 0.0012584107242039253\n",
      "Train:\t L2 norm of A \t  0.34793903485846495\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0020928936624275748\n",
      "Train:\t Perf B \t 0.0022580174580981464\n",
      "Train:\t Perf I \t 0.000969602826797976\n",
      "Train:\t L2 norm of A \t  0.3788183457870228\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0020500195753370286\n",
      "Train:\t Perf B \t 0.0023319655303032493\n",
      "Train:\t Perf I \t 0.0011594220769628536\n",
      "Train:\t L2 norm of A \t  0.38355468958673294\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0030925843872698974\n",
      "Train:\t Perf B \t 0.002334236959556164\n",
      "Train:\t Perf I \t 0.0011019770393676756\n",
      "Train:\t L2 norm of A \t  0.39511955015715966\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0034266603009541186\n",
      "Train:\t Perf B \t 0.0023889786441359147\n",
      "Train:\t Perf I \t 0.0011975303185623159\n",
      "Train:\t L2 norm of A \t  0.38914352262704643\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.001855201732906304\n",
      "Train:\t Perf B \t 0.0024244494076096214\n",
      "Train:\t Perf I \t 0.0012385418608554849\n",
      "Train:\t L2 norm of A \t  0.3487265653525736\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.0025072717805322523\n",
      "Train:\t Perf B \t 0.0022460278916207853\n",
      "Train:\t Perf I \t 0.001182740010865407\n",
      "Train:\t L2 norm of A \t  0.37363386018636086\n",
      "-------------- The gamma 0.190000\n",
      "Train:\tLoss \t  0.002481254815327242\n",
      "Train:\t Perf B \t 0.0023690678156005657\n",
      "Train:\t Perf I \t 0.0011603036662660683\n",
      "Train:\t L2 norm of A \t  0.375206022005188\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.0020214686297495748\n",
      "Train:\t Perf B \t 0.0022464698061745673\n",
      "Train:\t Perf I \t 0.0011795954004409624\n",
      "Train:\t L2 norm of A \t  0.40789331318678745\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.00518054688260155\n",
      "Train:\t Perf B \t 0.002286182052908287\n",
      "Train:\t Perf I \t 0.0010020243087686313\n",
      "Train:\t L2 norm of A \t  0.3895006042848467\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.0020670844333109187\n",
      "Train:\t Perf B \t 0.0023331203356305257\n",
      "Train:\t Perf I \t 0.0010902885168571368\n",
      "Train:\t L2 norm of A \t  0.38753984077182585\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.003096858375732764\n",
      "Train:\t Perf B \t 0.0020637670510521696\n",
      "Train:\t Perf I \t 0.0009247266034186811\n",
      "Train:\t L2 norm of A \t  0.3955942915797964\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.0028635999866696584\n",
      "Train:\t Perf B \t 0.0023987297344723523\n",
      "Train:\t Perf I \t 0.0013056088033376037\n",
      "Train:\t L2 norm of A \t  0.3738317136852644\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.0031880466512316653\n",
      "Train:\t Perf B \t 0.0022634242189477566\n",
      "Train:\t Perf I \t 0.0011158003190645776\n",
      "Train:\t L2 norm of A \t  0.3886830007983265\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.0017644609173201571\n",
      "Train:\t Perf B \t 0.0023496687573045325\n",
      "Train:\t Perf I \t 0.001144963983554566\n",
      "Train:\t L2 norm of A \t  0.3917390065194256\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.0039060737737470342\n",
      "Train:\t Perf B \t 0.002223337674688979\n",
      "Train:\t Perf I \t 0.001068382489885625\n",
      "Train:\t L2 norm of A \t  0.36558831204502834\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.002533234162655068\n",
      "Train:\t Perf B \t 0.00238630731219556\n",
      "Train:\t Perf I \t 0.0012886415435974627\n",
      "Train:\t L2 norm of A \t  0.358484355241431\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.0017511903126473437\n",
      "Train:\t Perf B \t 0.0023543221620641462\n",
      "Train:\t Perf I \t 0.0011172555254110555\n",
      "Train:\t L2 norm of A \t  0.3685502295310499\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.002288790089679523\n",
      "Train:\t Perf B \t 0.0023823589377921822\n",
      "Train:\t Perf I \t 0.0011822778879932532\n",
      "Train:\t L2 norm of A \t  0.37110955324746975\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.0038280484394414443\n",
      "Train:\t Perf B \t 0.002306027493031104\n",
      "Train:\t Perf I \t 0.0011953213343434172\n",
      "Train:\t L2 norm of A \t  0.36907293071512043\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.0017217447093966958\n",
      "Train:\t Perf B \t 0.002248663664469284\n",
      "Train:\t Perf I \t 0.0011802678586966258\n",
      "Train:\t L2 norm of A \t  0.3667471476732484\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.0032023942635623927\n",
      "Train:\t Perf B \t 0.002349186861056359\n",
      "Train:\t Perf I \t 0.001099791668476019\n",
      "Train:\t L2 norm of A \t  0.41378192231904226\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.002303152439532549\n",
      "Train:\t Perf B \t 0.002326243475812641\n",
      "Train:\t Perf I \t 0.0010836547852340402\n",
      "Train:\t L2 norm of A \t  0.3669364676820802\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.001934625416675474\n",
      "Train:\t Perf B \t 0.00224508023417361\n",
      "Train:\t Perf I \t 0.0010581562497699655\n",
      "Train:\t L2 norm of A \t  0.3871492772100115\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.0027459556791302516\n",
      "Train:\t Perf B \t 0.0023919342624378823\n",
      "Train:\t Perf I \t 0.0011508774752227852\n",
      "Train:\t L2 norm of A \t  0.36765959130976134\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.001954301275183873\n",
      "Train:\t Perf B \t 0.0022184905340790653\n",
      "Train:\t Perf I \t 0.0010066710576423915\n",
      "Train:\t L2 norm of A \t  0.39492715376043974\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.001695977741761745\n",
      "Train:\t Perf B \t 0.002298226528318741\n",
      "Train:\t Perf I \t 0.0011705735960362426\n",
      "Train:\t L2 norm of A \t  0.38630444476781267\n",
      "-------------- The gamma 0.200000\n",
      "Train:\tLoss \t  0.0019547042404520036\n",
      "Train:\t Perf B \t 0.0023088018947234894\n",
      "Train:\t Perf I \t 0.0012033969294999356\n",
      "Train:\t L2 norm of A \t  0.3776491351284214\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.002306638833348382\n",
      "Train:\t Perf B \t 0.002244967696796887\n",
      "Train:\t Perf I \t 0.0011578180474502958\n",
      "Train:\t L2 norm of A \t  0.3948723999310492\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0015606092138244082\n",
      "Train:\t Perf B \t 0.0022442005098106685\n",
      "Train:\t Perf I \t 0.0011169264213955865\n",
      "Train:\t L2 norm of A \t  0.40722136347329396\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0027379647480573113\n",
      "Train:\t Perf B \t 0.002283805550113172\n",
      "Train:\t Perf I \t 0.0010748094651554653\n",
      "Train:\t L2 norm of A \t  0.37522127849086295\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0028471817374462333\n",
      "Train:\t Perf B \t 0.0020956802735785764\n",
      "Train:\t Perf I \t 0.001102546463661804\n",
      "Train:\t L2 norm of A \t  0.39591248407137797\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0026120224195650596\n",
      "Train:\t Perf B \t 0.002207878040559465\n",
      "Train:\t Perf I \t 0.00106543032200668\n",
      "Train:\t L2 norm of A \t  0.38049392426952605\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0017556311387437627\n",
      "Train:\t Perf B \t 0.0021458609815872822\n",
      "Train:\t Perf I \t 0.0011555130159053237\n",
      "Train:\t L2 norm of A \t  0.3898028685797616\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0017523649100488565\n",
      "Train:\t Perf B \t 0.0023016624409325035\n",
      "Train:\t Perf I \t 0.0011699893477327826\n",
      "Train:\t L2 norm of A \t  0.40810183034041814\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0018848768603203297\n",
      "Train:\t Perf B \t 0.0023530973462571178\n",
      "Train:\t Perf I \t 0.0011894133573606019\n",
      "Train:\t L2 norm of A \t  0.35351925249980454\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0028316804897129733\n",
      "Train:\t Perf B \t 0.002301498473382499\n",
      "Train:\t Perf I \t 0.0011433214534412415\n",
      "Train:\t L2 norm of A \t  0.4190974908984911\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.004189678668421059\n",
      "Train:\t Perf B \t 0.0022760607558520132\n",
      "Train:\t Perf I \t 0.0011015350140906395\n",
      "Train:\t L2 norm of A \t  0.3833542442847323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.003356608860729558\n",
      "Train:\t Perf B \t 0.0025611236269137633\n",
      "Train:\t Perf I \t 0.0013019517019424987\n",
      "Train:\t L2 norm of A \t  0.37621472661497424\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.002402057447965688\n",
      "Train:\t Perf B \t 0.0023797320052673754\n",
      "Train:\t Perf I \t 0.0011489472788770206\n",
      "Train:\t L2 norm of A \t  0.38070995285309744\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0015654472990683861\n",
      "Train:\t Perf B \t 0.0023067983212595186\n",
      "Train:\t Perf I \t 0.0011612608813202144\n",
      "Train:\t L2 norm of A \t  0.35771441443163887\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.003504707754305896\n",
      "Train:\t Perf B \t 0.00220970418803593\n",
      "Train:\t Perf I \t 0.0011524602247394084\n",
      "Train:\t L2 norm of A \t  0.40611073386162994\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.00395511688466207\n",
      "Train:\t Perf B \t 0.0021253155312874125\n",
      "Train:\t Perf I \t 0.001081011648890851\n",
      "Train:\t L2 norm of A \t  0.3945955457737022\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0035753977876894767\n",
      "Train:\t Perf B \t 0.0022894664120278164\n",
      "Train:\t Perf I \t 0.0012064261178948786\n",
      "Train:\t L2 norm of A \t  0.3787650522834028\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0018237566025886655\n",
      "Train:\t Perf B \t 0.0022234152834793314\n",
      "Train:\t Perf I \t 0.0011168041984333731\n",
      "Train:\t L2 norm of A \t  0.4054934641814977\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0020904729162760298\n",
      "Train:\t Perf B \t 0.0022672316938010567\n",
      "Train:\t Perf I \t 0.0011314139463747914\n",
      "Train:\t L2 norm of A \t  0.39384622546061976\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.002969520039171292\n",
      "Train:\t Perf B \t 0.002185362145006576\n",
      "Train:\t Perf I \t 0.001073314540283021\n",
      "Train:\t L2 norm of A \t  0.38553789878580697\n",
      "-------------- The gamma 0.210000\n",
      "Train:\tLoss \t  0.0034351639302765964\n",
      "Train:\t Perf B \t 0.0021825293597912913\n",
      "Train:\t Perf I \t 0.0010721970749271244\n",
      "Train:\t L2 norm of A \t  0.4079439876503763\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.0019385771456999325\n",
      "Train:\t Perf B \t 0.002445914159130699\n",
      "Train:\t Perf I \t 0.0012522714559539071\n",
      "Train:\t L2 norm of A \t  0.37917491617705745\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.0016051356305762177\n",
      "Train:\t Perf B \t 0.0023242361007953815\n",
      "Train:\t Perf I \t 0.0011304397800137992\n",
      "Train:\t L2 norm of A \t  0.3888817644986917\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.0026671988275161207\n",
      "Train:\t Perf B \t 0.0022833879842555586\n",
      "Train:\t Perf I \t 0.001114169436783258\n",
      "Train:\t L2 norm of A \t  0.38148664853963105\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.003054134800748118\n",
      "Train:\t Perf B \t 0.0022730379070594183\n",
      "Train:\t Perf I \t 0.0010793427807006908\n",
      "Train:\t L2 norm of A \t  0.35779417704601557\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.004392178113031594\n",
      "Train:\t Perf B \t 0.0021628528382696332\n",
      "Train:\t Perf I \t 0.0010418925156111477\n",
      "Train:\t L2 norm of A \t  0.3842544221872069\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.002409429672770009\n",
      "Train:\t Perf B \t 0.0023974407777117767\n",
      "Train:\t Perf I \t 0.0011348841016732157\n",
      "Train:\t L2 norm of A \t  0.37735649149341405\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.002015447786267789\n",
      "Train:\t Perf B \t 0.002271404504461841\n",
      "Train:\t Perf I \t 0.0011112040494875174\n",
      "Train:\t L2 norm of A \t  0.3894325605125266\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.0034551348860939156\n",
      "Train:\t Perf B \t 0.0022243374541224565\n",
      "Train:\t Perf I \t 0.0011153012688416865\n",
      "Train:\t L2 norm of A \t  0.3824221711625991\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.0026796146467447644\n",
      "Train:\t Perf B \t 0.002180333579113116\n",
      "Train:\t Perf I \t 0.0011153371740959535\n",
      "Train:\t L2 norm of A \t  0.40262017242522113\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.0015058722403523646\n",
      "Train:\t Perf B \t 0.0021435289852504533\n",
      "Train:\t Perf I \t 0.0011196086955552274\n",
      "Train:\t L2 norm of A \t  0.39556725239085516\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.00400574730829171\n",
      "Train:\t Perf B \t 0.0023905130985649513\n",
      "Train:\t Perf I \t 0.0011268374881030644\n",
      "Train:\t L2 norm of A \t  0.3618681591389163\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.003085174749378838\n",
      "Train:\t Perf B \t 0.002406788429270595\n",
      "Train:\t Perf I \t 0.001250587150123446\n",
      "Train:\t L2 norm of A \t  0.35649200339136566\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.0019284023658825292\n",
      "Train:\t Perf B \t 0.002338635193856976\n",
      "Train:\t Perf I \t 0.0013007448339107384\n",
      "Train:\t L2 norm of A \t  0.3624170725050621\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.0037602251149577003\n",
      "Train:\t Perf B \t 0.002043844013441274\n",
      "Train:\t Perf I \t 0.001028930356183936\n",
      "Train:\t L2 norm of A \t  0.40535637209786723\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.003379399903499976\n",
      "Train:\t Perf B \t 0.0021868089364878306\n",
      "Train:\t Perf I \t 0.0010860194125401593\n",
      "Train:\t L2 norm of A \t  0.3983787835024386\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.002353805061631523\n",
      "Train:\t Perf B \t 0.0022820957068878994\n",
      "Train:\t Perf I \t 0.0010315401845224156\n",
      "Train:\t L2 norm of A \t  0.37774152769378616\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.001859505152433967\n",
      "Train:\t Perf B \t 0.002421519680316765\n",
      "Train:\t Perf I \t 0.0012316963308790954\n",
      "Train:\t L2 norm of A \t  0.3585048801266967\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.0033442636175915028\n",
      "Train:\t Perf B \t 0.00220741369987537\n",
      "Train:\t Perf I \t 0.0011308212395015334\n",
      "Train:\t L2 norm of A \t  0.3979648808731968\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.0016232462173414357\n",
      "Train:\t Perf B \t 0.0023179226503150614\n",
      "Train:\t Perf I \t 0.0011537852043345888\n",
      "Train:\t L2 norm of A \t  0.3879500988785213\n",
      "-------------- The gamma 0.220000\n",
      "Train:\tLoss \t  0.004367611947840488\n",
      "Train:\t Perf B \t 0.0022534148962758496\n",
      "Train:\t Perf I \t 0.0011571151642953172\n",
      "Train:\t L2 norm of A \t  0.38907782137956226\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.002898170939866639\n",
      "Train:\t Perf B \t 0.002119962513617498\n",
      "Train:\t Perf I \t 0.0010649940157508862\n",
      "Train:\t L2 norm of A \t  0.413326630782514\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.003458132301952331\n",
      "Train:\t Perf B \t 0.0022398644619041007\n",
      "Train:\t Perf I \t 0.0010684386401760893\n",
      "Train:\t L2 norm of A \t  0.40228161047714645\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.0037317584445467713\n",
      "Train:\t Perf B \t 0.0022628331700944217\n",
      "Train:\t Perf I \t 0.0011430526222848817\n",
      "Train:\t L2 norm of A \t  0.3816255572543344\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.003512257165888236\n",
      "Train:\t Perf B \t 0.0023091636490843418\n",
      "Train:\t Perf I \t 0.001218543370386775\n",
      "Train:\t L2 norm of A \t  0.3814991364693229\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.001692430680384405\n",
      "Train:\t Perf B \t 0.002312053800245037\n",
      "Train:\t Perf I \t 0.0010580456951465981\n",
      "Train:\t L2 norm of A \t  0.36540484626598313\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.0015569471465496922\n",
      "Train:\t Perf B \t 0.002287174781802763\n",
      "Train:\t Perf I \t 0.0011752846493395853\n",
      "Train:\t L2 norm of A \t  0.37295067406630705\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.003209316398426868\n",
      "Train:\t Perf B \t 0.0022581771233741435\n",
      "Train:\t Perf I \t 0.001106374933680553\n",
      "Train:\t L2 norm of A \t  0.40407454955150685\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.004309968903519442\n",
      "Train:\t Perf B \t 0.0022335338685805415\n",
      "Train:\t Perf I \t 0.0011658227116684312\n",
      "Train:\t L2 norm of A \t  0.3852071504993223\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.002053580637535465\n",
      "Train:\t Perf B \t 0.002251882355151591\n",
      "Train:\t Perf I \t 0.0011594358788385134\n",
      "Train:\t L2 norm of A \t  0.38118441614699217\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.0025800112154078774\n",
      "Train:\t Perf B \t 0.0022542683127239438\n",
      "Train:\t Perf I \t 0.0010580314496351254\n",
      "Train:\t L2 norm of A \t  0.3812438074449639\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.0027376445335975204\n",
      "Train:\t Perf B \t 0.0021788021218675704\n",
      "Train:\t Perf I \t 0.0011076991150945983\n",
      "Train:\t L2 norm of A \t  0.4147323910015142\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.0038749971273957075\n",
      "Train:\t Perf B \t 0.002227284804608794\n",
      "Train:\t Perf I \t 0.0012063017201345177\n",
      "Train:\t L2 norm of A \t  0.39818169312954044\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.002748024470317865\n",
      "Train:\t Perf B \t 0.002134837230537153\n",
      "Train:\t Perf I \t 0.0009846194612901592\n",
      "Train:\t L2 norm of A \t  0.42327868055399404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.002350306274776129\n",
      "Train:\t Perf B \t 0.002250939030551127\n",
      "Train:\t Perf I \t 0.0011027362825550256\n",
      "Train:\t L2 norm of A \t  0.3826266530778033\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.002335819266288477\n",
      "Train:\t Perf B \t 0.0023671019056838963\n",
      "Train:\t Perf I \t 0.001198941062063292\n",
      "Train:\t L2 norm of A \t  0.37615309013320286\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.0037085273686947046\n",
      "Train:\t Perf B \t 0.002230394687525357\n",
      "Train:\t Perf I \t 0.0011262616659620706\n",
      "Train:\t L2 norm of A \t  0.3866147353038195\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.0029950489565054466\n",
      "Train:\t Perf B \t 0.00239218922234011\n",
      "Train:\t Perf I \t 0.0012147778731830662\n",
      "Train:\t L2 norm of A \t  0.38672612460183675\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.0034270506459106423\n",
      "Train:\t Perf B \t 0.0021985328227387907\n",
      "Train:\t Perf I \t 0.0010694418971599244\n",
      "Train:\t L2 norm of A \t  0.4102194135001264\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.0020071032708875117\n",
      "Train:\t Perf B \t 0.0022311747031065142\n",
      "Train:\t Perf I \t 0.0010160594966983868\n",
      "Train:\t L2 norm of A \t  0.4062854459477687\n",
      "-------------- The gamma 0.230000\n",
      "Train:\tLoss \t  0.0016133792997288542\n",
      "Train:\t Perf B \t 0.0022776117182206524\n",
      "Train:\t Perf I \t 0.0010308695965296905\n",
      "Train:\t L2 norm of A \t  0.37520068648345156\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0017217881931560263\n",
      "Train:\t Perf B \t 0.002252086995992959\n",
      "Train:\t Perf I \t 0.00104276613914544\n",
      "Train:\t L2 norm of A \t  0.3858271242762119\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0047282848829562464\n",
      "Train:\t Perf B \t 0.0022831713727779032\n",
      "Train:\t Perf I \t 0.0011223118885196296\n",
      "Train:\t L2 norm of A \t  0.3707129657492894\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.003786748874461291\n",
      "Train:\t Perf B \t 0.002371371259250517\n",
      "Train:\t Perf I \t 0.0012260042171504295\n",
      "Train:\t L2 norm of A \t  0.38770639701193554\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.002167884651632322\n",
      "Train:\t Perf B \t 0.002115515566145895\n",
      "Train:\t Perf I \t 0.001024883032103806\n",
      "Train:\t L2 norm of A \t  0.4434725962446077\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0017458862467071238\n",
      "Train:\t Perf B \t 0.002380896452467956\n",
      "Train:\t Perf I \t 0.0010564137139419834\n",
      "Train:\t L2 norm of A \t  0.3542034301653011\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.002350254561596916\n",
      "Train:\t Perf B \t 0.002158528696583032\n",
      "Train:\t Perf I \t 0.001154551288182755\n",
      "Train:\t L2 norm of A \t  0.403017177125378\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.003976753937481331\n",
      "Train:\t Perf B \t 0.00216160669458126\n",
      "Train:\t Perf I \t 0.0011510427134423169\n",
      "Train:\t L2 norm of A \t  0.411226044771087\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0030677853124897437\n",
      "Train:\t Perf B \t 0.0021254471120628927\n",
      "Train:\t Perf I \t 0.001074625672206864\n",
      "Train:\t L2 norm of A \t  0.39660314385285755\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.002491945839234204\n",
      "Train:\t Perf B \t 0.002173787441418365\n",
      "Train:\t Perf I \t 0.0009892097481275252\n",
      "Train:\t L2 norm of A \t  0.39592166886262614\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0021986858765643325\n",
      "Train:\t Perf B \t 0.0023247368979060243\n",
      "Train:\t Perf I \t 0.0011489337899022328\n",
      "Train:\t L2 norm of A \t  0.3678143714547708\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0016132567322052838\n",
      "Train:\t Perf B \t 0.0022128661481335475\n",
      "Train:\t Perf I \t 0.0011568842870740342\n",
      "Train:\t L2 norm of A \t  0.40243701597794457\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0019235855030090402\n",
      "Train:\t Perf B \t 0.002141649827465132\n",
      "Train:\t Perf I \t 0.0010464849104017972\n",
      "Train:\t L2 norm of A \t  0.3972458390265321\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.002335579130443398\n",
      "Train:\t Perf B \t 0.0020778584146068447\n",
      "Train:\t Perf I \t 0.001056306617467494\n",
      "Train:\t L2 norm of A \t  0.4197881993190247\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0020516673185425457\n",
      "Train:\t Perf B \t 0.0022837435594444816\n",
      "Train:\t Perf I \t 0.0011289241529483713\n",
      "Train:\t L2 norm of A \t  0.3976636017720666\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.005584304445667024\n",
      "Train:\t Perf B \t 0.0023835804904240813\n",
      "Train:\t Perf I \t 0.0011894034015708698\n",
      "Train:\t L2 norm of A \t  0.4213581568699419\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0015791842199974104\n",
      "Train:\t Perf B \t 0.002169847347494784\n",
      "Train:\t Perf I \t 0.0011046740300201122\n",
      "Train:\t L2 norm of A \t  0.4052865287097596\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0026113711583293816\n",
      "Train:\t Perf B \t 0.0021895275117595407\n",
      "Train:\t Perf I \t 0.0011380172618743545\n",
      "Train:\t L2 norm of A \t  0.3711183238795579\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0014821274444529002\n",
      "Train:\t Perf B \t 0.0021273905685330083\n",
      "Train:\t Perf I \t 0.0011528222631056238\n",
      "Train:\t L2 norm of A \t  0.3885345568557227\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0054102936437020015\n",
      "Train:\t Perf B \t 0.0021234746886373756\n",
      "Train:\t Perf I \t 0.0010538163384334633\n",
      "Train:\t L2 norm of A \t  0.40753035883839084\n",
      "-------------- The gamma 0.240000\n",
      "Train:\tLoss \t  0.0043458277577739\n",
      "Train:\t Perf B \t 0.002121329980666794\n",
      "Train:\t Perf I \t 0.0011137961656921037\n",
      "Train:\t L2 norm of A \t  0.4057442105143243\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0030086885205775566\n",
      "Train:\t Perf B \t 0.0021114685556080917\n",
      "Train:\t Perf I \t 0.0009693096477709535\n",
      "Train:\t L2 norm of A \t  0.4202242619183468\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0041161473865372975\n",
      "Train:\t Perf B \t 0.002245342034526794\n",
      "Train:\t Perf I \t 0.0010876667623115695\n",
      "Train:\t L2 norm of A \t  0.42666273935234844\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0061699632788124415\n",
      "Train:\t Perf B \t 0.0021382675323467822\n",
      "Train:\t Perf I \t 0.001155438995103574\n",
      "Train:\t L2 norm of A \t  0.40346398227248276\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0017445124542341947\n",
      "Train:\t Perf B \t 0.002114337219646419\n",
      "Train:\t Perf I \t 0.0010640376606714597\n",
      "Train:\t L2 norm of A \t  0.39183526936675317\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.005125540231979297\n",
      "Train:\t Perf B \t 0.002099411059237803\n",
      "Train:\t Perf I \t 0.0011973131449397338\n",
      "Train:\t L2 norm of A \t  0.40904550173043264\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0019921175853201125\n",
      "Train:\t Perf B \t 0.002294528533515036\n",
      "Train:\t Perf I \t 0.0011137907787488652\n",
      "Train:\t L2 norm of A \t  0.38564135264614374\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0030517943557842406\n",
      "Train:\t Perf B \t 0.002142170882319197\n",
      "Train:\t Perf I \t 0.0010612825720122537\n",
      "Train:\t L2 norm of A \t  0.42344556279084133\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.001549341925835718\n",
      "Train:\t Perf B \t 0.0022468012418075806\n",
      "Train:\t Perf I \t 0.0010418261665174186\n",
      "Train:\t L2 norm of A \t  0.3918531147087546\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0025077415165124174\n",
      "Train:\t Perf B \t 0.0021356090859119747\n",
      "Train:\t Perf I \t 0.001162217630936864\n",
      "Train:\t L2 norm of A \t  0.4151822150972172\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0029472758606898892\n",
      "Train:\t Perf B \t 0.0021286176100786224\n",
      "Train:\t Perf I \t 0.0010398022886930556\n",
      "Train:\t L2 norm of A \t  0.41308158452868543\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0034539255220430855\n",
      "Train:\t Perf B \t 0.0023155723317893886\n",
      "Train:\t Perf I \t 0.0011526435050559904\n",
      "Train:\t L2 norm of A \t  0.38763772090105797\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.004597162706142161\n",
      "Train:\t Perf B \t 0.0022169048024894794\n",
      "Train:\t Perf I \t 0.0011778065532910315\n",
      "Train:\t L2 norm of A \t  0.440170581177659\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0017221752566163736\n",
      "Train:\t Perf B \t 0.0022778702657041635\n",
      "Train:\t Perf I \t 0.0011304863970225624\n",
      "Train:\t L2 norm of A \t  0.3773060530597621\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.001848671291213583\n",
      "Train:\t Perf B \t 0.002039708377513799\n",
      "Train:\t Perf I \t 0.0010118637765276128\n",
      "Train:\t L2 norm of A \t  0.43383694619315133\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0032986931996436264\n",
      "Train:\t Perf B \t 0.002189985947480551\n",
      "Train:\t Perf I \t 0.0011044610682013885\n",
      "Train:\t L2 norm of A \t  0.4189474767338097\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.004903186654507247\n",
      "Train:\t Perf B \t 0.002246456003492839\n",
      "Train:\t Perf I \t 0.001074311650456521\n",
      "Train:\t L2 norm of A \t  0.4258928126251609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0019143187727939481\n",
      "Train:\t Perf B \t 0.0023739533604233845\n",
      "Train:\t Perf I \t 0.0012088056086132495\n",
      "Train:\t L2 norm of A \t  0.37243440740376793\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0037993592464488305\n",
      "Train:\t Perf B \t 0.002160974400902761\n",
      "Train:\t Perf I \t 0.0010942700800226593\n",
      "Train:\t L2 norm of A \t  0.4208985651215131\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.0015383190201412288\n",
      "Train:\t Perf B \t 0.0023136451026011806\n",
      "Train:\t Perf I \t 0.001149900861462484\n",
      "Train:\t L2 norm of A \t  0.36808982285684894\n",
      "-------------- The gamma 0.250000\n",
      "Train:\tLoss \t  0.002015899468233424\n",
      "Train:\t Perf B \t 0.0022540653560132163\n",
      "Train:\t Perf I \t 0.0011126220971668123\n",
      "Train:\t L2 norm of A \t  0.4118713751611362\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.004739654193891154\n",
      "Train:\t Perf B \t 0.002152628036897669\n",
      "Train:\t Perf I \t 0.001029464031879648\n",
      "Train:\t L2 norm of A \t  0.40724334448379407\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.0023016603188467377\n",
      "Train:\t Perf B \t 0.00229756647160178\n",
      "Train:\t Perf I \t 0.0012320635111328984\n",
      "Train:\t L2 norm of A \t  0.39086983885657095\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.0035236613955047643\n",
      "Train:\t Perf B \t 0.002355835449028498\n",
      "Train:\t Perf I \t 0.0010955866301138924\n",
      "Train:\t L2 norm of A \t  0.3775784684830234\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.002836961678881049\n",
      "Train:\t Perf B \t 0.002133005874126075\n",
      "Train:\t Perf I \t 0.0011519328630215056\n",
      "Train:\t L2 norm of A \t  0.4032046486738781\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.002279650334504495\n",
      "Train:\t Perf B \t 0.0023355635743695633\n",
      "Train:\t Perf I \t 0.0012016389983317386\n",
      "Train:\t L2 norm of A \t  0.3652692156645479\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.003657481689148055\n",
      "Train:\t Perf B \t 0.0021817752212768327\n",
      "Train:\t Perf I \t 0.0010428582841464275\n",
      "Train:\t L2 norm of A \t  0.4115083826338802\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.0022559664311823722\n",
      "Train:\t Perf B \t 0.0022810761779844774\n",
      "Train:\t Perf I \t 0.0012862380987836246\n",
      "Train:\t L2 norm of A \t  0.3828464261674329\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.0024381022537060567\n",
      "Train:\t Perf B \t 0.0022186214517627384\n",
      "Train:\t Perf I \t 0.001175750134584809\n",
      "Train:\t L2 norm of A \t  0.42163080589910773\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.005717890422000987\n",
      "Train:\t Perf B \t 0.0018482481034760483\n",
      "Train:\t Perf I \t 0.000950719682535197\n",
      "Train:\t L2 norm of A \t  0.4256074549525069\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.0022508943528184904\n",
      "Train:\t Perf B \t 0.002127369005398559\n",
      "Train:\t Perf I \t 0.001119908065453383\n",
      "Train:\t L2 norm of A \t  0.415859713860187\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.004368017093190265\n",
      "Train:\t Perf B \t 0.0021815235105778577\n",
      "Train:\t Perf I \t 0.0011796757772756848\n",
      "Train:\t L2 norm of A \t  0.4088086123527639\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.002677685211776706\n",
      "Train:\t Perf B \t 0.002127452030783093\n",
      "Train:\t Perf I \t 0.0010963432551578892\n",
      "Train:\t L2 norm of A \t  0.42186451620013893\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.001744811069082105\n",
      "Train:\t Perf B \t 0.002292834384297831\n",
      "Train:\t Perf I \t 0.001068352444158033\n",
      "Train:\t L2 norm of A \t  0.37237294829416817\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.002699694085203112\n",
      "Train:\t Perf B \t 0.0021700579733702524\n",
      "Train:\t Perf I \t 0.001107451341776309\n",
      "Train:\t L2 norm of A \t  0.42316424505610556\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.001503163709119755\n",
      "Train:\t Perf B \t 0.002147490568941165\n",
      "Train:\t Perf I \t 0.0010735160123477446\n",
      "Train:\t L2 norm of A \t  0.39750120622345936\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.00473358005048136\n",
      "Train:\t Perf B \t 0.0020710602117550653\n",
      "Train:\t Perf I \t 0.0011301187427282609\n",
      "Train:\t L2 norm of A \t  0.40453691448615753\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.0038844393556557756\n",
      "Train:\t Perf B \t 0.0020654403436921385\n",
      "Train:\t Perf I \t 0.0008851803121452936\n",
      "Train:\t L2 norm of A \t  0.4174161068706826\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.0027783028669114383\n",
      "Train:\t Perf B \t 0.0023718671036782233\n",
      "Train:\t Perf I \t 0.0011909213515284385\n",
      "Train:\t L2 norm of A \t  0.3716503849518384\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.004408888498093281\n",
      "Train:\t Perf B \t 0.0020309908789637714\n",
      "Train:\t Perf I \t 0.0010569730531913791\n",
      "Train:\t L2 norm of A \t  0.3962689392150418\n",
      "-------------- The gamma 0.260000\n",
      "Train:\tLoss \t  0.003034945313548751\n",
      "Train:\t Perf B \t 0.002201392055529147\n",
      "Train:\t Perf I \t 0.0010542122912398555\n",
      "Train:\t L2 norm of A \t  0.4183778692608041\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.0030783881856965268\n",
      "Train:\t Perf B \t 0.002014978663845282\n",
      "Train:\t Perf I \t 0.001080613804881654\n",
      "Train:\t L2 norm of A \t  0.41487907564362786\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.002141945474366523\n",
      "Train:\t Perf B \t 0.0021027547333219965\n",
      "Train:\t Perf I \t 0.0010747088788757512\n",
      "Train:\t L2 norm of A \t  0.4118779261640426\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.0027835141591191857\n",
      "Train:\t Perf B \t 0.0023356940399386354\n",
      "Train:\t Perf I \t 0.001153966305341123\n",
      "Train:\t L2 norm of A \t  0.3639893207247546\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.0028965071737602076\n",
      "Train:\t Perf B \t 0.0021309394712860485\n",
      "Train:\t Perf I \t 0.0010424501289989672\n",
      "Train:\t L2 norm of A \t  0.45150965116906333\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.0014666239263003101\n",
      "Train:\t Perf B \t 0.0023295946433599854\n",
      "Train:\t Perf I \t 0.001223935527461539\n",
      "Train:\t L2 norm of A \t  0.384128674266952\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.0034424214216414433\n",
      "Train:\t Perf B \t 0.0020249410269564515\n",
      "Train:\t Perf I \t 0.0009966662374056333\n",
      "Train:\t L2 norm of A \t  0.41249437083390983\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.004707207012961315\n",
      "Train:\t Perf B \t 0.001897941819403281\n",
      "Train:\t Perf I \t 0.0008927948761031356\n",
      "Train:\t L2 norm of A \t  0.46629611248039016\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.004913828565026917\n",
      "Train:\t Perf B \t 0.0021345819466590185\n",
      "Train:\t Perf I \t 0.0010312687458886654\n",
      "Train:\t L2 norm of A \t  0.4113225167882657\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.0018142183257106675\n",
      "Train:\t Perf B \t 0.0022901290229538395\n",
      "Train:\t Perf I \t 0.0010717470894466882\n",
      "Train:\t L2 norm of A \t  0.3815445583681528\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.001735523475840275\n",
      "Train:\t Perf B \t 0.0022943330747451332\n",
      "Train:\t Perf I \t 0.0011753219913090275\n",
      "Train:\t L2 norm of A \t  0.3637798417485755\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.0038768460720951443\n",
      "Train:\t Perf B \t 0.0020455003079762704\n",
      "Train:\t Perf I \t 0.0009758069460573059\n",
      "Train:\t L2 norm of A \t  0.43345784828209266\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.00402258490294899\n",
      "Train:\t Perf B \t 0.0020617820401429847\n",
      "Train:\t Perf I \t 0.0010491926060050571\n",
      "Train:\t L2 norm of A \t  0.41429310720357665\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.004823498277566009\n",
      "Train:\t Perf B \t 0.0021027472132182866\n",
      "Train:\t Perf I \t 0.0009234742375718603\n",
      "Train:\t L2 norm of A \t  0.4550361828860646\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.004289281713606834\n",
      "Train:\t Perf B \t 0.002062346544272235\n",
      "Train:\t Perf I \t 0.0009784706786213225\n",
      "Train:\t L2 norm of A \t  0.4255746783981772\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.003925796368760226\n",
      "Train:\t Perf B \t 0.0021374595982232387\n",
      "Train:\t Perf I \t 0.0011583786144423857\n",
      "Train:\t L2 norm of A \t  0.4237733660087264\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.0038722512508078217\n",
      "Train:\t Perf B \t 0.002318508422197249\n",
      "Train:\t Perf I \t 0.0010659273204572971\n",
      "Train:\t L2 norm of A \t  0.3760442914836492\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.004996766399292674\n",
      "Train:\t Perf B \t 0.0022875621310518342\n",
      "Train:\t Perf I \t 0.0011230826505759471\n",
      "Train:\t L2 norm of A \t  0.39355374404025023\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.0016578117275541625\n",
      "Train:\t Perf B \t 0.0020854883212489844\n",
      "Train:\t Perf I \t 0.0010743943931732806\n",
      "Train:\t L2 norm of A \t  0.41156287149809895\n",
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.0029904012723139207\n",
      "Train:\t Perf B \t 0.0020999226587670836\n",
      "Train:\t Perf I \t 0.001025445097061819\n",
      "Train:\t L2 norm of A \t  0.405971134519024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.270000\n",
      "Train:\tLoss \t  0.004826601242418027\n",
      "Train:\t Perf B \t 0.0019925343676606448\n",
      "Train:\t Perf I \t 0.000993298919327169\n",
      "Train:\t L2 norm of A \t  0.432923876643488\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.0026176137595372052\n",
      "Train:\t Perf B \t 0.002003783802353428\n",
      "Train:\t Perf I \t 0.0010089156896736437\n",
      "Train:\t L2 norm of A \t  0.44555505527599093\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.0030394199746993774\n",
      "Train:\t Perf B \t 0.0021059191255540146\n",
      "Train:\t Perf I \t 0.0009977779444928763\n",
      "Train:\t L2 norm of A \t  0.4383702769923851\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.0035578855686932273\n",
      "Train:\t Perf B \t 0.002144101945555159\n",
      "Train:\t Perf I \t 0.0010877331596982176\n",
      "Train:\t L2 norm of A \t  0.43207974051778464\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.004051851175989841\n",
      "Train:\t Perf B \t 0.0021858845373898214\n",
      "Train:\t Perf I \t 0.0011075228042458165\n",
      "Train:\t L2 norm of A \t  0.4037375983398484\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.0037356649222782504\n",
      "Train:\t Perf B \t 0.0022311679486050886\n",
      "Train:\t Perf I \t 0.001027507348550628\n",
      "Train:\t L2 norm of A \t  0.4042031740835379\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.0036845702724975942\n",
      "Train:\t Perf B \t 0.0020558188484869474\n",
      "Train:\t Perf I \t 0.0009691696601481349\n",
      "Train:\t L2 norm of A \t  0.42711041602049615\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.0033088519742025856\n",
      "Train:\t Perf B \t 0.0019669296110287914\n",
      "Train:\t Perf I \t 0.001010531358004465\n",
      "Train:\t L2 norm of A \t  0.42529551032995677\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.003480072669720529\n",
      "Train:\t Perf B \t 0.002197794737341031\n",
      "Train:\t Perf I \t 0.0010635129711629165\n",
      "Train:\t L2 norm of A \t  0.41369537100670206\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.002085362522694353\n",
      "Train:\t Perf B \t 0.0019593333895739804\n",
      "Train:\t Perf I \t 0.0009723812318566399\n",
      "Train:\t L2 norm of A \t  0.4512675282574575\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.0013985553087373839\n",
      "Train:\t Perf B \t 0.002091606686862297\n",
      "Train:\t Perf I \t 0.0009880754904474521\n",
      "Train:\t L2 norm of A \t  0.409253637284586\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.0016722077952589021\n",
      "Train:\t Perf B \t 0.002156308118336945\n",
      "Train:\t Perf I \t 0.0010779138441160184\n",
      "Train:\t L2 norm of A \t  0.39776387409461356\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.0017052816717219805\n",
      "Train:\t Perf B \t 0.002001045004301947\n",
      "Train:\t Perf I \t 0.0011136946932738822\n",
      "Train:\t L2 norm of A \t  0.4228913524270018\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.004046394520535768\n",
      "Train:\t Perf B \t 0.002173450370453104\n",
      "Train:\t Perf I \t 0.0010661062863006805\n",
      "Train:\t L2 norm of A \t  0.4213230299062748\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.0034323430811608055\n",
      "Train:\t Perf B \t 0.0022325357128076635\n",
      "Train:\t Perf I \t 0.0010440162383889928\n",
      "Train:\t L2 norm of A \t  0.4189052071358869\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.0039547512895092216\n",
      "Train:\t Perf B \t 0.002303321376870042\n",
      "Train:\t Perf I \t 0.001093086141200039\n",
      "Train:\t L2 norm of A \t  0.3869994318493523\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.004186636580801952\n",
      "Train:\t Perf B \t 0.0021164053170708882\n",
      "Train:\t Perf I \t 0.001110736814642168\n",
      "Train:\t L2 norm of A \t  0.41296207205942503\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.003581318157820751\n",
      "Train:\t Perf B \t 0.002219023612810642\n",
      "Train:\t Perf I \t 0.001115761251254274\n",
      "Train:\t L2 norm of A \t  0.4086915754920859\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.002119969908805393\n",
      "Train:\t Perf B \t 0.0022126367096401162\n",
      "Train:\t Perf I \t 0.0011984237908871363\n",
      "Train:\t L2 norm of A \t  0.40853547947887414\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.002732988490425074\n",
      "Train:\t Perf B \t 0.002067671419612307\n",
      "Train:\t Perf I \t 0.0009770550379151035\n",
      "Train:\t L2 norm of A \t  0.42158653987873895\n",
      "-------------- The gamma 0.280000\n",
      "Train:\tLoss \t  0.004184257529019686\n",
      "Train:\t Perf B \t 0.002029541051469149\n",
      "Train:\t Perf I \t 0.0009487808639660407\n",
      "Train:\t L2 norm of A \t  0.4437722329326235\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.0024963352643110905\n",
      "Train:\t Perf B \t 0.002027125235860063\n",
      "Train:\t Perf I \t 0.0010308973278114336\n",
      "Train:\t L2 norm of A \t  0.4304700570840326\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.005189389370142974\n",
      "Train:\t Perf B \t 0.0020378100183506176\n",
      "Train:\t Perf I \t 0.0009574866560217687\n",
      "Train:\t L2 norm of A \t  0.42763088006459854\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.0045713909158614134\n",
      "Train:\t Perf B \t 0.0019033830422802726\n",
      "Train:\t Perf I \t 0.0008710157043963721\n",
      "Train:\t L2 norm of A \t  0.4539395714937586\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.002832757808604821\n",
      "Train:\t Perf B \t 0.002251499072568743\n",
      "Train:\t Perf I \t 0.0010838956979667283\n",
      "Train:\t L2 norm of A \t  0.39814955701719773\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.0023512530674797603\n",
      "Train:\t Perf B \t 0.0020628161397265123\n",
      "Train:\t Perf I \t 0.0010051939115006243\n",
      "Train:\t L2 norm of A \t  0.4074599644484632\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.0034835479765596487\n",
      "Train:\t Perf B \t 0.0020597831371305365\n",
      "Train:\t Perf I \t 0.0010620567752189659\n",
      "Train:\t L2 norm of A \t  0.43406255362977236\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.003082158879213734\n",
      "Train:\t Perf B \t 0.0020467413388510036\n",
      "Train:\t Perf I \t 0.0009833240251963546\n",
      "Train:\t L2 norm of A \t  0.42445139920883324\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.002890337773277833\n",
      "Train:\t Perf B \t 0.002053895159151784\n",
      "Train:\t Perf I \t 0.0009569525686212712\n",
      "Train:\t L2 norm of A \t  0.43811186360356147\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.0031874680042877513\n",
      "Train:\t Perf B \t 0.0020292302286201904\n",
      "Train:\t Perf I \t 0.0009860990810500192\n",
      "Train:\t L2 norm of A \t  0.4322804867078461\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.003533450368546146\n",
      "Train:\t Perf B \t 0.0022879592849382358\n",
      "Train:\t Perf I \t 0.0010824095723383396\n",
      "Train:\t L2 norm of A \t  0.4019434778384751\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.0034229245748512633\n",
      "Train:\t Perf B \t 0.001996534902957722\n",
      "Train:\t Perf I \t 0.000916699625159669\n",
      "Train:\t L2 norm of A \t  0.4182648822955762\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.002824037467779923\n",
      "Train:\t Perf B \t 0.001970233068434815\n",
      "Train:\t Perf I \t 0.0009126702278848117\n",
      "Train:\t L2 norm of A \t  0.4445228110157099\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.0020256466115881966\n",
      "Train:\t Perf B \t 0.0022621097766028567\n",
      "Train:\t Perf I \t 0.001140311379250747\n",
      "Train:\t L2 norm of A \t  0.4066074545382035\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.004616810422164494\n",
      "Train:\t Perf B \t 0.002083771968056171\n",
      "Train:\t Perf I \t 0.001101973270818992\n",
      "Train:\t L2 norm of A \t  0.41139056626370313\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.0017355918294834226\n",
      "Train:\t Perf B \t 0.0020088267801778355\n",
      "Train:\t Perf I \t 0.0009450461868070566\n",
      "Train:\t L2 norm of A \t  0.45433969102335325\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.006494769691856203\n",
      "Train:\t Perf B \t 0.002064492064149654\n",
      "Train:\t Perf I \t 0.00105824858059674\n",
      "Train:\t L2 norm of A \t  0.45111284872833074\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.001889199637373715\n",
      "Train:\t Perf B \t 0.0019567708994550174\n",
      "Train:\t Perf I \t 0.0010608881769197393\n",
      "Train:\t L2 norm of A \t  0.4372964591358438\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.0023290206668797804\n",
      "Train:\t Perf B \t 0.0022156539762786683\n",
      "Train:\t Perf I \t 0.001031189974262834\n",
      "Train:\t L2 norm of A \t  0.46532963202128963\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.0024062239536649342\n",
      "Train:\t Perf B \t 0.001994365254803822\n",
      "Train:\t Perf I \t 0.0010350941099139927\n",
      "Train:\t L2 norm of A \t  0.4308792250717338\n",
      "-------------- The gamma 0.290000\n",
      "Train:\tLoss \t  0.0017462836138802977\n",
      "Train:\t Perf B \t 0.00203338165398655\n",
      "Train:\t Perf I \t 0.0009667336777897782\n",
      "Train:\t L2 norm of A \t  0.43408597115838016\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.0038426125205593634\n",
      "Train:\t Perf B \t 0.0020632537190067705\n",
      "Train:\t Perf I \t 0.0010149162465972318\n",
      "Train:\t L2 norm of A \t  0.44702102701236673\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.002817309529446577\n",
      "Train:\t Perf B \t 0.0021149447787886266\n",
      "Train:\t Perf I \t 0.001000982142731402\n",
      "Train:\t L2 norm of A \t  0.4477136496665978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.0029019389891535444\n",
      "Train:\t Perf B \t 0.001763046589448453\n",
      "Train:\t Perf I \t 0.0007545907676011517\n",
      "Train:\t L2 norm of A \t  0.47108095048562726\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.003955488183960895\n",
      "Train:\t Perf B \t 0.0020032016428406926\n",
      "Train:\t Perf I \t 0.0009708218475447468\n",
      "Train:\t L2 norm of A \t  0.42540020133144146\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.0019066797276490284\n",
      "Train:\t Perf B \t 0.001800323332609202\n",
      "Train:\t Perf I \t 0.0009058308720312058\n",
      "Train:\t L2 norm of A \t  0.45670547753147295\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.0018248544871932797\n",
      "Train:\t Perf B \t 0.0023535273203501967\n",
      "Train:\t Perf I \t 0.0010676899501751522\n",
      "Train:\t L2 norm of A \t  0.3864734286678378\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.0015572496095637444\n",
      "Train:\t Perf B \t 0.002040383847740134\n",
      "Train:\t Perf I \t 0.0009811999035995239\n",
      "Train:\t L2 norm of A \t  0.46206543967652036\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.0017495451799639402\n",
      "Train:\t Perf B \t 0.0018355391760952193\n",
      "Train:\t Perf I \t 0.0008539662776184651\n",
      "Train:\t L2 norm of A \t  0.5100048172903778\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.0038143598156724044\n",
      "Train:\t Perf B \t 0.0019932144653325877\n",
      "Train:\t Perf I \t 0.0009372393131363581\n",
      "Train:\t L2 norm of A \t  0.4455140499317944\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.0017900742694640284\n",
      "Train:\t Perf B \t 0.002198275933980929\n",
      "Train:\t Perf I \t 0.0009788858885326232\n",
      "Train:\t L2 norm of A \t  0.38112796103241847\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.003981409820603267\n",
      "Train:\t Perf B \t 0.002063100771941329\n",
      "Train:\t Perf I \t 0.0010701641210780852\n",
      "Train:\t L2 norm of A \t  0.4520953223050512\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.004227717565078971\n",
      "Train:\t Perf B \t 0.0019192034636756265\n",
      "Train:\t Perf I \t 0.0009483743503474752\n",
      "Train:\t L2 norm of A \t  0.4439247229773902\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.002253505785344416\n",
      "Train:\t Perf B \t 0.00222570877867511\n",
      "Train:\t Perf I \t 0.0010458116195432594\n",
      "Train:\t L2 norm of A \t  0.40235238990038963\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.002446835800211539\n",
      "Train:\t Perf B \t 0.002018817051021899\n",
      "Train:\t Perf I \t 0.001039682479963105\n",
      "Train:\t L2 norm of A \t  0.43762333643974904\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.00296146657379193\n",
      "Train:\t Perf B \t 0.0021328627108445053\n",
      "Train:\t Perf I \t 0.000991836106654681\n",
      "Train:\t L2 norm of A \t  0.40883797625774837\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.001983429548548359\n",
      "Train:\t Perf B \t 0.002138602521317162\n",
      "Train:\t Perf I \t 0.0010615954316804215\n",
      "Train:\t L2 norm of A \t  0.42255257784739725\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.005897642780115901\n",
      "Train:\t Perf B \t 0.001966902350818056\n",
      "Train:\t Perf I \t 0.0010063387991116816\n",
      "Train:\t L2 norm of A \t  0.45783043650606947\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.004878513359477249\n",
      "Train:\t Perf B \t 0.001835273510352969\n",
      "Train:\t Perf I \t 0.0008679209528828892\n",
      "Train:\t L2 norm of A \t  0.47145261560500684\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.0018872187465955018\n",
      "Train:\t Perf B \t 0.0019031408076717524\n",
      "Train:\t Perf I \t 0.0010475408937234705\n",
      "Train:\t L2 norm of A \t  0.44127571614306205\n",
      "-------------- The gamma 0.300000\n",
      "Train:\tLoss \t  0.0034033376882819815\n",
      "Train:\t Perf B \t 0.0021171442934282106\n",
      "Train:\t Perf I \t 0.0010154561428825186\n",
      "Train:\t L2 norm of A \t  0.4219446719948648\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.005827102295790178\n",
      "Train:\t Perf B \t 0.0018436662210667913\n",
      "Train:\t Perf I \t 0.0008344563928834996\n",
      "Train:\t L2 norm of A \t  0.47185373212750975\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.004241922981595307\n",
      "Train:\t Perf B \t 0.0018345531438292433\n",
      "Train:\t Perf I \t 0.0009015312701952881\n",
      "Train:\t L2 norm of A \t  0.5087222473467841\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.0015470860583203642\n",
      "Train:\t Perf B \t 0.002108341652548274\n",
      "Train:\t Perf I \t 0.0010574334301059253\n",
      "Train:\t L2 norm of A \t  0.4429123995716649\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.004291602575538955\n",
      "Train:\t Perf B \t 0.0020636847105768084\n",
      "Train:\t Perf I \t 0.0011087198555304288\n",
      "Train:\t L2 norm of A \t  0.42264098043513926\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.004830881564931292\n",
      "Train:\t Perf B \t 0.0019553152425213815\n",
      "Train:\t Perf I \t 0.0009028035184932979\n",
      "Train:\t L2 norm of A \t  0.47752486349949586\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.0028341820574620813\n",
      "Train:\t Perf B \t 0.0018649195556184272\n",
      "Train:\t Perf I \t 0.0009466148457741772\n",
      "Train:\t L2 norm of A \t  0.5111902683459628\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.002340889337046478\n",
      "Train:\t Perf B \t 0.0020625522118354893\n",
      "Train:\t Perf I \t 0.0010035907754498532\n",
      "Train:\t L2 norm of A \t  0.43415417622640584\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.0014783559300365872\n",
      "Train:\t Perf B \t 0.0019011209954524141\n",
      "Train:\t Perf I \t 0.0010402172619747216\n",
      "Train:\t L2 norm of A \t  0.4713532578914249\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.002106470918684089\n",
      "Train:\t Perf B \t 0.0019098998287181543\n",
      "Train:\t Perf I \t 0.0010117093289163796\n",
      "Train:\t L2 norm of A \t  0.45775924916449967\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.002265846066430235\n",
      "Train:\t Perf B \t 0.0019722477487758056\n",
      "Train:\t Perf I \t 0.0009677385558864684\n",
      "Train:\t L2 norm of A \t  0.4177547012370305\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.003938160316384029\n",
      "Train:\t Perf B \t 0.0019607337626091625\n",
      "Train:\t Perf I \t 0.0009607179410532449\n",
      "Train:\t L2 norm of A \t  0.4503554151319705\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.0033233251497470882\n",
      "Train:\t Perf B \t 0.002201922520274318\n",
      "Train:\t Perf I \t 0.0010687854306305913\n",
      "Train:\t L2 norm of A \t  0.41096544537435337\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.0027186402471217784\n",
      "Train:\t Perf B \t 0.002053531336588347\n",
      "Train:\t Perf I \t 0.000950175097967562\n",
      "Train:\t L2 norm of A \t  0.41063276506069535\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.0017520967787989935\n",
      "Train:\t Perf B \t 0.0022140928096480964\n",
      "Train:\t Perf I \t 0.0010624742292915002\n",
      "Train:\t L2 norm of A \t  0.3905298141548953\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.0031629777334805033\n",
      "Train:\t Perf B \t 0.0022661584868221662\n",
      "Train:\t Perf I \t 0.0011572142074786798\n",
      "Train:\t L2 norm of A \t  0.39120597259255274\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.00146086672044327\n",
      "Train:\t Perf B \t 0.0020441174679491033\n",
      "Train:\t Perf I \t 0.0008929637620366191\n",
      "Train:\t L2 norm of A \t  0.43167391717289194\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.0031914534808380997\n",
      "Train:\t Perf B \t 0.0020114285843130983\n",
      "Train:\t Perf I \t 0.0009999600811087322\n",
      "Train:\t L2 norm of A \t  0.42249816763002834\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.0023703423472763974\n",
      "Train:\t Perf B \t 0.0019905035654644134\n",
      "Train:\t Perf I \t 0.0010062561095686103\n",
      "Train:\t L2 norm of A \t  0.44016104318421545\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.0029400019635583237\n",
      "Train:\t Perf B \t 0.0019032965414582624\n",
      "Train:\t Perf I \t 0.0009180885766125353\n",
      "Train:\t L2 norm of A \t  0.4614713557145536\n",
      "-------------- The gamma 0.310000\n",
      "Train:\tLoss \t  0.0027147209532435703\n",
      "Train:\t Perf B \t 0.002100048881065846\n",
      "Train:\t Perf I \t 0.0011021709136775652\n",
      "Train:\t L2 norm of A \t  0.3877933326884753\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.006211769406081443\n",
      "Train:\t Perf B \t 0.0021480199081058407\n",
      "Train:\t Perf I \t 0.0010782941154056241\n",
      "Train:\t L2 norm of A \t  0.43237621029124906\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.0032113931247993257\n",
      "Train:\t Perf B \t 0.0021027133966024718\n",
      "Train:\t Perf I \t 0.0009930985835813488\n",
      "Train:\t L2 norm of A \t  0.4173874182840713\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.0019676519956616386\n",
      "Train:\t Perf B \t 0.002159844276709882\n",
      "Train:\t Perf I \t 0.0010397408862697181\n",
      "Train:\t L2 norm of A \t  0.39244116406924806\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.0016765188680124062\n",
      "Train:\t Perf B \t 0.002021200771903715\n",
      "Train:\t Perf I \t 0.001091074150990702\n",
      "Train:\t L2 norm of A \t  0.4105611224262884\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.003257219143921115\n",
      "Train:\t Perf B \t 0.0019079232054164852\n",
      "Train:\t Perf I \t 0.0009327515419188088\n",
      "Train:\t L2 norm of A \t  0.48232490801777006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.0015809733951497494\n",
      "Train:\t Perf B \t 0.002192280277390142\n",
      "Train:\t Perf I \t 0.00110147610688607\n",
      "Train:\t L2 norm of A \t  0.42354427460880256\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.005726394694120595\n",
      "Train:\t Perf B \t 0.0017712589312731456\n",
      "Train:\t Perf I \t 0.0009441705034252713\n",
      "Train:\t L2 norm of A \t  0.49510851271368905\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.001974885702442306\n",
      "Train:\t Perf B \t 0.001867647193238847\n",
      "Train:\t Perf I \t 0.0009012753278683965\n",
      "Train:\t L2 norm of A \t  0.5087425831139778\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.002548517146701686\n",
      "Train:\t Perf B \t 0.002110701463736413\n",
      "Train:\t Perf I \t 0.0010519115292436769\n",
      "Train:\t L2 norm of A \t  0.3802399340900737\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.0029556837494034744\n",
      "Train:\t Perf B \t 0.0019828742403199717\n",
      "Train:\t Perf I \t 0.000986155872040566\n",
      "Train:\t L2 norm of A \t  0.4482994505223627\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.007019683512690011\n",
      "Train:\t Perf B \t 0.002061896185965139\n",
      "Train:\t Perf I \t 0.0009530301164050648\n",
      "Train:\t L2 norm of A \t  0.4750544285672863\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.005908739130597221\n",
      "Train:\t Perf B \t 0.0020498421451041987\n",
      "Train:\t Perf I \t 0.001149541347781474\n",
      "Train:\t L2 norm of A \t  0.4292793634979322\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.0014644357080449803\n",
      "Train:\t Perf B \t 0.002107348621180467\n",
      "Train:\t Perf I \t 0.0009986725872319173\n",
      "Train:\t L2 norm of A \t  0.42061403107406525\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.001551259071535944\n",
      "Train:\t Perf B \t 0.0020442139601665635\n",
      "Train:\t Perf I \t 0.0010179645867714694\n",
      "Train:\t L2 norm of A \t  0.4196383673253872\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.002387538446744307\n",
      "Train:\t Perf B \t 0.0022607048311916083\n",
      "Train:\t Perf I \t 0.001240259817003155\n",
      "Train:\t L2 norm of A \t  0.42204228297736135\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.0036593994281921534\n",
      "Train:\t Perf B \t 0.002187725212990265\n",
      "Train:\t Perf I \t 0.0011748025760474673\n",
      "Train:\t L2 norm of A \t  0.38518089974520864\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.0030490118973654197\n",
      "Train:\t Perf B \t 0.00206804325248237\n",
      "Train:\t Perf I \t 0.0009594489617096628\n",
      "Train:\t L2 norm of A \t  0.401631902242137\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.002207973886253837\n",
      "Train:\t Perf B \t 0.0023136460549814304\n",
      "Train:\t Perf I \t 0.0010996685117627728\n",
      "Train:\t L2 norm of A \t  0.38539186014406074\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.0024511700439123225\n",
      "Train:\t Perf B \t 0.0019076952024663183\n",
      "Train:\t Perf I \t 0.0009329459374815048\n",
      "Train:\t L2 norm of A \t  0.5015819593112203\n",
      "-------------- The gamma 0.320000\n",
      "Train:\tLoss \t  0.0038039834254220595\n",
      "Train:\t Perf B \t 0.002150322259333168\n",
      "Train:\t Perf I \t 0.001062198969231524\n",
      "Train:\t L2 norm of A \t  0.42600738318575354\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.003907101563128272\n",
      "Train:\t Perf B \t 0.0021302606535853023\n",
      "Train:\t Perf I \t 0.0010454397663328214\n",
      "Train:\t L2 norm of A \t  0.4030763166738205\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.004925998691435958\n",
      "Train:\t Perf B \t 0.0019207372477052953\n",
      "Train:\t Perf I \t 0.0009945974446022912\n",
      "Train:\t L2 norm of A \t  0.47596008516796606\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.0018574973075378854\n",
      "Train:\t Perf B \t 0.0022163352765941116\n",
      "Train:\t Perf I \t 0.001058604519224637\n",
      "Train:\t L2 norm of A \t  0.3825993937119823\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.007134357176928699\n",
      "Train:\t Perf B \t 0.0021073590482724104\n",
      "Train:\t Perf I \t 0.0010428688769593137\n",
      "Train:\t L2 norm of A \t  0.442936491741783\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.0022788190931132845\n",
      "Train:\t Perf B \t 0.0018578054130798558\n",
      "Train:\t Perf I \t 0.0009624422726556873\n",
      "Train:\t L2 norm of A \t  0.45172921724469517\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.004443218173188926\n",
      "Train:\t Perf B \t 0.0021015087632869703\n",
      "Train:\t Perf I \t 0.0010619881590387176\n",
      "Train:\t L2 norm of A \t  0.42279565268911085\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.003056610736506371\n",
      "Train:\t Perf B \t 0.002149671639181762\n",
      "Train:\t Perf I \t 0.0010798688396799732\n",
      "Train:\t L2 norm of A \t  0.3902598357417986\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.006411325822017753\n",
      "Train:\t Perf B \t 0.0020011102232945065\n",
      "Train:\t Perf I \t 0.0009561902515291138\n",
      "Train:\t L2 norm of A \t  0.43045016561093474\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.002938906274808862\n",
      "Train:\t Perf B \t 0.0017713829999068602\n",
      "Train:\t Perf I \t 0.0008610600930892537\n",
      "Train:\t L2 norm of A \t  0.5278775569547367\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.004603505541100324\n",
      "Train:\t Perf B \t 0.0020164451817043247\n",
      "Train:\t Perf I \t 0.001082275365005909\n",
      "Train:\t L2 norm of A \t  0.4619441102478548\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.0015328189596414037\n",
      "Train:\t Perf B \t 0.0020750964383153075\n",
      "Train:\t Perf I \t 0.0011362873066000896\n",
      "Train:\t L2 norm of A \t  0.4352668927576339\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.0026223239491853932\n",
      "Train:\t Perf B \t 0.0021819068004549575\n",
      "Train:\t Perf I \t 0.0011118016294578432\n",
      "Train:\t L2 norm of A \t  0.40543357219025805\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.0038332886898126135\n",
      "Train:\t Perf B \t 0.0018514421029727816\n",
      "Train:\t Perf I \t 0.0008769951328093574\n",
      "Train:\t L2 norm of A \t  0.4905586544392068\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.003367875035113444\n",
      "Train:\t Perf B \t 0.0020771909091566758\n",
      "Train:\t Perf I \t 0.001060949680001874\n",
      "Train:\t L2 norm of A \t  0.4567789444863999\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.0034211726903784615\n",
      "Train:\t Perf B \t 0.0019939966426749103\n",
      "Train:\t Perf I \t 0.0009568236450988257\n",
      "Train:\t L2 norm of A \t  0.4845583540644391\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.0019743998808144352\n",
      "Train:\t Perf B \t 0.0017225398553351327\n",
      "Train:\t Perf I \t 0.0007800282107520449\n",
      "Train:\t L2 norm of A \t  0.5084319651880115\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.0056511859717262225\n",
      "Train:\t Perf B \t 0.0020833003360538897\n",
      "Train:\t Perf I \t 0.0010414922978206908\n",
      "Train:\t L2 norm of A \t  0.43898820521988097\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.001949994550813802\n",
      "Train:\t Perf B \t 0.0019985853697163207\n",
      "Train:\t Perf I \t 0.0009424683851737018\n",
      "Train:\t L2 norm of A \t  0.4296910355665788\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.0018704782923392904\n",
      "Train:\t Perf B \t 0.0019812433170981933\n",
      "Train:\t Perf I \t 0.0010115071333214908\n",
      "Train:\t L2 norm of A \t  0.4493622490294147\n",
      "-------------- The gamma 0.330000\n",
      "Train:\tLoss \t  0.003982691206380328\n",
      "Train:\t Perf B \t 0.0019430783275914475\n",
      "Train:\t Perf I \t 0.000978343754294579\n",
      "Train:\t L2 norm of A \t  0.46591182314522306\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.0026117065794225517\n",
      "Train:\t Perf B \t 0.0021273786655889165\n",
      "Train:\t Perf I \t 0.0011149594126703194\n",
      "Train:\t L2 norm of A \t  0.4224771743223331\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.0029535134904500994\n",
      "Train:\t Perf B \t 0.00197254121674541\n",
      "Train:\t Perf I \t 0.0009546177008536097\n",
      "Train:\t L2 norm of A \t  0.43845862451912077\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.0029094283746048735\n",
      "Train:\t Perf B \t 0.0018969897991618226\n",
      "Train:\t Perf I \t 0.0009155397933079962\n",
      "Train:\t L2 norm of A \t  0.4681648725508497\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.004136538394153886\n",
      "Train:\t Perf B \t 0.002177982679500048\n",
      "Train:\t Perf I \t 0.0011589426378463793\n",
      "Train:\t L2 norm of A \t  0.43012679587910246\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.00325691862528721\n",
      "Train:\t Perf B \t 0.0018216668577942271\n",
      "Train:\t Perf I \t 0.0008812713861192736\n",
      "Train:\t L2 norm of A \t  0.5161733655881805\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.0020205065987674493\n",
      "Train:\t Perf B \t 0.001838664672990024\n",
      "Train:\t Perf I \t 0.0009352504572638995\n",
      "Train:\t L2 norm of A \t  0.4964280391509408\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.0030270166405044394\n",
      "Train:\t Perf B \t 0.0018492145402878963\n",
      "Train:\t Perf I \t 0.0009043888031054717\n",
      "Train:\t L2 norm of A \t  0.4649787489414303\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.0036265881815847707\n",
      "Train:\t Perf B \t 0.002068132618265487\n",
      "Train:\t Perf I \t 0.0010388167925384833\n",
      "Train:\t L2 norm of A \t  0.44452238461242116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.005115320789631699\n",
      "Train:\t Perf B \t 0.0019561268457192332\n",
      "Train:\t Perf I \t 0.000976453495634819\n",
      "Train:\t L2 norm of A \t  0.46853992566345554\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.0037284335591218237\n",
      "Train:\t Perf B \t 0.0021424695187197327\n",
      "Train:\t Perf I \t 0.0011913856474645994\n",
      "Train:\t L2 norm of A \t  0.467466446446652\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.005341976964476447\n",
      "Train:\t Perf B \t 0.0021470146264235554\n",
      "Train:\t Perf I \t 0.001092144454787652\n",
      "Train:\t L2 norm of A \t  0.41126592997230826\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.005339086199565539\n",
      "Train:\t Perf B \t 0.00198793423888152\n",
      "Train:\t Perf I \t 0.000898703646561468\n",
      "Train:\t L2 norm of A \t  0.4841108105781101\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.004840969509533437\n",
      "Train:\t Perf B \t 0.0018238565587752307\n",
      "Train:\t Perf I \t 0.000822798951698092\n",
      "Train:\t L2 norm of A \t  0.5173461180250499\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.006260621280320782\n",
      "Train:\t Perf B \t 0.0019441106754010077\n",
      "Train:\t Perf I \t 0.0009396179249817017\n",
      "Train:\t L2 norm of A \t  0.4441975828226485\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.004322748431342897\n",
      "Train:\t Perf B \t 0.0018981166332072844\n",
      "Train:\t Perf I \t 0.0008807908684276139\n",
      "Train:\t L2 norm of A \t  0.45496135091536355\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.004508665218475139\n",
      "Train:\t Perf B \t 0.0017320829766980148\n",
      "Train:\t Perf I \t 0.0008928693950943918\n",
      "Train:\t L2 norm of A \t  0.5002404160348797\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.004157139672473174\n",
      "Train:\t Perf B \t 0.002152439948852332\n",
      "Train:\t Perf I \t 0.00100577406162664\n",
      "Train:\t L2 norm of A \t  0.38876548944468514\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.004068897966404577\n",
      "Train:\t Perf B \t 0.0018537645736319002\n",
      "Train:\t Perf I \t 0.0008875375056336013\n",
      "Train:\t L2 norm of A \t  0.47437676040246995\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.004420493348929969\n",
      "Train:\t Perf B \t 0.0020515386688333037\n",
      "Train:\t Perf I \t 0.0009839399777218667\n",
      "Train:\t L2 norm of A \t  0.4416867330827928\n",
      "-------------- The gamma 0.340000\n",
      "Train:\tLoss \t  0.0034064373689134493\n",
      "Train:\t Perf B \t 0.0021228923006301726\n",
      "Train:\t Perf I \t 0.0010439007488684344\n",
      "Train:\t L2 norm of A \t  0.42925129069933216\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.004347497611699463\n",
      "Train:\t Perf B \t 0.0018891355470166292\n",
      "Train:\t Perf I \t 0.0008864358794242388\n",
      "Train:\t L2 norm of A \t  0.4737067871628741\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.0038086166804649495\n",
      "Train:\t Perf B \t 0.0019514558970172577\n",
      "Train:\t Perf I \t 0.000961358646995603\n",
      "Train:\t L2 norm of A \t  0.445353245751046\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.00361174648014339\n",
      "Train:\t Perf B \t 0.001789494890406226\n",
      "Train:\t Perf I \t 0.0009969165351653396\n",
      "Train:\t L2 norm of A \t  0.5233023197261378\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.001245583903216053\n",
      "Train:\t Perf B \t 0.001920814835504292\n",
      "Train:\t Perf I \t 0.0008572993039434207\n",
      "Train:\t L2 norm of A \t  0.46622287076444247\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.0016875535467488802\n",
      "Train:\t Perf B \t 0.001922480871495813\n",
      "Train:\t Perf I \t 0.0009939894295437962\n",
      "Train:\t L2 norm of A \t  0.46710009842617206\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.0017853545877410274\n",
      "Train:\t Perf B \t 0.001859040777060438\n",
      "Train:\t Perf I \t 0.0009050404416636023\n",
      "Train:\t L2 norm of A \t  0.4676629587901188\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.006947496250892683\n",
      "Train:\t Perf B \t 0.0019626364992310482\n",
      "Train:\t Perf I \t 0.0008886784881332981\n",
      "Train:\t L2 norm of A \t  0.4708226460835386\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.0037137111395452245\n",
      "Train:\t Perf B \t 0.0019290160970711898\n",
      "Train:\t Perf I \t 0.0009441766485792904\n",
      "Train:\t L2 norm of A \t  0.49236338268814217\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.0018513903596949321\n",
      "Train:\t Perf B \t 0.0018472613216246046\n",
      "Train:\t Perf I \t 0.0009237783243361921\n",
      "Train:\t L2 norm of A \t  0.4735344564732087\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.0027941368087819217\n",
      "Train:\t Perf B \t 0.0017938393006349648\n",
      "Train:\t Perf I \t 0.0008586622976015189\n",
      "Train:\t L2 norm of A \t  0.5008852180971274\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.0072192207823122525\n",
      "Train:\t Perf B \t 0.0017318929716389666\n",
      "Train:\t Perf I \t 0.0008948639277168706\n",
      "Train:\t L2 norm of A \t  0.5565464254659324\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.003691956782557821\n",
      "Train:\t Perf B \t 0.0016891867782163374\n",
      "Train:\t Perf I \t 0.0007837273521276579\n",
      "Train:\t L2 norm of A \t  0.5693564017918046\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.005818560223145824\n",
      "Train:\t Perf B \t 0.0021383606956940646\n",
      "Train:\t Perf I \t 0.001078086468055722\n",
      "Train:\t L2 norm of A \t  0.39786907581216713\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.004318987835734556\n",
      "Train:\t Perf B \t 0.0016512988727756476\n",
      "Train:\t Perf I \t 0.0008163061919306425\n",
      "Train:\t L2 norm of A \t  0.5481887380716245\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.001390610970017272\n",
      "Train:\t Perf B \t 0.0018698709563875358\n",
      "Train:\t Perf I \t 0.0008852476432371396\n",
      "Train:\t L2 norm of A \t  0.4665176395428633\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.002008527883928628\n",
      "Train:\t Perf B \t 0.001602427054755887\n",
      "Train:\t Perf I \t 0.0008139214801592817\n",
      "Train:\t L2 norm of A \t  0.5393048757081182\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.0015374089748224412\n",
      "Train:\t Perf B \t 0.0019931722585195153\n",
      "Train:\t Perf I \t 0.0009723882004219736\n",
      "Train:\t L2 norm of A \t  0.455022776981247\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.006076057904633761\n",
      "Train:\t Perf B \t 0.0017738431053092682\n",
      "Train:\t Perf I \t 0.0009360585262228745\n",
      "Train:\t L2 norm of A \t  0.5122514861092451\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.0016405717527342607\n",
      "Train:\t Perf B \t 0.0020651786243272496\n",
      "Train:\t Perf I \t 0.0009469578267800307\n",
      "Train:\t L2 norm of A \t  0.4370769614329335\n",
      "-------------- The gamma 0.350000\n",
      "Train:\tLoss \t  0.002823358790023741\n",
      "Train:\t Perf B \t 0.0018227405892914162\n",
      "Train:\t Perf I \t 0.0009367584824567182\n",
      "Train:\t L2 norm of A \t  0.5313984052646881\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.003385029324975823\n",
      "Train:\t Perf B \t 0.0019180990692960601\n",
      "Train:\t Perf I \t 0.0009749972413117105\n",
      "Train:\t L2 norm of A \t  0.45095038589077363\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.006608036203054357\n",
      "Train:\t Perf B \t 0.0018125241316404236\n",
      "Train:\t Perf I \t 0.000859654432762174\n",
      "Train:\t L2 norm of A \t  0.5225942510210623\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.001838231450306677\n",
      "Train:\t Perf B \t 0.001697733987405672\n",
      "Train:\t Perf I \t 0.0007654150541567495\n",
      "Train:\t L2 norm of A \t  0.5522995035723484\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.0020017319839491263\n",
      "Train:\t Perf B \t 0.0018573241240838901\n",
      "Train:\t Perf I \t 0.0009602891683672476\n",
      "Train:\t L2 norm of A \t  0.5308104405045777\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.0026830894607173055\n",
      "Train:\t Perf B \t 0.002251455342660693\n",
      "Train:\t Perf I \t 0.0010836100781194723\n",
      "Train:\t L2 norm of A \t  0.4066969896114535\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.0025470827076156523\n",
      "Train:\t Perf B \t 0.0017811207635635182\n",
      "Train:\t Perf I \t 0.0008812668324410179\n",
      "Train:\t L2 norm of A \t  0.5510776688012385\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.001966374359813989\n",
      "Train:\t Perf B \t 0.001957753619569202\n",
      "Train:\t Perf I \t 0.0009247978012465189\n",
      "Train:\t L2 norm of A \t  0.4442803572870351\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.0017462280762720484\n",
      "Train:\t Perf B \t 0.0019211566459129369\n",
      "Train:\t Perf I \t 0.0009260622980226703\n",
      "Train:\t L2 norm of A \t  0.4702255572530401\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.004099868068003483\n",
      "Train:\t Perf B \t 0.00199740828274765\n",
      "Train:\t Perf I \t 0.0009411147299090433\n",
      "Train:\t L2 norm of A \t  0.4593505708712433\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.0025132310582877083\n",
      "Train:\t Perf B \t 0.0019141861435600935\n",
      "Train:\t Perf I \t 0.001060538505008893\n",
      "Train:\t L2 norm of A \t  0.49914357591065067\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.0012044684421572433\n",
      "Train:\t Perf B \t 0.0018676093742181431\n",
      "Train:\t Perf I \t 0.001054927068278888\n",
      "Train:\t L2 norm of A \t  0.4709480293059625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.0026120390187412256\n",
      "Train:\t Perf B \t 0.00213418815884047\n",
      "Train:\t Perf I \t 0.0010855924683119311\n",
      "Train:\t L2 norm of A \t  0.4259924379344285\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.0024245922982866186\n",
      "Train:\t Perf B \t 0.0016260988107947064\n",
      "Train:\t Perf I \t 0.0007226195014429062\n",
      "Train:\t L2 norm of A \t  0.5458161515715942\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.0025092155658877516\n",
      "Train:\t Perf B \t 0.001810535626166566\n",
      "Train:\t Perf I \t 0.000885560271317564\n",
      "Train:\t L2 norm of A \t  0.5782859460216524\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.004409458223943454\n",
      "Train:\t Perf B \t 0.0018318966463080747\n",
      "Train:\t Perf I \t 0.0009283123237025525\n",
      "Train:\t L2 norm of A \t  0.48019206287188176\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.0050265903705870715\n",
      "Train:\t Perf B \t 0.0017599181029151298\n",
      "Train:\t Perf I \t 0.0009758482471617334\n",
      "Train:\t L2 norm of A \t  0.49988223791336434\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.004922872831807001\n",
      "Train:\t Perf B \t 0.0017267537954667948\n",
      "Train:\t Perf I \t 0.000897937118718587\n",
      "Train:\t L2 norm of A \t  0.5721286046317448\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.003484844346783164\n",
      "Train:\t Perf B \t 0.001998326379027226\n",
      "Train:\t Perf I \t 0.000917998220269229\n",
      "Train:\t L2 norm of A \t  0.4736536909345343\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.0025109975236497486\n",
      "Train:\t Perf B \t 0.0021068465363473024\n",
      "Train:\t Perf I \t 0.0010691365379584198\n",
      "Train:\t L2 norm of A \t  0.3988410045087284\n",
      "-------------- The gamma 0.360000\n",
      "Train:\tLoss \t  0.002364206597688971\n",
      "Train:\t Perf B \t 0.001910954131299504\n",
      "Train:\t Perf I \t 0.0009162405355576992\n",
      "Train:\t L2 norm of A \t  0.44702529768463306\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0025101012003099147\n",
      "Train:\t Perf B \t 0.0016443258465896427\n",
      "Train:\t Perf I \t 0.0008395572006969507\n",
      "Train:\t L2 norm of A \t  0.5468036294657426\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.004800315435273044\n",
      "Train:\t Perf B \t 0.001672641896739748\n",
      "Train:\t Perf I \t 0.0009166245224166017\n",
      "Train:\t L2 norm of A \t  0.5217956614356772\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0034947247883997685\n",
      "Train:\t Perf B \t 0.0017473737393716483\n",
      "Train:\t Perf I \t 0.0009163371811052902\n",
      "Train:\t L2 norm of A \t  0.5224769657327125\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0015627429980209812\n",
      "Train:\t Perf B \t 0.001799195235194749\n",
      "Train:\t Perf I \t 0.0009017390630260431\n",
      "Train:\t L2 norm of A \t  0.5349969068319617\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0022841187707962154\n",
      "Train:\t Perf B \t 0.0018297891406818265\n",
      "Train:\t Perf I \t 0.0008806779616930374\n",
      "Train:\t L2 norm of A \t  0.5057541205305597\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0031503723801442366\n",
      "Train:\t Perf B \t 0.002056086083837051\n",
      "Train:\t Perf I \t 0.0011163012823898373\n",
      "Train:\t L2 norm of A \t  0.44370513549039386\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0016071953079200877\n",
      "Train:\t Perf B \t 0.0017213572347987453\n",
      "Train:\t Perf I \t 0.0008420482252250141\n",
      "Train:\t L2 norm of A \t  0.5573642348876512\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.004694667554740668\n",
      "Train:\t Perf B \t 0.0017834374799183508\n",
      "Train:\t Perf I \t 0.0009685777639611901\n",
      "Train:\t L2 norm of A \t  0.518826325664773\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0038591897955656514\n",
      "Train:\t Perf B \t 0.001692153364901876\n",
      "Train:\t Perf I \t 0.0008315843114656298\n",
      "Train:\t L2 norm of A \t  0.5462525585209886\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0025708091206008074\n",
      "Train:\t Perf B \t 0.0020035619971837063\n",
      "Train:\t Perf I \t 0.000983958306626403\n",
      "Train:\t L2 norm of A \t  0.43432042255214487\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.007246104045589371\n",
      "Train:\t Perf B \t 0.0017782152990488656\n",
      "Train:\t Perf I \t 0.0009572359992969066\n",
      "Train:\t L2 norm of A \t  0.5155877816759626\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0017352011538095448\n",
      "Train:\t Perf B \t 0.0016884227976397715\n",
      "Train:\t Perf I \t 0.0008959650766324188\n",
      "Train:\t L2 norm of A \t  0.4999586487603583\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0022334156962307023\n",
      "Train:\t Perf B \t 0.0019153845020453246\n",
      "Train:\t Perf I \t 0.0009403360200428959\n",
      "Train:\t L2 norm of A \t  0.4501093867192829\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0028708311978498137\n",
      "Train:\t Perf B \t 0.0018886655749511176\n",
      "Train:\t Perf I \t 0.0008945663681536894\n",
      "Train:\t L2 norm of A \t  0.5101001280222514\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0024266304832382146\n",
      "Train:\t Perf B \t 0.0021636575705618988\n",
      "Train:\t Perf I \t 0.0010053423415278975\n",
      "Train:\t L2 norm of A \t  0.3981002715121239\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0012506357643683754\n",
      "Train:\t Perf B \t 0.001792493472351933\n",
      "Train:\t Perf I \t 0.0009666438096672553\n",
      "Train:\t L2 norm of A \t  0.4990962493582197\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.002091568698409757\n",
      "Train:\t Perf B \t 0.0021618295871600425\n",
      "Train:\t Perf I \t 0.0010856386574940476\n",
      "Train:\t L2 norm of A \t  0.41175830238044614\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.002389759264423157\n",
      "Train:\t Perf B \t 0.0018561673331793519\n",
      "Train:\t Perf I \t 0.000958362285645982\n",
      "Train:\t L2 norm of A \t  0.501738523411007\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.0037678034245889522\n",
      "Train:\t Perf B \t 0.0018441523077209123\n",
      "Train:\t Perf I \t 0.0009085656588664252\n",
      "Train:\t L2 norm of A \t  0.5083115726819276\n",
      "-------------- The gamma 0.370000\n",
      "Train:\tLoss \t  0.002964838718879888\n",
      "Train:\t Perf B \t 0.00195780507816682\n",
      "Train:\t Perf I \t 0.0009627573435680781\n",
      "Train:\t L2 norm of A \t  0.4462230119775393\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.0034041560736541945\n",
      "Train:\t Perf B \t 0.001831139181743575\n",
      "Train:\t Perf I \t 0.000885351858903592\n",
      "Train:\t L2 norm of A \t  0.5023392609707721\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.006633061935322852\n",
      "Train:\t Perf B \t 0.0016280937138997537\n",
      "Train:\t Perf I \t 0.0008316240168857735\n",
      "Train:\t L2 norm of A \t  0.5823256280556972\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.003964131917494879\n",
      "Train:\t Perf B \t 0.0019751994186195136\n",
      "Train:\t Perf I \t 0.0008615704634569435\n",
      "Train:\t L2 norm of A \t  0.49611297996047965\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.001987898220813403\n",
      "Train:\t Perf B \t 0.001857430544276466\n",
      "Train:\t Perf I \t 0.0009327751097987305\n",
      "Train:\t L2 norm of A \t  0.46703698470171\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.0036816468199387417\n",
      "Train:\t Perf B \t 0.0016961987111612212\n",
      "Train:\t Perf I \t 0.0008522348566545701\n",
      "Train:\t L2 norm of A \t  0.4641735442628324\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.005963697780652488\n",
      "Train:\t Perf B \t 0.0018037308403321393\n",
      "Train:\t Perf I \t 0.0009357586761462074\n",
      "Train:\t L2 norm of A \t  0.5160896520901721\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.0015657557149668891\n",
      "Train:\t Perf B \t 0.0017867688740134905\n",
      "Train:\t Perf I \t 0.0009378520518108468\n",
      "Train:\t L2 norm of A \t  0.5183113446181525\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.0029091142037921433\n",
      "Train:\t Perf B \t 0.0018230613759789341\n",
      "Train:\t Perf I \t 0.0008968630983198876\n",
      "Train:\t L2 norm of A \t  0.47402682912242994\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.001958827487006848\n",
      "Train:\t Perf B \t 0.0016601589047268654\n",
      "Train:\t Perf I \t 0.0007859038962769358\n",
      "Train:\t L2 norm of A \t  0.5359064620915412\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.003746995391040592\n",
      "Train:\t Perf B \t 0.0017122512133958244\n",
      "Train:\t Perf I \t 0.0008590556986117152\n",
      "Train:\t L2 norm of A \t  0.4562847115425696\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.0018409274108186074\n",
      "Train:\t Perf B \t 0.0016306690026765011\n",
      "Train:\t Perf I \t 0.0008387464971013516\n",
      "Train:\t L2 norm of A \t  0.5562265009842828\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.0012740026293339229\n",
      "Train:\t Perf B \t 0.0017822586498654083\n",
      "Train:\t Perf I \t 0.0008484537445365534\n",
      "Train:\t L2 norm of A \t  0.5418103354148988\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.0029685325786429455\n",
      "Train:\t Perf B \t 0.002012134585575311\n",
      "Train:\t Perf I \t 0.0010229404567299544\n",
      "Train:\t L2 norm of A \t  0.46260550843070475\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.005202732921080096\n",
      "Train:\t Perf B \t 0.0021490920694663147\n",
      "Train:\t Perf I \t 0.0010932424451464955\n",
      "Train:\t L2 norm of A \t  0.4160935794618256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.0030722286389809534\n",
      "Train:\t Perf B \t 0.0019041470820358795\n",
      "Train:\t Perf I \t 0.0009505092622737175\n",
      "Train:\t L2 norm of A \t  0.4297586503887272\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.002962696391569797\n",
      "Train:\t Perf B \t 0.0018905192034376116\n",
      "Train:\t Perf I \t 0.0010018258660493403\n",
      "Train:\t L2 norm of A \t  0.464616739893245\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.005451128212864999\n",
      "Train:\t Perf B \t 0.0017307606510184542\n",
      "Train:\t Perf I \t 0.000849706518947552\n",
      "Train:\t L2 norm of A \t  0.5478961755151089\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.003645325544827757\n",
      "Train:\t Perf B \t 0.0018458284801715866\n",
      "Train:\t Perf I \t 0.0008609942279071771\n",
      "Train:\t L2 norm of A \t  0.4658635539509641\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.004999829963216035\n",
      "Train:\t Perf B \t 0.0020512711018556055\n",
      "Train:\t Perf I \t 0.0009850166283263337\n",
      "Train:\t L2 norm of A \t  0.45360706714237276\n",
      "-------------- The gamma 0.380000\n",
      "Train:\tLoss \t  0.0017120327731739977\n",
      "Train:\t Perf B \t 0.0016379866711386712\n",
      "Train:\t Perf I \t 0.0008201292336537826\n",
      "Train:\t L2 norm of A \t  0.549651511483978\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.008106445693106645\n",
      "Train:\t Perf B \t 0.0017541876111455904\n",
      "Train:\t Perf I \t 0.0008827208561156189\n",
      "Train:\t L2 norm of A \t  0.49675735424035955\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0020205538202556646\n",
      "Train:\t Perf B \t 0.0018324304405761692\n",
      "Train:\t Perf I \t 0.0008704780012335371\n",
      "Train:\t L2 norm of A \t  0.4658900234648014\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0013951096157057085\n",
      "Train:\t Perf B \t 0.001731366305614121\n",
      "Train:\t Perf I \t 0.000835736625264563\n",
      "Train:\t L2 norm of A \t  0.5229349388398914\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0026802120765701026\n",
      "Train:\t Perf B \t 0.0019193987456892526\n",
      "Train:\t Perf I \t 0.0008787322642461775\n",
      "Train:\t L2 norm of A \t  0.49848299010588143\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0034272054418069174\n",
      "Train:\t Perf B \t 0.0018801308078626364\n",
      "Train:\t Perf I \t 0.0009238311651007791\n",
      "Train:\t L2 norm of A \t  0.4946868289434969\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0017601872890249013\n",
      "Train:\t Perf B \t 0.0016878437078283743\n",
      "Train:\t Perf I \t 0.0008569530468578312\n",
      "Train:\t L2 norm of A \t  0.5375208198375402\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0032525135047948204\n",
      "Train:\t Perf B \t 0.0015003145586595342\n",
      "Train:\t Perf I \t 0.0007152168499922742\n",
      "Train:\t L2 norm of A \t  0.6095643857969042\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0022583123318216975\n",
      "Train:\t Perf B \t 0.0015714814417732005\n",
      "Train:\t Perf I \t 0.0008236780453022536\n",
      "Train:\t L2 norm of A \t  0.5753752295940919\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0022900672555480578\n",
      "Train:\t Perf B \t 0.0015444572153450618\n",
      "Train:\t Perf I \t 0.0008164054933376456\n",
      "Train:\t L2 norm of A \t  0.5773450880604964\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0013553155994850165\n",
      "Train:\t Perf B \t 0.0015974828930782345\n",
      "Train:\t Perf I \t 0.000790565523039563\n",
      "Train:\t L2 norm of A \t  0.5564647910211166\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.002350297971735621\n",
      "Train:\t Perf B \t 0.001935156154129065\n",
      "Train:\t Perf I \t 0.0009563349119005951\n",
      "Train:\t L2 norm of A \t  0.4748817993793057\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0029054200592518223\n",
      "Train:\t Perf B \t 0.0016840340601779222\n",
      "Train:\t Perf I \t 0.0008188061445692818\n",
      "Train:\t L2 norm of A \t  0.5075347479380238\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.006955333995391165\n",
      "Train:\t Perf B \t 0.0015059360873189561\n",
      "Train:\t Perf I \t 0.0007308570445803594\n",
      "Train:\t L2 norm of A \t  0.6071124002361837\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.002341734423074003\n",
      "Train:\t Perf B \t 0.001660683103763219\n",
      "Train:\t Perf I \t 0.0008324686604903161\n",
      "Train:\t L2 norm of A \t  0.549781023550374\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0031051158946688003\n",
      "Train:\t Perf B \t 0.0018467028714914478\n",
      "Train:\t Perf I \t 0.0009200541060020867\n",
      "Train:\t L2 norm of A \t  0.4629372489816429\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0014914395050046568\n",
      "Train:\t Perf B \t 0.001744133120514256\n",
      "Train:\t Perf I \t 0.0010305400309452166\n",
      "Train:\t L2 norm of A \t  0.5062253163119481\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.005221946984253092\n",
      "Train:\t Perf B \t 0.0017381589276314322\n",
      "Train:\t Perf I \t 0.0008730947727556687\n",
      "Train:\t L2 norm of A \t  0.5546869114314965\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0024336479022587358\n",
      "Train:\t Perf B \t 0.0018578400741504281\n",
      "Train:\t Perf I \t 0.0009132403288189117\n",
      "Train:\t L2 norm of A \t  0.506175650541135\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.004768296378394715\n",
      "Train:\t Perf B \t 0.00194987384636622\n",
      "Train:\t Perf I \t 0.0009466112106818202\n",
      "Train:\t L2 norm of A \t  0.4759562688573443\n",
      "-------------- The gamma 0.390000\n",
      "Train:\tLoss \t  0.0029460869203103116\n",
      "Train:\t Perf B \t 0.0015528217447379857\n",
      "Train:\t Perf I \t 0.0007758731824354808\n",
      "Train:\t L2 norm of A \t  0.5960291563975407\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.003839162257968502\n",
      "Train:\t Perf B \t 0.0017356085311605177\n",
      "Train:\t Perf I \t 0.0007920924118966507\n",
      "Train:\t L2 norm of A \t  0.49455556145236024\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.006153145692525443\n",
      "Train:\t Perf B \t 0.0018273593282880343\n",
      "Train:\t Perf I \t 0.0009693868619994339\n",
      "Train:\t L2 norm of A \t  0.458953851518056\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.0018759720500402639\n",
      "Train:\t Perf B \t 0.002121314512143876\n",
      "Train:\t Perf I \t 0.0010929174523240082\n",
      "Train:\t L2 norm of A \t  0.4166362695068387\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.0033127284264313915\n",
      "Train:\t Perf B \t 0.0016303474304743973\n",
      "Train:\t Perf I \t 0.0007954145437628749\n",
      "Train:\t L2 norm of A \t  0.5219871961564094\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.0012727657574437125\n",
      "Train:\t Perf B \t 0.001612444175328389\n",
      "Train:\t Perf I \t 0.0007780694034568773\n",
      "Train:\t L2 norm of A \t  0.5854420309755058\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.0012379492939211419\n",
      "Train:\t Perf B \t 0.0016897479391513174\n",
      "Train:\t Perf I \t 0.0007527047172710356\n",
      "Train:\t L2 norm of A \t  0.5268250928814276\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.001855410126415492\n",
      "Train:\t Perf B \t 0.0018647930612734024\n",
      "Train:\t Perf I \t 0.0008235222544622231\n",
      "Train:\t L2 norm of A \t  0.5174979182794985\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.001684787767218628\n",
      "Train:\t Perf B \t 0.001758006880767236\n",
      "Train:\t Perf I \t 0.0009465689403747016\n",
      "Train:\t L2 norm of A \t  0.514429059817772\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.001356774694909732\n",
      "Train:\t Perf B \t 0.0017066775626628154\n",
      "Train:\t Perf I \t 0.0008592786634944145\n",
      "Train:\t L2 norm of A \t  0.5169823702177359\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.0013400890661247982\n",
      "Train:\t Perf B \t 0.0016532400368563897\n",
      "Train:\t Perf I \t 0.0007756342302181268\n",
      "Train:\t L2 norm of A \t  0.531183696204327\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.0034832159163063546\n",
      "Train:\t Perf B \t 0.001925183172760379\n",
      "Train:\t Perf I \t 0.000990815447686889\n",
      "Train:\t L2 norm of A \t  0.49531968891246453\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.004677109186870474\n",
      "Train:\t Perf B \t 0.0016094188323778423\n",
      "Train:\t Perf I \t 0.0007742386218804571\n",
      "Train:\t L2 norm of A \t  0.6003823424447469\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.00139368903997748\n",
      "Train:\t Perf B \t 0.0017115923176280715\n",
      "Train:\t Perf I \t 0.0008482493976643383\n",
      "Train:\t L2 norm of A \t  0.511651888665813\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.003946359906981347\n",
      "Train:\t Perf B \t 0.0017040444887126303\n",
      "Train:\t Perf I \t 0.0007933754457496022\n",
      "Train:\t L2 norm of A \t  0.5721432224215842\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.0038437903555780025\n",
      "Train:\t Perf B \t 0.001527597672565999\n",
      "Train:\t Perf I \t 0.0006851245236963396\n",
      "Train:\t L2 norm of A \t  0.6046973709846806\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.002255847732157236\n",
      "Train:\t Perf B \t 0.001725903732354742\n",
      "Train:\t Perf I \t 0.000800380190919439\n",
      "Train:\t L2 norm of A \t  0.49298973262259227\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.006476362634966479\n",
      "Train:\t Perf B \t 0.0017649640035426971\n",
      "Train:\t Perf I \t 0.0007873161043288562\n",
      "Train:\t L2 norm of A \t  0.5058868864743294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.005171607579205369\n",
      "Train:\t Perf B \t 0.0017789701247435323\n",
      "Train:\t Perf I \t 0.00089743518168423\n",
      "Train:\t L2 norm of A \t  0.5267972916537993\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.002389359463403916\n",
      "Train:\t Perf B \t 0.0016663404710503863\n",
      "Train:\t Perf I \t 0.0008395000321628039\n",
      "Train:\t L2 norm of A \t  0.5336856388600904\n",
      "-------------- The gamma 0.400000\n",
      "Train:\tLoss \t  0.005503679615827863\n",
      "Train:\t Perf B \t 0.00202107493367324\n",
      "Train:\t Perf I \t 0.0010002844434136775\n",
      "Train:\t L2 norm of A \t  0.4244829408152773\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.0030759393993137854\n",
      "Train:\t Perf B \t 0.00160661090596035\n",
      "Train:\t Perf I \t 0.0008378607684731055\n",
      "Train:\t L2 norm of A \t  0.5123388975570192\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.004447596637387817\n",
      "Train:\t Perf B \t 0.0015037366811692942\n",
      "Train:\t Perf I \t 0.0007376078241960944\n",
      "Train:\t L2 norm of A \t  0.5924678392304237\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.005480619924681481\n",
      "Train:\t Perf B \t 0.0015796152085369379\n",
      "Train:\t Perf I \t 0.0007855707841556376\n",
      "Train:\t L2 norm of A \t  0.5847638382587995\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.006574238447951326\n",
      "Train:\t Perf B \t 0.0016329772135713642\n",
      "Train:\t Perf I \t 0.0008373166202656203\n",
      "Train:\t L2 norm of A \t  0.5935209310409127\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.001900418482194984\n",
      "Train:\t Perf B \t 0.0014714768765753824\n",
      "Train:\t Perf I \t 0.0007982231921605334\n",
      "Train:\t L2 norm of A \t  0.5679596411133108\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.0022643925728531053\n",
      "Train:\t Perf B \t 0.0015520522200416424\n",
      "Train:\t Perf I \t 0.0007585406767067448\n",
      "Train:\t L2 norm of A \t  0.5393094119756207\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.0021298054869342466\n",
      "Train:\t Perf B \t 0.0016203065705797002\n",
      "Train:\t Perf I \t 0.0007808908096358843\n",
      "Train:\t L2 norm of A \t  0.5326885575084561\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.002179155797621041\n",
      "Train:\t Perf B \t 0.0017651691047504351\n",
      "Train:\t Perf I \t 0.0008652863255950311\n",
      "Train:\t L2 norm of A \t  0.4993461322711525\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.004031615378651329\n",
      "Train:\t Perf B \t 0.0017160491742859634\n",
      "Train:\t Perf I \t 0.0009268950164239326\n",
      "Train:\t L2 norm of A \t  0.4855412478328909\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.0013137603076877767\n",
      "Train:\t Perf B \t 0.0015654130247194085\n",
      "Train:\t Perf I \t 0.0007637829879028622\n",
      "Train:\t L2 norm of A \t  0.5578930609251785\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.0026752571092818343\n",
      "Train:\t Perf B \t 0.0015509571606381353\n",
      "Train:\t Perf I \t 0.0007464771543468382\n",
      "Train:\t L2 norm of A \t  0.5829098386951298\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.0013487814362880067\n",
      "Train:\t Perf B \t 0.0016952252870179804\n",
      "Train:\t Perf I \t 0.0008815413223793866\n",
      "Train:\t L2 norm of A \t  0.5276502558533637\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.0019961132477630196\n",
      "Train:\t Perf B \t 0.0011699499058180165\n",
      "Train:\t Perf I \t 0.0006855385104845162\n",
      "Train:\t L2 norm of A \t  0.6312269548563358\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.003565754207735073\n",
      "Train:\t Perf B \t 0.0019781940053656554\n",
      "Train:\t Perf I \t 0.0010467106662758994\n",
      "Train:\t L2 norm of A \t  0.47527746906188983\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.00155581646930608\n",
      "Train:\t Perf B \t 0.0017436033042294722\n",
      "Train:\t Perf I \t 0.000798942703109054\n",
      "Train:\t L2 norm of A \t  0.5081491833111168\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.003026629763868783\n",
      "Train:\t Perf B \t 0.0017331264421868516\n",
      "Train:\t Perf I \t 0.0008246653980508893\n",
      "Train:\t L2 norm of A \t  0.5240059305549499\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.002112527758917087\n",
      "Train:\t Perf B \t 0.0015942867364084205\n",
      "Train:\t Perf I \t 0.000738689467306883\n",
      "Train:\t L2 norm of A \t  0.5337196893892009\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.0043404573528739165\n",
      "Train:\t Perf B \t 0.0016968633758234362\n",
      "Train:\t Perf I \t 0.0008788457607063099\n",
      "Train:\t L2 norm of A \t  0.531103300137942\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.002180838317814541\n",
      "Train:\t Perf B \t 0.0016638842462057754\n",
      "Train:\t Perf I \t 0.0009297581893963426\n",
      "Train:\t L2 norm of A \t  0.5184175169318898\n",
      "-------------- The gamma 0.410000\n",
      "Train:\tLoss \t  0.002202902952231504\n",
      "Train:\t Perf B \t 0.001839621578903171\n",
      "Train:\t Perf I \t 0.0009290774334263858\n",
      "Train:\t L2 norm of A \t  0.5075669093619971\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0061341889485542955\n",
      "Train:\t Perf B \t 0.0013864534220531222\n",
      "Train:\t Perf I \t 0.0007342757837735563\n",
      "Train:\t L2 norm of A \t  0.6465886147864381\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0027255957932379184\n",
      "Train:\t Perf B \t 0.0017239301584387676\n",
      "Train:\t Perf I \t 0.0008890270187492319\n",
      "Train:\t L2 norm of A \t  0.5013687067948008\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0023241687582925322\n",
      "Train:\t Perf B \t 0.001505322615054451\n",
      "Train:\t Perf I \t 0.0006776477527764235\n",
      "Train:\t L2 norm of A \t  0.5961673557872258\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.002619299349748572\n",
      "Train:\t Perf B \t 0.001684124850718907\n",
      "Train:\t Perf I \t 0.0008436285188335044\n",
      "Train:\t L2 norm of A \t  0.532716314887079\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.00248367597973968\n",
      "Train:\t Perf B \t 0.0014481001490017945\n",
      "Train:\t Perf I \t 0.000742708983687303\n",
      "Train:\t L2 norm of A \t  0.5876376635189745\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0024471489660105135\n",
      "Train:\t Perf B \t 0.001629223410117973\n",
      "Train:\t Perf I \t 0.0007945677994909885\n",
      "Train:\t L2 norm of A \t  0.5315573380356269\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0018079454870351412\n",
      "Train:\t Perf B \t 0.0016509359874785385\n",
      "Train:\t Perf I \t 0.0007774072927240226\n",
      "Train:\t L2 norm of A \t  0.5877710590050435\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0027433415848469563\n",
      "Train:\t Perf B \t 0.0015345916350911167\n",
      "Train:\t Perf I \t 0.0007407379199279161\n",
      "Train:\t L2 norm of A \t  0.550232687293682\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0033250362201447626\n",
      "Train:\t Perf B \t 0.0018032489628070616\n",
      "Train:\t Perf I \t 0.0009061451604890009\n",
      "Train:\t L2 norm of A \t  0.5176557495045564\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.004401534562819718\n",
      "Train:\t Perf B \t 0.0015584231525248731\n",
      "Train:\t Perf I \t 0.0009073840904048289\n",
      "Train:\t L2 norm of A \t  0.5783806473822246\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0026557288811086812\n",
      "Train:\t Perf B \t 0.001922320967181787\n",
      "Train:\t Perf I \t 0.0010905924846205354\n",
      "Train:\t L2 norm of A \t  0.48664831444983764\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0046923712702809355\n",
      "Train:\t Perf B \t 0.0019236343485970664\n",
      "Train:\t Perf I \t 0.0010341142106141076\n",
      "Train:\t L2 norm of A \t  0.4886141397647858\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0021483255890763302\n",
      "Train:\t Perf B \t 0.001837570043018507\n",
      "Train:\t Perf I \t 0.0009387286518398675\n",
      "Train:\t L2 norm of A \t  0.4967250685652553\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.004134105830304649\n",
      "Train:\t Perf B \t 0.0016883349094870684\n",
      "Train:\t Perf I \t 0.0009120747381126819\n",
      "Train:\t L2 norm of A \t  0.5801981506398514\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0025608619518773246\n",
      "Train:\t Perf B \t 0.001510952987390021\n",
      "Train:\t Perf I \t 0.0007739713173763243\n",
      "Train:\t L2 norm of A \t  0.5783288455104608\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.002449483885744546\n",
      "Train:\t Perf B \t 0.001596639984272147\n",
      "Train:\t Perf I \t 0.0007367404774757956\n",
      "Train:\t L2 norm of A \t  0.5786180671691453\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0018531635597259747\n",
      "Train:\t Perf B \t 0.001624409644983914\n",
      "Train:\t Perf I \t 0.0008316598590970998\n",
      "Train:\t L2 norm of A \t  0.5388284407266475\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.004670032985609386\n",
      "Train:\t Perf B \t 0.001540930129680255\n",
      "Train:\t Perf I \t 0.0007592509093416643\n",
      "Train:\t L2 norm of A \t  0.5543527836931601\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0036245771685659007\n",
      "Train:\t Perf B \t 0.0020250522207248623\n",
      "Train:\t Perf I \t 0.0010308870195195614\n",
      "Train:\t L2 norm of A \t  0.4537512209619369\n",
      "-------------- The gamma 0.420000\n",
      "Train:\tLoss \t  0.0023146906458012006\n",
      "Train:\t Perf B \t 0.0015713857357013848\n",
      "Train:\t Perf I \t 0.0007178746960362739\n",
      "Train:\t L2 norm of A \t  0.5343374061317726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.001657129398854794\n",
      "Train:\t Perf B \t 0.0015914108012988586\n",
      "Train:\t Perf I \t 0.0007394346599278767\n",
      "Train:\t L2 norm of A \t  0.5552729550127584\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.002943585502486757\n",
      "Train:\t Perf B \t 0.001921554904599379\n",
      "Train:\t Perf I \t 0.0009564810825779339\n",
      "Train:\t L2 norm of A \t  0.4870973284772098\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.004038032233034751\n",
      "Train:\t Perf B \t 0.0018411574873111673\n",
      "Train:\t Perf I \t 0.0008532956262519657\n",
      "Train:\t L2 norm of A \t  0.49841418858681064\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.0016982914748353687\n",
      "Train:\t Perf B \t 0.002027864592972033\n",
      "Train:\t Perf I \t 0.0009684435385206914\n",
      "Train:\t L2 norm of A \t  0.4267242006115779\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.001810929701822174\n",
      "Train:\t Perf B \t 0.0014756146039654\n",
      "Train:\t Perf I \t 0.0008307567729573538\n",
      "Train:\t L2 norm of A \t  0.6265904665794605\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.004937858428785992\n",
      "Train:\t Perf B \t 0.00155095446341489\n",
      "Train:\t Perf I \t 0.0007681629334489795\n",
      "Train:\t L2 norm of A \t  0.5501026101017896\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.00293643007617927\n",
      "Train:\t Perf B \t 0.0018226258699697642\n",
      "Train:\t Perf I \t 0.0010445492317626706\n",
      "Train:\t L2 norm of A \t  0.481289853204022\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.0040276193889197485\n",
      "Train:\t Perf B \t 0.0017949860978072215\n",
      "Train:\t Perf I \t 0.000920787445165705\n",
      "Train:\t L2 norm of A \t  0.4773201434467194\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.0020590618391990697\n",
      "Train:\t Perf B \t 0.001702666791326378\n",
      "Train:\t Perf I \t 0.0008299135742089453\n",
      "Train:\t L2 norm of A \t  0.5373168271845217\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.002420538328578129\n",
      "Train:\t Perf B \t 0.0018900242527544046\n",
      "Train:\t Perf I \t 0.0008622328086626508\n",
      "Train:\t L2 norm of A \t  0.4831197333102811\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.003582303223496313\n",
      "Train:\t Perf B \t 0.0015330931723878268\n",
      "Train:\t Perf I \t 0.0007549867927540518\n",
      "Train:\t L2 norm of A \t  0.5456506959289094\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.005366069722235517\n",
      "Train:\t Perf B \t 0.001897185368665956\n",
      "Train:\t Perf I \t 0.0008902404041189928\n",
      "Train:\t L2 norm of A \t  0.4829332748421831\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.004640149797771581\n",
      "Train:\t Perf B \t 0.001685833880062342\n",
      "Train:\t Perf I \t 0.000818783490820215\n",
      "Train:\t L2 norm of A \t  0.5417225234637842\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.002201175491694754\n",
      "Train:\t Perf B \t 0.0013427853975580852\n",
      "Train:\t Perf I \t 0.0007194203410658472\n",
      "Train:\t L2 norm of A \t  0.5968783903255873\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.0032459579424040075\n",
      "Train:\t Perf B \t 0.0017927472423802963\n",
      "Train:\t Perf I \t 0.0009200834809309373\n",
      "Train:\t L2 norm of A \t  0.5026946512610633\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.0014864371256758293\n",
      "Train:\t Perf B \t 0.0016776912107592272\n",
      "Train:\t Perf I \t 0.0008111938554238398\n",
      "Train:\t L2 norm of A \t  0.5372952314631543\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.0036395994776324005\n",
      "Train:\t Perf B \t 0.0018158706978723424\n",
      "Train:\t Perf I \t 0.0008763047179281079\n",
      "Train:\t L2 norm of A \t  0.4958067631806939\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.0014145204887320085\n",
      "Train:\t Perf B \t 0.0017513770828720267\n",
      "Train:\t Perf I \t 0.0009304622229305444\n",
      "Train:\t L2 norm of A \t  0.5207062526194496\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.0036864120136451653\n",
      "Train:\t Perf B \t 0.0015570470787611079\n",
      "Train:\t Perf I \t 0.000820103075878428\n",
      "Train:\t L2 norm of A \t  0.5497833145685522\n",
      "-------------- The gamma 0.430000\n",
      "Train:\tLoss \t  0.0029467506390252994\n",
      "Train:\t Perf B \t 0.0017334856967605825\n",
      "Train:\t Perf I \t 0.0008697013839251245\n",
      "Train:\t L2 norm of A \t  0.5210850475329482\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.002430587555583417\n",
      "Train:\t Perf B \t 0.0015651502767255858\n",
      "Train:\t Perf I \t 0.0007533280614015181\n",
      "Train:\t L2 norm of A \t  0.5563182949833564\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.002828485066009283\n",
      "Train:\t Perf B \t 0.0014715813513522438\n",
      "Train:\t Perf I \t 0.0008105691910172238\n",
      "Train:\t L2 norm of A \t  0.5771515158881608\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.001435957796044529\n",
      "Train:\t Perf B \t 0.0015004169391307923\n",
      "Train:\t Perf I \t 0.000706361154659339\n",
      "Train:\t L2 norm of A \t  0.5633726441715387\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.0014972351441705435\n",
      "Train:\t Perf B \t 0.0014711810620188378\n",
      "Train:\t Perf I \t 0.0007615163819397171\n",
      "Train:\t L2 norm of A \t  0.6060689470024098\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.007217210602577569\n",
      "Train:\t Perf B \t 0.0015175866938054792\n",
      "Train:\t Perf I \t 0.0007883089530483695\n",
      "Train:\t L2 norm of A \t  0.6241230965199254\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.0016470551518779793\n",
      "Train:\t Perf B \t 0.0015980722813529332\n",
      "Train:\t Perf I \t 0.0007658506105443192\n",
      "Train:\t L2 norm of A \t  0.5605223637801899\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.001923173260387287\n",
      "Train:\t Perf B \t 0.0016249699052425387\n",
      "Train:\t Perf I \t 0.0007761357623188782\n",
      "Train:\t L2 norm of A \t  0.5354549540101949\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.004349571019864985\n",
      "Train:\t Perf B \t 0.0019325144946080878\n",
      "Train:\t Perf I \t 0.0009795727407313174\n",
      "Train:\t L2 norm of A \t  0.46301493446619785\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.003812746544463006\n",
      "Train:\t Perf B \t 0.0018798891682488752\n",
      "Train:\t Perf I \t 0.0009365104810194757\n",
      "Train:\t L2 norm of A \t  0.4930552263901865\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.005384630204944362\n",
      "Train:\t Perf B \t 0.0016818873308146612\n",
      "Train:\t Perf I \t 0.0008325953774090888\n",
      "Train:\t L2 norm of A \t  0.5459608222501562\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.003769568397849759\n",
      "Train:\t Perf B \t 0.001869339435019824\n",
      "Train:\t Perf I \t 0.0009396200899255193\n",
      "Train:\t L2 norm of A \t  0.5036436832408963\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.0013630309108035594\n",
      "Train:\t Perf B \t 0.0015091307772693379\n",
      "Train:\t Perf I \t 0.0007761965458729788\n",
      "Train:\t L2 norm of A \t  0.5950856466176335\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.007320144031572646\n",
      "Train:\t Perf B \t 0.001772162164038024\n",
      "Train:\t Perf I \t 0.0008921433349976569\n",
      "Train:\t L2 norm of A \t  0.4743614157076216\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.0028170405355910206\n",
      "Train:\t Perf B \t 0.0013744134290670335\n",
      "Train:\t Perf I \t 0.0007367230523624022\n",
      "Train:\t L2 norm of A \t  0.6350412715401824\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.0023541192062595446\n",
      "Train:\t Perf B \t 0.0017890563637092567\n",
      "Train:\t Perf I \t 0.0008464944727233114\n",
      "Train:\t L2 norm of A \t  0.5364158293036826\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.001556445063258728\n",
      "Train:\t Perf B \t 0.0014601748737374063\n",
      "Train:\t Perf I \t 0.0007309309175999449\n",
      "Train:\t L2 norm of A \t  0.617300525334534\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.0019632905419772464\n",
      "Train:\t Perf B \t 0.0015157120572443085\n",
      "Train:\t Perf I \t 0.000805590985104754\n",
      "Train:\t L2 norm of A \t  0.5673168501953653\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.0028635196976616725\n",
      "Train:\t Perf B \t 0.001513488135063287\n",
      "Train:\t Perf I \t 0.000731113236360177\n",
      "Train:\t L2 norm of A \t  0.5970377136449411\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.0018143908877209533\n",
      "Train:\t Perf B \t 0.0015157887128510858\n",
      "Train:\t Perf I \t 0.0007853879240771307\n",
      "Train:\t L2 norm of A \t  0.5964379879074884\n",
      "-------------- The gamma 0.440000\n",
      "Train:\tLoss \t  0.0026429590864299494\n",
      "Train:\t Perf B \t 0.0018623038584336825\n",
      "Train:\t Perf I \t 0.0009315095382050884\n",
      "Train:\t L2 norm of A \t  0.48474735385612905\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.003721770763495715\n",
      "Train:\t Perf B \t 0.0016778448174227115\n",
      "Train:\t Perf I \t 0.0007951433613980103\n",
      "Train:\t L2 norm of A \t  0.515330369517222\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.001911365678759775\n",
      "Train:\t Perf B \t 0.0014500985970307492\n",
      "Train:\t Perf I \t 0.0007429305195017436\n",
      "Train:\t L2 norm of A \t  0.5864559940949291\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.0033022290434611312\n",
      "Train:\t Perf B \t 0.0018178428410476738\n",
      "Train:\t Perf I \t 0.0009626305176947885\n",
      "Train:\t L2 norm of A \t  0.500970500917681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.0027073184983902954\n",
      "Train:\t Perf B \t 0.001627170864856824\n",
      "Train:\t Perf I \t 0.0008369769939184119\n",
      "Train:\t L2 norm of A \t  0.554455632042076\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.0020999293354560813\n",
      "Train:\t Perf B \t 0.001517209455165194\n",
      "Train:\t Perf I \t 0.0006804817346564817\n",
      "Train:\t L2 norm of A \t  0.60956746870288\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.0039026161384241582\n",
      "Train:\t Perf B \t 0.0015867547308049477\n",
      "Train:\t Perf I \t 0.0007623027445856358\n",
      "Train:\t L2 norm of A \t  0.5823000711300784\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.002288391057417314\n",
      "Train:\t Perf B \t 0.0010472627644988503\n",
      "Train:\t Perf I \t 0.0005900462861361394\n",
      "Train:\t L2 norm of A \t  0.7149694734600313\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.003502532464989554\n",
      "Train:\t Perf B \t 0.0014932019496517637\n",
      "Train:\t Perf I \t 0.0007241916866718742\n",
      "Train:\t L2 norm of A \t  0.61298108600038\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.003338484755976984\n",
      "Train:\t Perf B \t 0.001593609774803181\n",
      "Train:\t Perf I \t 0.0008005635079589218\n",
      "Train:\t L2 norm of A \t  0.5759662057328836\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.0035000203289408537\n",
      "Train:\t Perf B \t 0.0014897936501503991\n",
      "Train:\t Perf I \t 0.00069534670723547\n",
      "Train:\t L2 norm of A \t  0.590352426967783\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.0031277471988040116\n",
      "Train:\t Perf B \t 0.001754614589696407\n",
      "Train:\t Perf I \t 0.0008791533494172703\n",
      "Train:\t L2 norm of A \t  0.5454726352607314\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.002437358523952712\n",
      "Train:\t Perf B \t 0.0014256756154055164\n",
      "Train:\t Perf I \t 0.0007048497581342713\n",
      "Train:\t L2 norm of A \t  0.5610310251031279\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.0037234833837369\n",
      "Train:\t Perf B \t 0.0015324932500626498\n",
      "Train:\t Perf I \t 0.0007041878043618254\n",
      "Train:\t L2 norm of A \t  0.5720631528261583\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.0038115182253663543\n",
      "Train:\t Perf B \t 0.00128563722240774\n",
      "Train:\t Perf I \t 0.0006478913061976502\n",
      "Train:\t L2 norm of A \t  0.6012382067211102\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.0028397578163583963\n",
      "Train:\t Perf B \t 0.0014000221544570818\n",
      "Train:\t Perf I \t 0.0007582490827519785\n",
      "Train:\t L2 norm of A \t  0.5969533574884611\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.00182186283995305\n",
      "Train:\t Perf B \t 0.0014601349505610859\n",
      "Train:\t Perf I \t 0.0007157518424551981\n",
      "Train:\t L2 norm of A \t  0.5730758555070502\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.0046249252000503845\n",
      "Train:\t Perf B \t 0.0016562312952928934\n",
      "Train:\t Perf I \t 0.0007690522727862141\n",
      "Train:\t L2 norm of A \t  0.5170339133984624\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.003251537052833974\n",
      "Train:\t Perf B \t 0.0014278779810698683\n",
      "Train:\t Perf I \t 0.000726904435415807\n",
      "Train:\t L2 norm of A \t  0.5925878895158887\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.0014920391673504125\n",
      "Train:\t Perf B \t 0.0016246695463242708\n",
      "Train:\t Perf I \t 0.0008525503641463555\n",
      "Train:\t L2 norm of A \t  0.5249028702276385\n",
      "-------------- The gamma 0.450000\n",
      "Train:\tLoss \t  0.005212157144768102\n",
      "Train:\t Perf B \t 0.0017342003797461223\n",
      "Train:\t Perf I \t 0.0008617862291284585\n",
      "Train:\t L2 norm of A \t  0.5279624540323643\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.001995769365164599\n",
      "Train:\t Perf B \t 0.0015990919224076897\n",
      "Train:\t Perf I \t 0.0007364649333937933\n",
      "Train:\t L2 norm of A \t  0.5414471082666442\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.002063442465407621\n",
      "Train:\t Perf B \t 0.0014099440793253807\n",
      "Train:\t Perf I \t 0.0007154825681577653\n",
      "Train:\t L2 norm of A \t  0.5753099931620431\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.002589540724748895\n",
      "Train:\t Perf B \t 0.0014814533902252783\n",
      "Train:\t Perf I \t 0.000684936104833298\n",
      "Train:\t L2 norm of A \t  0.5725056115877069\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.003546349949422556\n",
      "Train:\t Perf B \t 0.00140370907495619\n",
      "Train:\t Perf I \t 0.0007116920138362175\n",
      "Train:\t L2 norm of A \t  0.5793325030431532\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.0033876505674619566\n",
      "Train:\t Perf B \t 0.001505066141652995\n",
      "Train:\t Perf I \t 0.0007740858996428383\n",
      "Train:\t L2 norm of A \t  0.5799461219661274\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.0023442544883419147\n",
      "Train:\t Perf B \t 0.0015662240473055208\n",
      "Train:\t Perf I \t 0.0007169073618713951\n",
      "Train:\t L2 norm of A \t  0.5740878109689216\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.0024032177704006387\n",
      "Train:\t Perf B \t 0.0015077461372207536\n",
      "Train:\t Perf I \t 0.0007116514311189646\n",
      "Train:\t L2 norm of A \t  0.5839721014028583\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.003868086869635369\n",
      "Train:\t Perf B \t 0.0014898410468312388\n",
      "Train:\t Perf I \t 0.0006894857014573163\n",
      "Train:\t L2 norm of A \t  0.6032944319665874\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.0024845977581128267\n",
      "Train:\t Perf B \t 0.0015029785387794395\n",
      "Train:\t Perf I \t 0.0007057252898817175\n",
      "Train:\t L2 norm of A \t  0.5767434298823688\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.003261707001344532\n",
      "Train:\t Perf B \t 0.001725587675132936\n",
      "Train:\t Perf I \t 0.0008079673550544438\n",
      "Train:\t L2 norm of A \t  0.5214441415009167\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.0028358774091960938\n",
      "Train:\t Perf B \t 0.001842185205384256\n",
      "Train:\t Perf I \t 0.0008502198377586462\n",
      "Train:\t L2 norm of A \t  0.4553864268108193\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.0014448605404161145\n",
      "Train:\t Perf B \t 0.0015840868848520254\n",
      "Train:\t Perf I \t 0.0006809717764926992\n",
      "Train:\t L2 norm of A \t  0.5588978276944706\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.0020291767506196968\n",
      "Train:\t Perf B \t 0.001659770360990017\n",
      "Train:\t Perf I \t 0.0009109604619591526\n",
      "Train:\t L2 norm of A \t  0.5237244089343079\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.0018296917903840922\n",
      "Train:\t Perf B \t 0.0015986226797835505\n",
      "Train:\t Perf I \t 0.0007427559822819133\n",
      "Train:\t L2 norm of A \t  0.5798000867638939\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.005130592149777931\n",
      "Train:\t Perf B \t 0.0015317120472814355\n",
      "Train:\t Perf I \t 0.000799537305743377\n",
      "Train:\t L2 norm of A \t  0.6038974626171646\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.0019484229220949494\n",
      "Train:\t Perf B \t 0.0008856485327175336\n",
      "Train:\t Perf I \t 0.0006605963653893364\n",
      "Train:\t L2 norm of A \t  0.7321830316550222\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.0022137820739924897\n",
      "Train:\t Perf B \t 0.0014497674178966\n",
      "Train:\t Perf I \t 0.0007468595790322678\n",
      "Train:\t L2 norm of A \t  0.5826481170312161\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.0013866845220026878\n",
      "Train:\t Perf B \t 0.0015293307907769296\n",
      "Train:\t Perf I \t 0.0007596927462979312\n",
      "Train:\t L2 norm of A \t  0.6023828583435216\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.0031709662397046826\n",
      "Train:\t Perf B \t 0.0018536290343577541\n",
      "Train:\t Perf I \t 0.0008670911737033302\n",
      "Train:\t L2 norm of A \t  0.4897087905951184\n",
      "-------------- The gamma 0.460000\n",
      "Train:\tLoss \t  0.002295073628036997\n",
      "Train:\t Perf B \t 0.0015137654582634333\n",
      "Train:\t Perf I \t 0.0007931574682911833\n",
      "Train:\t L2 norm of A \t  0.5549980956246207\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.0033130860798074578\n",
      "Train:\t Perf B \t 0.0017308756922612997\n",
      "Train:\t Perf I \t 0.00084780099904344\n",
      "Train:\t L2 norm of A \t  0.5062828235220744\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.005295325876731725\n",
      "Train:\t Perf B \t 0.001710295883236034\n",
      "Train:\t Perf I \t 0.0007928155476045179\n",
      "Train:\t L2 norm of A \t  0.5205890127585172\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.0019486238056986276\n",
      "Train:\t Perf B \t 0.0015662693435947726\n",
      "Train:\t Perf I \t 0.0007326424990371969\n",
      "Train:\t L2 norm of A \t  0.5496992027188021\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.0024622813483627113\n",
      "Train:\t Perf B \t 0.0017974720391728844\n",
      "Train:\t Perf I \t 0.0008797638904416657\n",
      "Train:\t L2 norm of A \t  0.4943029228283399\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.0017170689139000617\n",
      "Train:\t Perf B \t 0.001520037915763071\n",
      "Train:\t Perf I \t 0.0007311885218353149\n",
      "Train:\t L2 norm of A \t  0.6057507704656457\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.005112124943655471\n",
      "Train:\t Perf B \t 0.0014493840319972355\n",
      "Train:\t Perf I \t 0.000605457238950837\n",
      "Train:\t L2 norm of A \t  0.6176112235973397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.004848963430448702\n",
      "Train:\t Perf B \t 0.0017014012154406527\n",
      "Train:\t Perf I \t 0.0008109387058200746\n",
      "Train:\t L2 norm of A \t  0.5555706793146559\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.002564204211487283\n",
      "Train:\t Perf B \t 0.0016958410673917443\n",
      "Train:\t Perf I \t 0.0008611788392568362\n",
      "Train:\t L2 norm of A \t  0.4960196321721698\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.003847047369898821\n",
      "Train:\t Perf B \t 0.001399101970724089\n",
      "Train:\t Perf I \t 0.0006756502331192538\n",
      "Train:\t L2 norm of A \t  0.5814619227483944\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.001994613561214107\n",
      "Train:\t Perf B \t 0.00147754749229816\n",
      "Train:\t Perf I \t 0.0008077279926990962\n",
      "Train:\t L2 norm of A \t  0.6357464272039706\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.0027505118245762013\n",
      "Train:\t Perf B \t 0.0016577694379292787\n",
      "Train:\t Perf I \t 0.0008123328380143154\n",
      "Train:\t L2 norm of A \t  0.5402802409227166\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.0017230010040344042\n",
      "Train:\t Perf B \t 0.0012520326719819195\n",
      "Train:\t Perf I \t 0.0006961447159352338\n",
      "Train:\t L2 norm of A \t  0.6292323166204802\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.0020807819268217633\n",
      "Train:\t Perf B \t 0.0013342584366748185\n",
      "Train:\t Perf I \t 0.000700601468123754\n",
      "Train:\t L2 norm of A \t  0.6287552199273668\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.002802518635883236\n",
      "Train:\t Perf B \t 0.0015224050294293137\n",
      "Train:\t Perf I \t 0.0008487537866697684\n",
      "Train:\t L2 norm of A \t  0.5948619450906504\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.0014495301030198863\n",
      "Train:\t Perf B \t 0.0013256490838382806\n",
      "Train:\t Perf I \t 0.0006910461791214395\n",
      "Train:\t L2 norm of A \t  0.6129764454409475\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.0017568291476426257\n",
      "Train:\t Perf B \t 0.0018381212323133728\n",
      "Train:\t Perf I \t 0.0008867260245021823\n",
      "Train:\t L2 norm of A \t  0.4666052227277435\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.002098565494347617\n",
      "Train:\t Perf B \t 0.0015422878058995902\n",
      "Train:\t Perf I \t 0.0007934560627436642\n",
      "Train:\t L2 norm of A \t  0.5815640040260853\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.001960234611250814\n",
      "Train:\t Perf B \t 0.0015642313116071454\n",
      "Train:\t Perf I \t 0.0007831243929918211\n",
      "Train:\t L2 norm of A \t  0.5437402057595994\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.004730418953738941\n",
      "Train:\t Perf B \t 0.00168387451990948\n",
      "Train:\t Perf I \t 0.0008679636363911373\n",
      "Train:\t L2 norm of A \t  0.5382547862244715\n",
      "-------------- The gamma 0.470000\n",
      "Train:\tLoss \t  0.002277609307965366\n",
      "Train:\t Perf B \t 0.0019143244337746282\n",
      "Train:\t Perf I \t 0.0009571643352762793\n",
      "Train:\t L2 norm of A \t  0.48338766390474486\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.0017200499309641772\n",
      "Train:\t Perf B \t 0.0015123134523490225\n",
      "Train:\t Perf I \t 0.0007723915690519351\n",
      "Train:\t L2 norm of A \t  0.5773023131779429\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.0016329829118270694\n",
      "Train:\t Perf B \t 0.0018565296226581136\n",
      "Train:\t Perf I \t 0.0009081063245560757\n",
      "Train:\t L2 norm of A \t  0.4986069474063298\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.0028442178441101854\n",
      "Train:\t Perf B \t 0.0015269461317214653\n",
      "Train:\t Perf I \t 0.0007627129394796876\n",
      "Train:\t L2 norm of A \t  0.5816063019324295\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.006871767327317178\n",
      "Train:\t Perf B \t 0.001987939678345363\n",
      "Train:\t Perf I \t 0.0009908664110062975\n",
      "Train:\t L2 norm of A \t  0.4445170461315522\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.002611390407427074\n",
      "Train:\t Perf B \t 0.0014146549524010768\n",
      "Train:\t Perf I \t 0.0006844582755722513\n",
      "Train:\t L2 norm of A \t  0.5803042923505718\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.003806035887947205\n",
      "Train:\t Perf B \t 0.0012977802426450157\n",
      "Train:\t Perf I \t 0.0005722098734000798\n",
      "Train:\t L2 norm of A \t  0.6112537584463699\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.0016795211002393288\n",
      "Train:\t Perf B \t 0.0014510584129890332\n",
      "Train:\t Perf I \t 0.0007286050001325892\n",
      "Train:\t L2 norm of A \t  0.5858991259794063\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.001653807509424547\n",
      "Train:\t Perf B \t 0.0014119396259153958\n",
      "Train:\t Perf I \t 0.0007517841478134123\n",
      "Train:\t L2 norm of A \t  0.583036682864162\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.003508127539357568\n",
      "Train:\t Perf B \t 0.0011952534535886869\n",
      "Train:\t Perf I \t 0.0006316157794681255\n",
      "Train:\t L2 norm of A \t  0.6323209805114731\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.0018662588399228654\n",
      "Train:\t Perf B \t 0.0012204884115757713\n",
      "Train:\t Perf I \t 0.0006649318568224065\n",
      "Train:\t L2 norm of A \t  0.650378297312774\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.003170849284101192\n",
      "Train:\t Perf B \t 0.0016709090767094833\n",
      "Train:\t Perf I \t 0.0008425136094518781\n",
      "Train:\t L2 norm of A \t  0.5593105551435933\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.0014337005832256121\n",
      "Train:\t Perf B \t 0.0014733356704640407\n",
      "Train:\t Perf I \t 0.0007261061444213973\n",
      "Train:\t L2 norm of A \t  0.5963760646371353\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.0016739516822056625\n",
      "Train:\t Perf B \t 0.0016535709433890403\n",
      "Train:\t Perf I \t 0.0007479498790968152\n",
      "Train:\t L2 norm of A \t  0.517661225378805\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.0017179221620477719\n",
      "Train:\t Perf B \t 0.001463493844535583\n",
      "Train:\t Perf I \t 0.0006391324653123144\n",
      "Train:\t L2 norm of A \t  0.6220880674731075\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.00508253739317764\n",
      "Train:\t Perf B \t 0.0015602168126641462\n",
      "Train:\t Perf I \t 0.0008026188436128414\n",
      "Train:\t L2 norm of A \t  0.5400059354156664\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.002878988843402541\n",
      "Train:\t Perf B \t 0.001830693885184158\n",
      "Train:\t Perf I \t 0.0009828395556375617\n",
      "Train:\t L2 norm of A \t  0.5262173053615627\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.001925543210474893\n",
      "Train:\t Perf B \t 0.0017653939145850991\n",
      "Train:\t Perf I \t 0.0008711541701621952\n",
      "Train:\t L2 norm of A \t  0.5160701783385263\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.0023321864084335526\n",
      "Train:\t Perf B \t 0.001547315534931338\n",
      "Train:\t Perf I \t 0.000719875155643779\n",
      "Train:\t L2 norm of A \t  0.5988396160318732\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.0015912416568209802\n",
      "Train:\t Perf B \t 0.0013716626018410478\n",
      "Train:\t Perf I \t 0.0006266907890518206\n",
      "Train:\t L2 norm of A \t  0.6249677396201154\n",
      "-------------- The gamma 0.480000\n",
      "Train:\tLoss \t  0.0037209953994575408\n",
      "Train:\t Perf B \t 0.001403002420633802\n",
      "Train:\t Perf I \t 0.0007903452521774999\n",
      "Train:\t L2 norm of A \t  0.5832315354425519\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.0027742619796127534\n",
      "Train:\t Perf B \t 0.0011305101439564344\n",
      "Train:\t Perf I \t 0.0006050392640247664\n",
      "Train:\t L2 norm of A \t  0.6565992402789395\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.0013478091404459888\n",
      "Train:\t Perf B \t 0.0018484032665535347\n",
      "Train:\t Perf I \t 0.0008941066591119001\n",
      "Train:\t L2 norm of A \t  0.4974566721967126\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.006246429628697803\n",
      "Train:\t Perf B \t 0.0014309822868846214\n",
      "Train:\t Perf I \t 0.0007405054822243839\n",
      "Train:\t L2 norm of A \t  0.6000341579766668\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.005406553738289599\n",
      "Train:\t Perf B \t 0.0013016802204060037\n",
      "Train:\t Perf I \t 0.0006481630126535833\n",
      "Train:\t L2 norm of A \t  0.6491205218329575\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.004086673548565724\n",
      "Train:\t Perf B \t 0.001452104265130021\n",
      "Train:\t Perf I \t 0.0007096232673643071\n",
      "Train:\t L2 norm of A \t  0.6017913414619664\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.003333575854267845\n",
      "Train:\t Perf B \t 0.0015746641627712454\n",
      "Train:\t Perf I \t 0.0007671173027223378\n",
      "Train:\t L2 norm of A \t  0.5382899788608865\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.0014136261509909115\n",
      "Train:\t Perf B \t 0.0014395694471421336\n",
      "Train:\t Perf I \t 0.0006885363803043885\n",
      "Train:\t L2 norm of A \t  0.5789946357217115\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.0053642798888277774\n",
      "Train:\t Perf B \t 0.0015526515227254868\n",
      "Train:\t Perf I \t 0.0007684315403103148\n",
      "Train:\t L2 norm of A \t  0.5683455496703083\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.0033255175250570387\n",
      "Train:\t Perf B \t 0.0015429901024697958\n",
      "Train:\t Perf I \t 0.0007278623248322035\n",
      "Train:\t L2 norm of A \t  0.5807355877131313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.001799196855664377\n",
      "Train:\t Perf B \t 0.0014984589661930113\n",
      "Train:\t Perf I \t 0.0007444532380887285\n",
      "Train:\t L2 norm of A \t  0.5990311262826202\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.0029877465119092683\n",
      "Train:\t Perf B \t 0.0013220537322656582\n",
      "Train:\t Perf I \t 0.0006902099982392463\n",
      "Train:\t L2 norm of A \t  0.6242156245617556\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.0025158305002907656\n",
      "Train:\t Perf B \t 0.0011729762504069992\n",
      "Train:\t Perf I \t 0.0006111412753989114\n",
      "Train:\t L2 norm of A \t  0.6090874621718697\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.002303270843691982\n",
      "Train:\t Perf B \t 0.0017372314538450426\n",
      "Train:\t Perf I \t 0.0008771999613825544\n",
      "Train:\t L2 norm of A \t  0.5512655745742772\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.001316695906606383\n",
      "Train:\t Perf B \t 0.0014233780743679522\n",
      "Train:\t Perf I \t 0.0006782618124468075\n",
      "Train:\t L2 norm of A \t  0.5975062204374266\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.001624930575361435\n",
      "Train:\t Perf B \t 0.0016161646419977807\n",
      "Train:\t Perf I \t 0.0007793958757359282\n",
      "Train:\t L2 norm of A \t  0.525918217568401\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.00543812590383866\n",
      "Train:\t Perf B \t 0.0015246733463112187\n",
      "Train:\t Perf I \t 0.0007978815670474184\n",
      "Train:\t L2 norm of A \t  0.6718254892841911\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.004757025801961419\n",
      "Train:\t Perf B \t 0.00159297634073953\n",
      "Train:\t Perf I \t 0.0007532662348809363\n",
      "Train:\t L2 norm of A \t  0.5383762239753137\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.0016340109677900588\n",
      "Train:\t Perf B \t 0.0011506585779462518\n",
      "Train:\t Perf I \t 0.0005893231764782669\n",
      "Train:\t L2 norm of A \t  0.6286725449368542\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.006648880424776845\n",
      "Train:\t Perf B \t 0.0013882304313420956\n",
      "Train:\t Perf I \t 0.000647588131720863\n",
      "Train:\t L2 norm of A \t  0.6209351062917828\n",
      "-------------- The gamma 0.490000\n",
      "Train:\tLoss \t  0.0035795183116253677\n",
      "Train:\t Perf B \t 0.0016977808898458189\n",
      "Train:\t Perf I \t 0.0008991748398456075\n",
      "Train:\t L2 norm of A \t  0.5293043274882854\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.004087644705349833\n",
      "Train:\t Perf B \t 0.0018050319943698377\n",
      "Train:\t Perf I \t 0.0009430564836185323\n",
      "Train:\t L2 norm of A \t  0.5302196449016425\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.002907783039779405\n",
      "Train:\t Perf B \t 0.0013510452182796053\n",
      "Train:\t Perf I \t 0.0006327234957703982\n",
      "Train:\t L2 norm of A \t  0.6443587820473018\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0034699156767865028\n",
      "Train:\t Perf B \t 0.0011845003759315097\n",
      "Train:\t Perf I \t 0.0006614611629837389\n",
      "Train:\t L2 norm of A \t  0.6484967151368382\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0025229812704371203\n",
      "Train:\t Perf B \t 0.0017621604010200481\n",
      "Train:\t Perf I \t 0.0008775551301388545\n",
      "Train:\t L2 norm of A \t  0.5004980994794562\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0014584165655894188\n",
      "Train:\t Perf B \t 0.0015710384995600687\n",
      "Train:\t Perf I \t 0.0007844191376505666\n",
      "Train:\t L2 norm of A \t  0.5456019321956203\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0027070665645441312\n",
      "Train:\t Perf B \t 0.0009392651317639989\n",
      "Train:\t Perf I \t 0.000647428117983943\n",
      "Train:\t L2 norm of A \t  0.7147447659340358\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0025735157136408697\n",
      "Train:\t Perf B \t 0.0014691871155945563\n",
      "Train:\t Perf I \t 0.0007175386256062918\n",
      "Train:\t L2 norm of A \t  0.632623314586859\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0028914962072754116\n",
      "Train:\t Perf B \t 0.0011654010500590634\n",
      "Train:\t Perf I \t 0.0006166573248080938\n",
      "Train:\t L2 norm of A \t  0.6346983815565511\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0039721132638553\n",
      "Train:\t Perf B \t 0.0017317588468221449\n",
      "Train:\t Perf I \t 0.0008483758198780082\n",
      "Train:\t L2 norm of A \t  0.5008010874324843\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0031455933774593224\n",
      "Train:\t Perf B \t 0.0012497939331839708\n",
      "Train:\t Perf I \t 0.000653087276424927\n",
      "Train:\t L2 norm of A \t  0.6500392027598093\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0013560907406882927\n",
      "Train:\t Perf B \t 0.0015683632212213931\n",
      "Train:\t Perf I \t 0.000808559902970491\n",
      "Train:\t L2 norm of A \t  0.5747899388788776\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.001356172090975344\n",
      "Train:\t Perf B \t 0.0013512090333936252\n",
      "Train:\t Perf I \t 0.0006848439028911269\n",
      "Train:\t L2 norm of A \t  0.6018551271647318\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.006734211984361486\n",
      "Train:\t Perf B \t 0.0013908896048692044\n",
      "Train:\t Perf I \t 0.0007314124239741794\n",
      "Train:\t L2 norm of A \t  0.6089390755898594\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0016225318381164985\n",
      "Train:\t Perf B \t 0.0013633670522704621\n",
      "Train:\t Perf I \t 0.0006829616406095903\n",
      "Train:\t L2 norm of A \t  0.6046927498129337\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.002103111863516399\n",
      "Train:\t Perf B \t 0.0012880217250480857\n",
      "Train:\t Perf I \t 0.0006809489086239947\n",
      "Train:\t L2 norm of A \t  0.5988072887810708\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.002443673642677195\n",
      "Train:\t Perf B \t 0.0016902467891998542\n",
      "Train:\t Perf I \t 0.0007761956464825169\n",
      "Train:\t L2 norm of A \t  0.5315333438068618\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0033786827930769227\n",
      "Train:\t Perf B \t 0.0018135734167493037\n",
      "Train:\t Perf I \t 0.0009209416905960877\n",
      "Train:\t L2 norm of A \t  0.48447763713790093\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0025440476313395867\n",
      "Train:\t Perf B \t 0.0014792207160839307\n",
      "Train:\t Perf I \t 0.0007186906606322498\n",
      "Train:\t L2 norm of A \t  0.5764828385843899\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.0015153281262290893\n",
      "Train:\t Perf B \t 0.0013427767305744106\n",
      "Train:\t Perf I \t 0.0006728188769883219\n",
      "Train:\t L2 norm of A \t  0.6035755323080043\n",
      "-------------- The gamma 0.500000\n",
      "Train:\tLoss \t  0.005335980039264423\n",
      "Train:\t Perf B \t 0.0014495292364860143\n",
      "Train:\t Perf I \t 0.0007756112247562289\n",
      "Train:\t L2 norm of A \t  0.6274887185051837\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.0016069900955028068\n",
      "Train:\t Perf B \t 0.00141831831043134\n",
      "Train:\t Perf I \t 0.0006895602533598482\n",
      "Train:\t L2 norm of A \t  0.5868209625663123\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.004020507239377829\n",
      "Train:\t Perf B \t 0.0012564989804806706\n",
      "Train:\t Perf I \t 0.0005787836033843328\n",
      "Train:\t L2 norm of A \t  0.61207423858599\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.004805423407741017\n",
      "Train:\t Perf B \t 0.0015974245679562255\n",
      "Train:\t Perf I \t 0.0007052636729293747\n",
      "Train:\t L2 norm of A \t  0.5732997564967721\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.0025626944582442172\n",
      "Train:\t Perf B \t 0.001422270543074907\n",
      "Train:\t Perf I \t 0.0007558723042133498\n",
      "Train:\t L2 norm of A \t  0.5906283928145135\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.002320890882501252\n",
      "Train:\t Perf B \t 0.00119992007501555\n",
      "Train:\t Perf I \t 0.000589176243986652\n",
      "Train:\t L2 norm of A \t  0.6120019093554806\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.0017352222389940082\n",
      "Train:\t Perf B \t 0.0013892559308047072\n",
      "Train:\t Perf I \t 0.000662462826977286\n",
      "Train:\t L2 norm of A \t  0.5903630059820437\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.004377041604066788\n",
      "Train:\t Perf B \t 0.0016055155704719419\n",
      "Train:\t Perf I \t 0.0007845907889834243\n",
      "Train:\t L2 norm of A \t  0.5700505246826347\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.0019052908299207025\n",
      "Train:\t Perf B \t 0.0013018761078500082\n",
      "Train:\t Perf I \t 0.0006934251470041958\n",
      "Train:\t L2 norm of A \t  0.6365748918716078\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.0017737374894014745\n",
      "Train:\t Perf B \t 0.0014253476908912795\n",
      "Train:\t Perf I \t 0.0007261595233247366\n",
      "Train:\t L2 norm of A \t  0.5861255344676213\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.0025513995776539973\n",
      "Train:\t Perf B \t 0.0013416539834973896\n",
      "Train:\t Perf I \t 0.0006734643878024423\n",
      "Train:\t L2 norm of A \t  0.6151846299738892\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.0024281697313129585\n",
      "Train:\t Perf B \t 0.0019245352615195352\n",
      "Train:\t Perf I \t 0.0011272105894967288\n",
      "Train:\t L2 norm of A \t  0.4673082102566308\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.002417824844468556\n",
      "Train:\t Perf B \t 0.0016415861011129394\n",
      "Train:\t Perf I \t 0.0008438869597985591\n",
      "Train:\t L2 norm of A \t  0.5394331211238573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.003211577078552968\n",
      "Train:\t Perf B \t 0.001185524047885565\n",
      "Train:\t Perf I \t 0.0006239644604909206\n",
      "Train:\t L2 norm of A \t  0.6437654518305792\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.0034165547561655183\n",
      "Train:\t Perf B \t 0.0015444242735938429\n",
      "Train:\t Perf I \t 0.0008298298762634998\n",
      "Train:\t L2 norm of A \t  0.621152360494358\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.005144951615490079\n",
      "Train:\t Perf B \t 0.0017237068106959567\n",
      "Train:\t Perf I \t 0.0009085652103458582\n",
      "Train:\t L2 norm of A \t  0.5535651400005072\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.0022351397309374205\n",
      "Train:\t Perf B \t 0.0018737707387827167\n",
      "Train:\t Perf I \t 0.0008991209855004435\n",
      "Train:\t L2 norm of A \t  0.4950771512454322\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.0020363818302755344\n",
      "Train:\t Perf B \t 0.0013886196404966978\n",
      "Train:\t Perf I \t 0.0007467113311048884\n",
      "Train:\t L2 norm of A \t  0.615486848739846\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.002302955059937443\n",
      "Train:\t Perf B \t 0.0017530541406364974\n",
      "Train:\t Perf I \t 0.00092052025289203\n",
      "Train:\t L2 norm of A \t  0.5171163305804352\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.0022835038769635167\n",
      "Train:\t Perf B \t 0.0012699291824262706\n",
      "Train:\t Perf I \t 0.000641692503667015\n",
      "Train:\t L2 norm of A \t  0.6121341457822823\n",
      "-------------- The gamma 0.510000\n",
      "Train:\tLoss \t  0.0017727644793724413\n",
      "Train:\t Perf B \t 0.0012279478480240124\n",
      "Train:\t Perf I \t 0.0006024255339743769\n",
      "Train:\t L2 norm of A \t  0.6507719823175687\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0025505966404461584\n",
      "Train:\t Perf B \t 0.001253961062689282\n",
      "Train:\t Perf I \t 0.0006768979258361333\n",
      "Train:\t L2 norm of A \t  0.6207022655395015\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0021607120744909234\n",
      "Train:\t Perf B \t 0.0005949100497931386\n",
      "Train:\t Perf I \t 0.0005258610062262178\n",
      "Train:\t L2 norm of A \t  0.7155725437536998\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0029318994350766166\n",
      "Train:\t Perf B \t 0.0012410696114545843\n",
      "Train:\t Perf I \t 0.000631882642984304\n",
      "Train:\t L2 norm of A \t  0.6220550342635993\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0016229116935041372\n",
      "Train:\t Perf B \t 0.0012788692056549813\n",
      "Train:\t Perf I \t 0.0006151158543725867\n",
      "Train:\t L2 norm of A \t  0.5993405060603126\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0026265474370122835\n",
      "Train:\t Perf B \t 0.0016582406750257138\n",
      "Train:\t Perf I \t 0.0008038923258487069\n",
      "Train:\t L2 norm of A \t  0.5776339717349888\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0023374634858728356\n",
      "Train:\t Perf B \t 0.0013149420765939854\n",
      "Train:\t Perf I \t 0.0006665906231574109\n",
      "Train:\t L2 norm of A \t  0.6070529506095323\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0014644917252418412\n",
      "Train:\t Perf B \t 0.0016046653970296612\n",
      "Train:\t Perf I \t 0.0007865259772029989\n",
      "Train:\t L2 norm of A \t  0.565810040275423\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0022625308110052686\n",
      "Train:\t Perf B \t 0.001326772726522226\n",
      "Train:\t Perf I \t 0.0006381420282869827\n",
      "Train:\t L2 norm of A \t  0.5930381410829716\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0074447806983083675\n",
      "Train:\t Perf B \t 0.0012542263464177241\n",
      "Train:\t Perf I \t 0.0006579320973458307\n",
      "Train:\t L2 norm of A \t  0.6206419845318767\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0018818972358693089\n",
      "Train:\t Perf B \t 0.0013268267175070778\n",
      "Train:\t Perf I \t 0.000685960847946165\n",
      "Train:\t L2 norm of A \t  0.6027194642152716\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0033320774937064274\n",
      "Train:\t Perf B \t 0.001700237064016639\n",
      "Train:\t Perf I \t 0.0009252443398951631\n",
      "Train:\t L2 norm of A \t  0.4957283738674169\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0033025842305271953\n",
      "Train:\t Perf B \t 0.0014536606862210351\n",
      "Train:\t Perf I \t 0.0006440978281406984\n",
      "Train:\t L2 norm of A \t  0.5787735573268735\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0024825033547287954\n",
      "Train:\t Perf B \t 0.0011975660616391425\n",
      "Train:\t Perf I \t 0.0006040248565174122\n",
      "Train:\t L2 norm of A \t  0.612935545146095\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0020064379107385477\n",
      "Train:\t Perf B \t 0.0013681343302201793\n",
      "Train:\t Perf I \t 0.000720353313931123\n",
      "Train:\t L2 norm of A \t  0.6056021530551441\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0025174276006238562\n",
      "Train:\t Perf B \t 0.0011379782381013258\n",
      "Train:\t Perf I \t 0.0005882152893560897\n",
      "Train:\t L2 norm of A \t  0.6505087697429172\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0018263577404406396\n",
      "Train:\t Perf B \t 0.00151720294044763\n",
      "Train:\t Perf I \t 0.0007710982960904882\n",
      "Train:\t L2 norm of A \t  0.5709699794652009\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.004535324719884231\n",
      "Train:\t Perf B \t 0.0015044510649403092\n",
      "Train:\t Perf I \t 0.000674976528379925\n",
      "Train:\t L2 norm of A \t  0.5723617480028922\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0026228081382364088\n",
      "Train:\t Perf B \t 0.0013763564350729013\n",
      "Train:\t Perf I \t 0.0006965056340039856\n",
      "Train:\t L2 norm of A \t  0.6154575652086179\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.0014726873822752022\n",
      "Train:\t Perf B \t 0.0016869588553668912\n",
      "Train:\t Perf I \t 0.0008753188375294587\n",
      "Train:\t L2 norm of A \t  0.5125127446982407\n",
      "-------------- The gamma 0.520000\n",
      "Train:\tLoss \t  0.002254595029072577\n",
      "Train:\t Perf B \t 0.0014623190271535092\n",
      "Train:\t Perf I \t 0.0006938951777140117\n",
      "Train:\t L2 norm of A \t  0.6132019304576252\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.0036557401548822444\n",
      "Train:\t Perf B \t 0.0007993955409307265\n",
      "Train:\t Perf I \t 0.0006236733378088185\n",
      "Train:\t L2 norm of A \t  0.7865175502027959\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.0014202510632599092\n",
      "Train:\t Perf B \t 0.0016273415815505534\n",
      "Train:\t Perf I \t 0.0008364166353461387\n",
      "Train:\t L2 norm of A \t  0.5556430759624702\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.0023709372694643255\n",
      "Train:\t Perf B \t 0.0016530082839632152\n",
      "Train:\t Perf I \t 0.000898132153062672\n",
      "Train:\t L2 norm of A \t  0.5417286205149112\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.0022875408427138573\n",
      "Train:\t Perf B \t 0.0015528970308419981\n",
      "Train:\t Perf I \t 0.0008264141416499419\n",
      "Train:\t L2 norm of A \t  0.5681591516082908\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.003160322054431512\n",
      "Train:\t Perf B \t 0.0014497542305811641\n",
      "Train:\t Perf I \t 0.0007671761511068819\n",
      "Train:\t L2 norm of A \t  0.5464544754897233\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.005580311613310417\n",
      "Train:\t Perf B \t 0.001334292042977451\n",
      "Train:\t Perf I \t 0.0006279969678866053\n",
      "Train:\t L2 norm of A \t  0.6693873957585394\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.002510906036701106\n",
      "Train:\t Perf B \t 0.0010679461126410924\n",
      "Train:\t Perf I \t 0.0005427942877308153\n",
      "Train:\t L2 norm of A \t  0.6458786895602522\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.002040713651769735\n",
      "Train:\t Perf B \t 0.0009531999804781195\n",
      "Train:\t Perf I \t 0.0005857506405954092\n",
      "Train:\t L2 norm of A \t  0.6608875696605939\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.001576107331681243\n",
      "Train:\t Perf B \t 0.0011800093835949309\n",
      "Train:\t Perf I \t 0.0006419210073020022\n",
      "Train:\t L2 norm of A \t  0.6624908822066633\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.003371757487519796\n",
      "Train:\t Perf B \t 0.0012869267307471124\n",
      "Train:\t Perf I \t 0.0006932561711594513\n",
      "Train:\t L2 norm of A \t  0.6177981532486885\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.0037539088110143497\n",
      "Train:\t Perf B \t 0.0010926578599381805\n",
      "Train:\t Perf I \t 0.0005514975478968429\n",
      "Train:\t L2 norm of A \t  0.7038235722178678\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.006744679406493324\n",
      "Train:\t Perf B \t 0.0013208038329679292\n",
      "Train:\t Perf I \t 0.0006367718857647174\n",
      "Train:\t L2 norm of A \t  0.6837469018428682\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.0020260694119854012\n",
      "Train:\t Perf B \t 0.0012277070685087963\n",
      "Train:\t Perf I \t 0.0006428341976729197\n",
      "Train:\t L2 norm of A \t  0.6356515373374291\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.002882863048186367\n",
      "Train:\t Perf B \t 0.0018732554687146228\n",
      "Train:\t Perf I \t 0.0008714545998685503\n",
      "Train:\t L2 norm of A \t  0.5016218601077699\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.004764967911717269\n",
      "Train:\t Perf B \t 0.0014931405884139634\n",
      "Train:\t Perf I \t 0.0007705906210903996\n",
      "Train:\t L2 norm of A \t  0.5994850410644144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.0032106774938120007\n",
      "Train:\t Perf B \t 0.0013901173746753634\n",
      "Train:\t Perf I \t 0.0006924336135159763\n",
      "Train:\t L2 norm of A \t  0.6298111408506778\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.0034182414804710317\n",
      "Train:\t Perf B \t 0.001030117283176628\n",
      "Train:\t Perf I \t 0.0005391627905683738\n",
      "Train:\t L2 norm of A \t  0.6373276241478788\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.002967054984569397\n",
      "Train:\t Perf B \t 0.0012867043686822582\n",
      "Train:\t Perf I \t 0.0006184653499299271\n",
      "Train:\t L2 norm of A \t  0.646516886252099\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.001614024059424084\n",
      "Train:\t Perf B \t 0.0016432896483980994\n",
      "Train:\t Perf I \t 0.0008413081305459605\n",
      "Train:\t L2 norm of A \t  0.5382653317050945\n",
      "-------------- The gamma 0.530000\n",
      "Train:\tLoss \t  0.0018084905464427802\n",
      "Train:\t Perf B \t 0.0013077884508235409\n",
      "Train:\t Perf I \t 0.0006380301997396863\n",
      "Train:\t L2 norm of A \t  0.5990954957812609\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.0040579168470692446\n",
      "Train:\t Perf B \t 0.0012308200606693726\n",
      "Train:\t Perf I \t 0.0006318874907546411\n",
      "Train:\t L2 norm of A \t  0.6307109139575527\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.0018823342726715271\n",
      "Train:\t Perf B \t 0.0013049005106324388\n",
      "Train:\t Perf I \t 0.0006261561065551066\n",
      "Train:\t L2 norm of A \t  0.671128959117355\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.002078597831995586\n",
      "Train:\t Perf B \t 0.0012820209404496793\n",
      "Train:\t Perf I \t 0.0006101622669868672\n",
      "Train:\t L2 norm of A \t  0.5918802769922936\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.004062732389994525\n",
      "Train:\t Perf B \t 0.0015154169812378097\n",
      "Train:\t Perf I \t 0.0007444167984125427\n",
      "Train:\t L2 norm of A \t  0.5747383933145592\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.0025913566643147945\n",
      "Train:\t Perf B \t 0.0016901897324035778\n",
      "Train:\t Perf I \t 0.000797839992851604\n",
      "Train:\t L2 norm of A \t  0.5214323941366679\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.0030439019493947723\n",
      "Train:\t Perf B \t 0.0010883592586549387\n",
      "Train:\t Perf I \t 0.0005785698613730164\n",
      "Train:\t L2 norm of A \t  0.6860001698368121\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.005360413603517163\n",
      "Train:\t Perf B \t 0.0011252836375595032\n",
      "Train:\t Perf I \t 0.0006597628443893723\n",
      "Train:\t L2 norm of A \t  0.6244041919906143\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.0025902043833661965\n",
      "Train:\t Perf B \t 0.0012807520193299268\n",
      "Train:\t Perf I \t 0.0007041344555553252\n",
      "Train:\t L2 norm of A \t  0.6231663992770685\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.0024275241559444096\n",
      "Train:\t Perf B \t 0.001457083572321087\n",
      "Train:\t Perf I \t 0.0006861300462514407\n",
      "Train:\t L2 norm of A \t  0.5950785588379441\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.001828117824649727\n",
      "Train:\t Perf B \t 0.0014038695336340316\n",
      "Train:\t Perf I \t 0.0007002343383508513\n",
      "Train:\t L2 norm of A \t  0.5894218922867915\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.002003979896324233\n",
      "Train:\t Perf B \t 0.0014584217313754688\n",
      "Train:\t Perf I \t 0.0007158651767435939\n",
      "Train:\t L2 norm of A \t  0.6028461286805221\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.0021252297850518184\n",
      "Train:\t Perf B \t 0.0012633397921717985\n",
      "Train:\t Perf I \t 0.0005517029739618667\n",
      "Train:\t L2 norm of A \t  0.6168926083507064\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.0020804895855042922\n",
      "Train:\t Perf B \t 0.0014535729336518307\n",
      "Train:\t Perf I \t 0.000710197779228151\n",
      "Train:\t L2 norm of A \t  0.586040509746348\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.001462567352995041\n",
      "Train:\t Perf B \t 0.0013578690841483376\n",
      "Train:\t Perf I \t 0.0006751247016762708\n",
      "Train:\t L2 norm of A \t  0.60917625425305\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.0015358406726882468\n",
      "Train:\t Perf B \t 0.0013444544373298337\n",
      "Train:\t Perf I \t 0.0006910721157841625\n",
      "Train:\t L2 norm of A \t  0.5780965108284177\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.0068911906031942395\n",
      "Train:\t Perf B \t 0.0014896483266244724\n",
      "Train:\t Perf I \t 0.0007019774344789367\n",
      "Train:\t L2 norm of A \t  0.5973611270852138\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.0015350452548928291\n",
      "Train:\t Perf B \t 0.0011985068223459875\n",
      "Train:\t Perf I \t 0.0005990738209946769\n",
      "Train:\t L2 norm of A \t  0.6148266288974955\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.004122651584222442\n",
      "Train:\t Perf B \t 0.0011178429994097708\n",
      "Train:\t Perf I \t 0.0005812650306686092\n",
      "Train:\t L2 norm of A \t  0.6331157502561627\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.003317730353480118\n",
      "Train:\t Perf B \t 0.0006380373262690175\n",
      "Train:\t Perf I \t 0.00048291206912355925\n",
      "Train:\t L2 norm of A \t  0.819450069671346\n",
      "-------------- The gamma 0.540000\n",
      "Train:\tLoss \t  0.003254785396606134\n",
      "Train:\t Perf B \t 0.0018608878802214882\n",
      "Train:\t Perf I \t 0.0008946151029741624\n",
      "Train:\t L2 norm of A \t  0.538030063534396\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.004535387145805941\n",
      "Train:\t Perf B \t 0.0011636101979901469\n",
      "Train:\t Perf I \t 0.0006112489137058835\n",
      "Train:\t L2 norm of A \t  0.6419391027344278\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.002483141606865701\n",
      "Train:\t Perf B \t 0.0009250005420373561\n",
      "Train:\t Perf I \t 0.0006041947873961094\n",
      "Train:\t L2 norm of A \t  0.7080364899891793\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.0031943942597244734\n",
      "Train:\t Perf B \t 0.0016593756605103168\n",
      "Train:\t Perf I \t 0.0009117033773290248\n",
      "Train:\t L2 norm of A \t  0.5359263061268097\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.0047960334119560456\n",
      "Train:\t Perf B \t 0.0008697679192921249\n",
      "Train:\t Perf I \t 0.0005813379373304204\n",
      "Train:\t L2 norm of A \t  0.6979373982239303\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.002465566991733045\n",
      "Train:\t Perf B \t 0.0007686356374059628\n",
      "Train:\t Perf I \t 0.0007577163134203592\n",
      "Train:\t L2 norm of A \t  0.612249872326806\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.004188608938148914\n",
      "Train:\t Perf B \t 0.0012142712853158014\n",
      "Train:\t Perf I \t 0.0006695861080456487\n",
      "Train:\t L2 norm of A \t  0.668537482754943\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.001876378601540828\n",
      "Train:\t Perf B \t 0.001352261305053172\n",
      "Train:\t Perf I \t 0.0007225984531808775\n",
      "Train:\t L2 norm of A \t  0.6369179376626489\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.002397282868363497\n",
      "Train:\t Perf B \t 0.00132356803818973\n",
      "Train:\t Perf I \t 0.0006774075503301363\n",
      "Train:\t L2 norm of A \t  0.6091914821094068\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.0034997877686632995\n",
      "Train:\t Perf B \t 0.0013427365159261345\n",
      "Train:\t Perf I \t 0.0007005900936514032\n",
      "Train:\t L2 norm of A \t  0.6139133725920167\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.0033222564374147234\n",
      "Train:\t Perf B \t 0.0013430902492847629\n",
      "Train:\t Perf I \t 0.0006242996778486437\n",
      "Train:\t L2 norm of A \t  0.5828710841449026\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.00207153438513115\n",
      "Train:\t Perf B \t 0.001007316088225479\n",
      "Train:\t Perf I \t 0.0005092464761026926\n",
      "Train:\t L2 norm of A \t  0.6965927200811618\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.002310153880808771\n",
      "Train:\t Perf B \t 0.001728140409268173\n",
      "Train:\t Perf I \t 0.000868656165721823\n",
      "Train:\t L2 norm of A \t  0.5386060944065718\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.002216670857775015\n",
      "Train:\t Perf B \t 0.0009252532386907005\n",
      "Train:\t Perf I \t 0.0005271395681764866\n",
      "Train:\t L2 norm of A \t  0.6861357085538594\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.0022409296770956153\n",
      "Train:\t Perf B \t 0.0017505979265818447\n",
      "Train:\t Perf I \t 0.0008942048816280205\n",
      "Train:\t L2 norm of A \t  0.5173706540807302\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.003911283327093491\n",
      "Train:\t Perf B \t 0.0013275561143155277\n",
      "Train:\t Perf I \t 0.0006951238025067074\n",
      "Train:\t L2 norm of A \t  0.6300416769158877\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.0016016157911187495\n",
      "Train:\t Perf B \t 0.0014132712005172866\n",
      "Train:\t Perf I \t 0.0006575558285613757\n",
      "Train:\t L2 norm of A \t  0.5954901806152941\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.0027196933922170215\n",
      "Train:\t Perf B \t 0.001072998033992758\n",
      "Train:\t Perf I \t 0.0006734300866443096\n",
      "Train:\t L2 norm of A \t  0.6898440218971109\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.0031219162235042396\n",
      "Train:\t Perf B \t 0.0014106087352291443\n",
      "Train:\t Perf I \t 0.0006711656858004091\n",
      "Train:\t L2 norm of A \t  0.60832700653136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.005811686341360975\n",
      "Train:\t Perf B \t 0.0017416704771050465\n",
      "Train:\t Perf I \t 0.0008810649606000671\n",
      "Train:\t L2 norm of A \t  0.5192720510175433\n",
      "-------------- The gamma 0.550000\n",
      "Train:\tLoss \t  0.0019857080431726197\n",
      "Train:\t Perf B \t 0.001509017705222954\n",
      "Train:\t Perf I \t 0.0007057363773342699\n",
      "Train:\t L2 norm of A \t  0.567195950164299\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.0031910786695001966\n",
      "Train:\t Perf B \t 0.0012207879109773132\n",
      "Train:\t Perf I \t 0.0005726303170368854\n",
      "Train:\t L2 norm of A \t  0.6417324679454135\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.0015479070310340695\n",
      "Train:\t Perf B \t 0.0008380961206714552\n",
      "Train:\t Perf I \t 0.0005085360827217062\n",
      "Train:\t L2 norm of A \t  0.7294432806638829\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.004960084981433559\n",
      "Train:\t Perf B \t 0.0015038869300061394\n",
      "Train:\t Perf I \t 0.0007432162723001096\n",
      "Train:\t L2 norm of A \t  0.5728662024978575\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.0028425235776057857\n",
      "Train:\t Perf B \t 0.0012217085101882718\n",
      "Train:\t Perf I \t 0.0005702834470993588\n",
      "Train:\t L2 norm of A \t  0.6256501614124911\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.0023143764835681256\n",
      "Train:\t Perf B \t 0.0012834143500192433\n",
      "Train:\t Perf I \t 0.0006550606850194054\n",
      "Train:\t L2 norm of A \t  0.6212552900882353\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.003867068062888095\n",
      "Train:\t Perf B \t 0.0012750019678869895\n",
      "Train:\t Perf I \t 0.0006311380436379228\n",
      "Train:\t L2 norm of A \t  0.6159598312675617\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.0023874901436710195\n",
      "Train:\t Perf B \t 0.0011633540514236055\n",
      "Train:\t Perf I \t 0.0005156191557585683\n",
      "Train:\t L2 norm of A \t  0.6262197712391698\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.002311780054196738\n",
      "Train:\t Perf B \t 0.0012990401460377297\n",
      "Train:\t Perf I \t 0.0007213170620894017\n",
      "Train:\t L2 norm of A \t  0.6256795292394084\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.003759855432970165\n",
      "Train:\t Perf B \t 0.001475805951424701\n",
      "Train:\t Perf I \t 0.0007278896129793989\n",
      "Train:\t L2 norm of A \t  0.593143418693905\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.0020376308360958795\n",
      "Train:\t Perf B \t 0.001424757732662413\n",
      "Train:\t Perf I \t 0.0006945068636894362\n",
      "Train:\t L2 norm of A \t  0.5862585645813123\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.006445451276667591\n",
      "Train:\t Perf B \t 0.0018011923535418016\n",
      "Train:\t Perf I \t 0.0009658617060472415\n",
      "Train:\t L2 norm of A \t  0.512149479875256\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.005048843843832788\n",
      "Train:\t Perf B \t 0.0010081914103202232\n",
      "Train:\t Perf I \t 0.0005615099805199537\n",
      "Train:\t L2 norm of A \t  0.7077079304266677\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.0013581427146375769\n",
      "Train:\t Perf B \t 0.0007117066428501534\n",
      "Train:\t Perf I \t 0.00047598432338311037\n",
      "Train:\t L2 norm of A \t  0.714102442200005\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.0039522418539957825\n",
      "Train:\t Perf B \t 0.001058624789355499\n",
      "Train:\t Perf I \t 0.0005360508157052513\n",
      "Train:\t L2 norm of A \t  0.682244789327808\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.001855808395860688\n",
      "Train:\t Perf B \t 0.0016992817336905107\n",
      "Train:\t Perf I \t 0.0008137068196391758\n",
      "Train:\t L2 norm of A \t  0.5380397504637614\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.0013432558544609285\n",
      "Train:\t Perf B \t 0.0009705716659958227\n",
      "Train:\t Perf I \t 0.0005353610001545456\n",
      "Train:\t L2 norm of A \t  0.6951561319945303\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.004338460445526479\n",
      "Train:\t Perf B \t 0.0016126468174152437\n",
      "Train:\t Perf I \t 0.000830342830890099\n",
      "Train:\t L2 norm of A \t  0.5791225277028569\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.002410392929910075\n",
      "Train:\t Perf B \t 0.0013688583936336115\n",
      "Train:\t Perf I \t 0.000692871343084247\n",
      "Train:\t L2 norm of A \t  0.5747897382414089\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.0033620696751301385\n",
      "Train:\t Perf B \t 0.0016343321631890732\n",
      "Train:\t Perf I \t 0.0007797236282147612\n",
      "Train:\t L2 norm of A \t  0.5508548863731643\n",
      "-------------- The gamma 0.560000\n",
      "Train:\tLoss \t  0.0033254991922395784\n",
      "Train:\t Perf B \t 0.0009669051348486851\n",
      "Train:\t Perf I \t 0.0004841786216034446\n",
      "Train:\t L2 norm of A \t  0.7161833743282535\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.003396760219975347\n",
      "Train:\t Perf B \t 0.0010944387620435375\n",
      "Train:\t Perf I \t 0.0005881774798274133\n",
      "Train:\t L2 norm of A \t  0.6360066520057189\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.003082934324107856\n",
      "Train:\t Perf B \t 0.0007910046667494799\n",
      "Train:\t Perf I \t 0.0005729282341443188\n",
      "Train:\t L2 norm of A \t  0.774807394024055\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.001569826625612082\n",
      "Train:\t Perf B \t 0.0013413743754261856\n",
      "Train:\t Perf I \t 0.0007208415521382437\n",
      "Train:\t L2 norm of A \t  0.63663014596307\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.005629157402013042\n",
      "Train:\t Perf B \t 0.0015805646941021594\n",
      "Train:\t Perf I \t 0.0007652774350512607\n",
      "Train:\t L2 norm of A \t  0.6178233177954073\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.005497400852372692\n",
      "Train:\t Perf B \t 0.0012301706415396715\n",
      "Train:\t Perf I \t 0.0006015793872044515\n",
      "Train:\t L2 norm of A \t  0.6297468615833398\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.0030181236927937782\n",
      "Train:\t Perf B \t 0.0011917377025583446\n",
      "Train:\t Perf I \t 0.0006057120468792867\n",
      "Train:\t L2 norm of A \t  0.6328312288466773\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.003091472154964163\n",
      "Train:\t Perf B \t 0.0013066968107485319\n",
      "Train:\t Perf I \t 0.0006382063590588072\n",
      "Train:\t L2 norm of A \t  0.6077020504491193\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.003629039049076563\n",
      "Train:\t Perf B \t 0.0012643628068769969\n",
      "Train:\t Perf I \t 0.0006548784087370958\n",
      "Train:\t L2 norm of A \t  0.6344125957981741\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.0015289982290787063\n",
      "Train:\t Perf B \t 0.001753714686855291\n",
      "Train:\t Perf I \t 0.0008741369951188353\n",
      "Train:\t L2 norm of A \t  0.5471630749038147\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.0038370770037673718\n",
      "Train:\t Perf B \t 0.0016980566397337314\n",
      "Train:\t Perf I \t 0.0008997258250631571\n",
      "Train:\t L2 norm of A \t  0.51351972705159\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.004121195017886802\n",
      "Train:\t Perf B \t 0.0017546371859648794\n",
      "Train:\t Perf I \t 0.0009154242361872977\n",
      "Train:\t L2 norm of A \t  0.5434043624583799\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.0016705594620531315\n",
      "Train:\t Perf B \t 0.0012104460878616904\n",
      "Train:\t Perf I \t 0.0006040779014361208\n",
      "Train:\t L2 norm of A \t  0.6713736241317387\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.005714107292886789\n",
      "Train:\t Perf B \t 0.001393789609885333\n",
      "Train:\t Perf I \t 0.0006845041636745981\n",
      "Train:\t L2 norm of A \t  0.6130350497988918\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.0029696289212642855\n",
      "Train:\t Perf B \t 0.0009649428176457223\n",
      "Train:\t Perf I \t 0.0005858374438617261\n",
      "Train:\t L2 norm of A \t  0.671750985260729\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.00148639563290934\n",
      "Train:\t Perf B \t 0.0013810413020921184\n",
      "Train:\t Perf I \t 0.0007021368461843647\n",
      "Train:\t L2 norm of A \t  0.6402648011628623\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.0022715817886673016\n",
      "Train:\t Perf B \t 0.0007936581258794624\n",
      "Train:\t Perf I \t 0.0005755294404493151\n",
      "Train:\t L2 norm of A \t  0.7439815453105546\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.002092189887135866\n",
      "Train:\t Perf B \t 0.0012572032125912152\n",
      "Train:\t Perf I \t 0.0006091103527670061\n",
      "Train:\t L2 norm of A \t  0.6300311218759944\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.002237259132069927\n",
      "Train:\t Perf B \t 0.0012989989411803533\n",
      "Train:\t Perf I \t 0.0006706372437791586\n",
      "Train:\t L2 norm of A \t  0.6264017178374227\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.004031761242036954\n",
      "Train:\t Perf B \t 0.0018124532004050418\n",
      "Train:\t Perf I \t 0.0008430685075043762\n",
      "Train:\t L2 norm of A \t  0.5394568213070053\n",
      "-------------- The gamma 0.570000\n",
      "Train:\tLoss \t  0.001974545702012066\n",
      "Train:\t Perf B \t 0.0014161920168167487\n",
      "Train:\t Perf I \t 0.000719657916680203\n",
      "Train:\t L2 norm of A \t  0.5512669527875704\n",
      "-------------- The gamma 0.580000\n",
      "Train:\tLoss \t  0.002485545226497828\n",
      "Train:\t Perf B \t 0.0014723971278517703\n",
      "Train:\t Perf I \t 0.0007750333700819084\n",
      "Train:\t L2 norm of A \t  0.621500419691859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- The gamma 0.580000\n",
      "Train:\tLoss \t  0.001480432215972528\n",
      "Train:\t Perf B \t 0.0013787601888329418\n",
      "Train:\t Perf I \t 0.0007527991875343826\n",
      "Train:\t L2 norm of A \t  0.5994028932011694\n",
      "-------------- The gamma 0.580000\n",
      "Train:\tLoss \t  0.0030420742479319216\n",
      "Train:\t Perf B \t 0.0011565814864641223\n",
      "Train:\t Perf I \t 0.0007235694968384729\n",
      "Train:\t L2 norm of A \t  0.6578107938933088\n",
      "-------------- The gamma 0.580000\n",
      "Train:\tLoss \t  0.001988802733295653\n",
      "Train:\t Perf B \t 0.001422932609781771\n",
      "Train:\t Perf I \t 0.0006851848422651507\n",
      "Train:\t L2 norm of A \t  0.6168815167955362\n",
      "-------------- The gamma 0.580000\n",
      "Train:\tLoss \t  0.0019657326886306295\n",
      "Train:\t Perf B \t 0.0014186220197256886\n",
      "Train:\t Perf I \t 0.0006456005394687874\n",
      "Train:\t L2 norm of A \t  0.6144967044194417\n",
      "-------------- The gamma 0.580000\n",
      "Train:\tLoss \t  0.0023822214309466715\n",
      "Train:\t Perf B \t 0.001345909281967371\n",
      "Train:\t Perf I \t 0.000663772180511223\n",
      "Train:\t L2 norm of A \t  0.6274217202935805\n",
      "-------------- The gamma 0.580000\n",
      "Train:\tLoss \t  0.00299391187346652\n",
      "Train:\t Perf B \t 0.0016898959496701768\n",
      "Train:\t Perf I \t 0.0008535106518768246\n",
      "Train:\t L2 norm of A \t  0.5522102615008703\n",
      "-------------- The gamma 0.580000\n",
      "Train:\tLoss \t  0.0018326393804288144\n",
      "Train:\t Perf B \t 0.0015844141955569811\n",
      "Train:\t Perf I \t 0.0007190631954332783\n",
      "Train:\t L2 norm of A \t  0.5639088694826746\n",
      "-------------- The gamma 0.580000\n",
      "Train:\tLoss \t  0.003290659011421597\n",
      "Train:\t Perf B \t 0.0013181231251759773\n",
      "Train:\t Perf I \t 0.0007155929747081605\n",
      "Train:\t L2 norm of A \t  0.6299279454741104\n",
      "-------------- The gamma 0.580000\n",
      "Train:\tLoss \t  0.0038355355237849596\n",
      "Train:\t Perf B \t 0.0009902130965390933\n",
      "Train:\t Perf I \t 0.0007217937354420991\n",
      "Train:\t L2 norm of A \t  0.6943421044073987\n"
     ]
    }
   ],
   "source": [
    "M_epoch=2000    \n",
    "# Training loop\n",
    "L=[]\n",
    "pss=[]\n",
    "pssb=[]\n",
    "gammax=.7\n",
    "gamdel=.01\n",
    "for gamma in np.arange(0,gammax+gamdel,gamdel):\n",
    "# gamma=gammax\n",
    "    for i in range(20):\n",
    "        rnn = RNN(2, 2, hidden_size=4)\n",
    "        M_epoch=random.randint(1000, 4000)\n",
    "        for epoch in range(M_epoch):\n",
    "            train_loss = rnn.processData(x,backprop=True,g0=gamma)\n",
    "            if epoch % (M_epoch/1) == ((M_epoch/1)- 1):\n",
    "                sigma=.05\n",
    "                yp=rnn.Prediction(x,0,res,seq_len)\n",
    "                yn=rnn.Prediction(x,sigma,res,seq_len)\n",
    "                pss=np.append(pss,((yp-yn)*(yp-yn)).mean(axis=None))\n",
    "                L=np.append(L,((x[seq_len+1:]-yp)*(x[seq_len+1:]-yp)).mean(axis=None))\n",
    "                pssb=np.append(pssb,rnn.Perf_bound()*sigma**2)\n",
    "                print('-------------- The gamma %f' % gamma)\n",
    "                print('Train:\\tLoss \\t ' ,L[-1])\n",
    "                print('Train:\\t Perf B \\t',pssb[-1])\n",
    "                print('Train:\\t Perf I \\t',pss[-1])\n",
    "                print('Train:\\t L2 norm of A \\t ',np.linalg.norm(rnn.Whh,2))\n",
    "\n",
    "                if np.linalg.norm(rnn.Whh,2)>1:\n",
    "                    print('========= Norm problem =========')\n",
    "                    break\n",
    "#             for sigma in np.arange(0,1,1/10):\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3778dc1130>]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gaml=np.linspace(0,gammax,len(L))\n",
    "plt.figure(figsize=(6, 6))\n",
    "# plt.plot(gaml,(pssb),'ro')\n",
    "plt.plot(gaml,pss,'bo')\n",
    "plt.plot(gaml,L,'go')\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(pss,L,'ro')\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(pssb,L,'ro')\n",
    "# plt.plot(gaml,1/gaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open('LPss1.pckl', 'wb')\n",
    "pickle.dump([L,pss,pssb], f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3778ee0130>,\n",
       " <matplotlib.lines.Line2D at 0x7f3778ee0220>]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "perf,perfb=0,0\n",
    "for sigma in np.arange(0,.1,1/50):\n",
    "    yp=Prediction(x,0,res,seq_len)\n",
    "    yn=Prediction(x,sigma,res,seq_len)\n",
    "    perf=np.append(perf,((yp-yn)*(yp-yn)).mean(axis=None))\n",
    "    perfb=np.append(perfb,Perf_bound(RNN)*sigma**2)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(perf)\n",
    "plt.plot(perfb)\n",
    "A,B,C,b,c=rnn.Weights()\n",
    "# mng = plt.get_current_fig_manager()\n",
    "# mng.frame.Maximize(True)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "yp=Prediction(x,0,res,seq_len)\n",
    "yn=Prediction(x,sigma,res,seq_len)\n",
    "plt.plot(x[seq_len:])\n",
    "plt.plot(yp)\n",
    "plt.plot(yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f377a53a970>]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdaElEQVR4nO3dfZRcdZ3n8fe3bj30UzoP3U0SkmAHEtCAo0KMzIoORwZMdCSO4gg6x8wuR8ZzYNdZnXVw3WVY3D+GWVfOsHLGYRZcUBlwmHHNrnEQhxFdDmCaiECAQJMEkhCSTrrJQz9X1Xf/uLc61dXdSTVdXdW59XmdU6du/e6tut++1f25v/7dW3XN3RERkfhK1LoAERGZXQp6EZGYU9CLiMScgl5EJOYU9CIiMZesdQGl2tvbvbOzs9ZliIicVp566qlD7t4x2bw5F/SdnZ10dXXVugwRkdOKmb061TwN3YiIxJyCXkQk5hT0IiIxp6AXEYk5Bb2ISMyVFfRmtt7MdphZt5ndOMn8D5rZNjPLmtlVJfM2mdnL0W1TpQoXEZHynDLozSwA7gA2AGuAa8xsTclirwF/BNxX8txFwJ8D7wPWAX9uZgtnXraIiJSrnB79OqDb3Xe6+whwP7CxeAF33+3uzwD5kud+GHjY3XvdvQ94GFhfgbon2H9kkG/+dAc7e47PxsuLiJy2ygn6ZcCeosd7o7ZylPVcM7vOzLrMrKunp6fMlx6v59gwtz/Szc6e/rf0fBGRuJoTB2Pd/U53X+vuazs6Jv0E7ykFCQMgm9eFVEREipUT9PuAFUWPl0dt5ZjJc6clFYQ/SjZfOnokIlLfygn6rcBqM1tpZmngamBzma//EHCFmS2MDsJeEbVVXLLQo8+pRy8iUuyUQe/uWeAGwoB+AfiBu283s1vM7EoAM3uvme0FPgX8jZltj57bC3ydcGexFbglaqu4ZKLQo1fQi4gUK+vbK919C7ClpO2moumthMMykz33buDuGdRYlmRQ6NFr6EZEpNicOBhbCYWgH1WPXkRknPgEfTR0k1OPXkRknPgEfaDTK0VEJhOboE9FPfpRnXUjIjJObIK+0KPP6Tx6EZFx4hP00Xn06tGLiIwXm6A3M4KE6ZOxIiIlYhP0EPbqdTBWRGS8+AW9hm5ERMaJV9AHCX0yVkSkRKyCPhVo6EZEpFSsgj7Q0I2IyASxCvpkIsGozroRERknVkGfCoychm5ERMaJVdBr6EZEZKJYBX0qSDCqs25ERMaJVdAnNXQjIjJBrII+SCR04RERkRKxCvpUwvSBKRGRErEK+qQ+MCUiMkG8gj6hr0AQESkVr6BXj15EZIJ4BX0iofPoRURKxCzodeEREZFS8Qr6QJ+MFREpFaugTwUJjdGLiJSIVdAHOo9eRGSCWAV9KjB9MlZEpESsgj4dJBjJqkcvIlIsVkGvb68UEZkoVkGfTqpHLyJSqqygN7P1ZrbDzLrN7MZJ5mfM7IFo/pNm1hm1p8zsHjN71sxeMLOvVrj+cdLJ8KybvMbpRUTGnDLozSwA7gA2AGuAa8xsTcli1wJ97r4KuA24NWr/FJBx93cCFwF/XNgJzIZ0MvxxRjR8IyIyppwe/Tqg2913uvsIcD+wsWSZjcA90fSDwGVmZoADzWaWBBqBEeBoRSqfRDoIf5xhDd+IiIwpJ+iXAXuKHu+N2iZdxt2zwBGgjTD0+4H9wGvAN9y9t3QFZnadmXWZWVdPT8+0f4iCQo9eB2RFRE6Y7YOx64AccCawEviymZ1dupC73+nua919bUdHx1teWaFHrwOyIiInlBP0+4AVRY+XR22TLhMN08wHDgOfAf7J3Ufd/SDwGLB2pkVPZWyMXkEvIjKmnKDfCqw2s5VmlgauBjaXLLMZ2BRNXwU84u5OOFzzIQAzawYuBl6sROGT0cFYEZGJThn00Zj7DcBDwAvAD9x9u5ndYmZXRovdBbSZWTfwJaBwCuYdQIuZbSfcYXzH3Z+p9A9RkNLQjYjIBMlyFnL3LcCWkrabiqaHCE+lLH3e8cnaZ4t69CIiE8Xqk7EZ9ehFRCaIVdDrYKyIyEQKehGRmItV0I8djNUYvYjImFgFvT4ZKyIyUbyCXt91IyIyQayCPqMxehGRCWIV9DoYKyIyUayCXgdjRUQmilXQjx2MVY9eRGRMrII+mTDM1KMXESkWq6A3M9KBLhAuIlIsVkEP4fCNTq8UETkhfkEfJDR0IyJSJH5Bn0zoYKyISJFYBr169CIiJ8Qu6DPJBMOjCnoRkYLYBX1DKmAom6t1GSIic0Ysg35wREEvIlIQu6BvTAUM6WCsiMiY2AV9QyrBkHr0IiJjYhf0jamAwVEFvYhIQeyCviEVMKSgFxEZE8ugV49eROSE2AV9Y1o9ehGRYrEL+oZkwGjOyerTsSIiQAyDvjEd/kg6xVJEJBS/oE8FAPrQlIhIJHZBn4mCXuP0IiKh2AV9o4JeRGSc2AV9w1jQa4xeRATKDHozW29mO8ys28xunGR+xsweiOY/aWadRfN+y8weN7PtZvasmTVUsP4Jxsbo1aMXEQHKCHozC4A7gA3AGuAaM1tTsti1QJ+7rwJuA26NnpsEvgd8wd3PBy4FRitW/SQKZ90o6EVEQuX06NcB3e6+091HgPuBjSXLbATuiaYfBC4zMwOuAJ5x998AuPthd5/VBM4kNUYvIlKsnKBfBuwperw3apt0GXfPAkeANuBcwM3sITPbZmZfmWwFZnadmXWZWVdPT890f4ZxGtMKehGRYrN9MDYJXAJ8Nrr/fTO7rHQhd7/T3de6+9qOjo4ZrVDn0YuIjFdO0O8DVhQ9Xh61TbpMNC4/HzhM2Pv/hbsfcvcBYAtw4UyLPhkdjBURGa+coN8KrDazlWaWBq4GNpcssxnYFE1fBTzi7g48BLzTzJqiHcDvAM9XpvTJNWXCoO8fzs7makREThvJUy3g7lkzu4EwtAPgbnffbma3AF3uvhm4C/iumXUDvYQ7A9y9z8y+SbizcGCLu/94ln4WIDwYmwqMfg3diIgAZQQ9gLtvIRx2KW67qWh6CPjUFM/9HuEpllXTnEmqRy8iEondJ2MBmtNJjivoRUSAmAZ9i3r0IiJjYhn0zZmA/mGN0YuIQGyDXkM3IiIF8Qz6tIZuREQK4hn0GqMXERkTy6BvyQQauhERicQy6JszSfpHcoQfzhURqW+xDfpc3hnO6ipTIiKxDPqWTPiBX43Ti4jENOibx4Je59KLiMQy6As9+mPDs3rVQhGR00Isg761MQz6o4MauhERiWXQz29MAXBkUD16EZGYB/1IjSsREam9mAe9evQiIrEM+pZMkiBhCnoREWIa9GZGa0NSQS8iQkyDHmBBU5ojOutGRCS+Qd/amFKPXkSEGAf9/MYURwZ01o2ISLyDXj16EZE4B70OxoqIQKyDPsXRoay+k15E6l6sgz6Xd11pSkTqXmyDfkFTGoC+fg3fiEh9i23Qt7eEQX+4f7jGlYiI1FaMgz4DwKHjOsVSROpbbIO+LQr6w8fVoxeR+hbfoG8uDN2oRy8i9S22Qd+QCpiXSdJzTD16EalvZQW9ma03sx1m1m1mN04yP2NmD0TznzSzzpL5Z5nZcTP70wrVXZa2lrR69CJS904Z9GYWAHcAG4A1wDVmtqZksWuBPndfBdwG3Foy/5vAT2Ze7vS0t2Q4pB69iNS5cnr064Bud9/p7iPA/cDGkmU2AvdE0w8Cl5mZAZjZx4FdwPaKVDwNYY9eQS8i9a2coF8G7Cl6vDdqm3QZd88CR4A2M2sB/gz4LydbgZldZ2ZdZtbV09NTbu2n1NaS4bBOrxSROjfbB2NvBm5z9+MnW8jd73T3te6+tqOjo2Irb2/J0DswQjaXr9hrioicbpJlLLMPWFH0eHnUNtkye80sCcwHDgPvA64ys78EFgB5Mxty92/NtPByLGltwB0OHBtm2YLGaqxSRGTOKSfotwKrzWwlYaBfDXymZJnNwCbgceAq4BEPvzbyA4UFzOxm4Hi1Qh5g6YIGAPa/OaigF5G6dcqgd/esmd0APAQEwN3uvt3MbgG63H0zcBfwXTPrBnoJdwY1d+b8MNz3HxmqcSUiIrVTTo8ed98CbClpu6loegj41Cle4+a3UN+MjPXojwxWe9UiInNGbD8ZC9DakKIlk+T1N9WjF5H6FeugB1g6v0E9ehGpa/EP+gWNGqMXkboW+6A/c36Dhm5EpK7FPuiXzm/k0PFhhrO5WpciIlITsQ/65QvDUyz39WmcXkTqU+yDvrO9CYDdh/trXImISG3EP+jbmgHYdWigxpWIiNRG7IN+UXOaeQ1Jdh9Sj15E6lPsg97M6Gxr1tCNiNSt2Ac9QGe7gl5E6lddBP3Ktib29Q0yktX30otI/amLoO9sbybv8Kp69SJSh+oi6M9bMg+AF984VuNKRESqry6CftUZLQQJY4eCXkTqUF0EfSYZcHZ7My++cbTWpYiIVF1dBD3A25e2auhGROpS/QT9knns7Rvk2NBorUsREamqugn6dywND8i+sF+9ehGpL3UT9O9avgCAX7/WV9tCRESqrG6Cvq0lw9vamvj1a2/WuhQRkaqqm6AHeM+KBWx7rQ93r3UpIiJVU19Bf9ZCDh4b1jVkRaSu1FnQLwBgm8bpRaSO1FXQv2NpK83pgMdfOVzrUkREqqaugj4VJLj47Db+X/ehWpciIlI1dRX0AJesbufVwwPs6dWlBUWkPtRf0K9qB1CvXkTqRt0F/aozWljS2sDPdxysdSkiIlVRd0FvZly+ZjGPvtTDwEi21uWIiMy6ugt6gA3vXMLQaJ5Hd/TUuhQRkVlXVtCb2Xoz22Fm3WZ24yTzM2b2QDT/STPrjNovN7OnzOzZ6P5DFa7/LVnXuYhFzWl+8twbtS5FRGTWnTLozSwA7gA2AGuAa8xsTcli1wJ97r4KuA24NWo/BHzM3d8JbAK+W6nCZyIZJPjw+Yv55xcOMDiSq3U5IiKzqpwe/Tqg2913uvsIcD+wsWSZjcA90fSDwGVmZu7+a3d/PWrfDjSaWaYShc/Ux9+9jP6RHFue3V/rUkREZlU5Qb8M2FP0eG/UNuky7p4FjgBtJct8Etjm7sOlKzCz68ysy8y6enqqM26+buUizm5v5v6tr1VlfSIitVKVg7Fmdj7hcM4fTzbf3e9097Xuvrajo6MaJWFmfPq9K9i6u4/ug7oYiYjEVzlBvw9YUfR4edQ26TJmlgTmA4ejx8uBHwKfc/dXZlpwJX3iwuWkgwTfeWx3rUsREZk15QT9VmC1ma00szRwNbC5ZJnNhAdbAa4CHnF3N7MFwI+BG939sQrVXDEd8zJ88qJl/P1Te+k5NmFESUQkFk4Z9NGY+w3AQ8ALwA/cfbuZ3WJmV0aL3QW0mVk38CWgcArmDcAq4CYzezq6nVHxn2IGrvvgOYzm8nznsV21LkVEZFbYXLva0tq1a72rq6uq67z++9t49KUefv4fLqW9ZU6cFCQiMi1m9pS7r51sXl1+MrbUv7/8XAZHc/zVz16udSkiIhWnoCf8orPPvu8s7vvVazoDR0RiR0Ef+eJlq2lOB/zHf3yOfH5uDWeJiMyEgj7S1pLhP//eGn61u5d7H99d63JERCpGQV/kqouWc+l5Hdz6Tzs0hCMisaGgL2Jm/MUnfoumdMAXvreN48P6vnoROf0p6Essmd/A/7jmPezsOc5XHvyNxutF5LSnoJ/Ev1rVzo0b3s6WZ9/g6z9+nrn2WQMRkelI1rqAuerzHzibN44Mc/dju1jQmOaLv7u61iWJiLwlCvopmBn/6aPv4M3BEW772UsMjub4s/XnYWa1Lk1EZFoU9CeRSBjfuOpdNKYCvv3oK/T2D/P1j19AJhnUujQRkbIp6E8hkTD+68cvoK05ze2PdPPyweN8+w8vYnFrQ61LExEpiw7GlsHM+NIV5/HXn72QHW8c46O3/5KHnz9Q67JERMqioJ+GDe9cyo+ufz8d8xr4/L1dfPkHv+HIwGityxIROSkF/TStXjyPH13/fv7th1bxv5/ex+9841+49/HdZHP5WpcmIjIpBf1bkE4m+PIV5/F/briEty+Zx00/2s5Hbv8lP35mPzl9wEpE5hgF/QysObOVv/v8xXz7Dy8im3euv28bV9z2KP/w1F6Gs7lalyciAugKUxWTyzs/eW4/33qkmxffOEZbc5o/eO8KPrPuLFYsaqp1eSIScye7wpSCvsLcnV+8fIjvP/EqP3vhAA5cvLKNj73rTNZfsIRFzelalygiMaSgr5H9RwZ5YOseNj/9OjsP9RMkjPevaufyNYu59NwO9fRFpGIU9DXm7mx//Sj/95n9bHl2P6/1DgBwdkczl557BpesbuOity1ifmOqxpWKyOlKQT+HuDs7D/Xz8x09/HzHQZ7c1ctINo8ZnLd4Hms7F/LezkW8e8UCzlrUpO/WEZGyKOjnsMGRHE/veZOu3b1sfbWPba/2jV3wZF4myTvObOWCM+dz/pmtnL+slZXtzfquHRGZ4GRBr++6qbHGdMBvn9PGb5/TBkA2l+fFN47x3L4jbH/9KM+9foT7fvUqQ6PhB7ISBmctauKcjhZWndHCOR0tnHNGM51tzSxqTus/ABGZQEE/xySDBBcsm88Fy+aPtWVzeXYd6uf5/Ud55eBxunuO88rBfn758iFGij6R25QOWL6wkRULm8L7ReH98oVNLJnfwKKmNImEdgQi9UZBfxpIBglWL57H6sXzxrXn8s6e3gG6Dx7ntd4B9vQNsLdvkD29Azy5q3fCNW+TCeOMeRnOaG1gcWuGxa0NLG5tGGtra06zKLo1pDQ8JBIXCvrTWJAwOtub6WxvnjDP3TkyODoW/AeODnHw2DAHjg5z8NgQuw7188TOXo4MTv6lbE3pYCz0FzalaWtOs7DocWtjktaGFK2NKVobkrQ2ppjXkNTxA5E5SEEfU2bGgqY0C5rS44aBSg2N5jh4dJgDx4bo7R8Zu/UVpgfC+1d6jtPbP8LAyMm/2qEhlWBew4nwL+wM5jUkaU4HNKWTNGfG37dkkjSlA5oL9+kkTZmAdJDQMQeRClDQ17mGVMBZbU2c1Vbeh7eGRnP0DYxwbCjL0cHR8H5olKODoxyN2sLHYfubAyO81jvAsaFRBkZyp9xRFEsmbGwH0JgKyKQCGlIJGlMBDdF0Q/JEe0MqoCFZND12H92SJx6nkwlSgZFOJsgEJx4nA339k8SPgl6mpSEVsHR+I0un/ifhpPJ5Z3A0R/9wlv6R8H5gJEf/SJaB4cL9+HnHh7MMjuYYHs0xNJpnaDTH0aHRsemh0Xw4L5tjNDez04UTRhT6CTLJBOkgQSq6L7Snk0XzosfjnpNMkEyEO41UdB8+PtEWJIxUkAjbEifmp4qXTSTG2oKEkYoej5tXaEuY/vuRKSnopaoSCaM5k6Q5Mzu/etlcnqFsYQdwYscwnM2N2zGM5HKMZp3hXJ6RbHgbLUyX3I8WLVNoOz6cHd+ezTOSc0ayOUZyebI5J1vlr6wOEmHgp4IECQsfB4kEQQICM4LACMxIRMslzMaek0iE84JEya14+ejxuOWDic9LlCyTDArrIqwnqi0RLZuwcKgxXBckzE48juYljLF6LZpOFC1feJ3i1wxfh7Gaxq0nmleosXQ94etMvZ7Tbada1l+bma0H/goIgP/p7n9RMj8D3AtcBBwGPu3uu6N5XwWuBXLAv3P3hypWvUiJZJCgJUjQMks7kulwD8M+DP0w/Eej+1zeGc3lyUb34WMnG7WFz8uHbfnx80fzTm7sucXPidaRc/IeriPnTi4X3uej1y2ezheWyZ+4jWTz45fPh683bvlc4XmQi+rLO2TzefJ5xl4zrk7sbE7sIEp3IAkzjPE7ECu6L95hmYEBH3r7GXzto2sqXu8p/xrMLADuAC4H9gJbzWyzuz9ftNi1QJ+7rzKzq4FbgU+b2RrgauB84EzgZ2Z2rrvry9ol9syMVGCEZ6rW39lI7mH4j+1ESnYohfmFnZJH0ydu0eM8E9o82skU2j1aT2E6X7Tu4vWM3SZ5zbwTve6J6XzRenzK1yzUOb6+senCtsifeFyoc9xyDkvmN87Ke1FOt2cd0O3uOwHM7H5gI1Ac9BuBm6PpB4FvWfi/zUbgfncfBnaZWXf0eo9XpnwRmavCYZJw6ERqq5xTDJYBe4oe743aJl3G3bPAEaCtzOdiZteZWZeZdfX09JRfvYiInNKcOJfM3e9097Xuvrajo6PW5YiIxEo5Qb8PWFH0eHnUNukyZpYE5hMelC3nuSIiMovKCfqtwGozW2lmacKDq5tLltkMbIqmrwIe8fD7jzcDV5tZxsxWAquBX1WmdBERKccpD8a6e9bMbgAeIjx14G53325mtwBd7r4ZuAv4bnSwtZdwZ0C03A8ID9xmget1xo2ISHXpwiMiIjFwsguPzImDsSIiMnsU9CIiMTfnhm7MrAd4dQYv0Q4cqlA5laS6pkd1TY/qmp441vU2d5/0/PQ5F/QzZWZdU41T1ZLqmh7VNT2qa3rqrS4N3YiIxJyCXkQk5uIY9HfWuoApqK7pUV3To7qmp67qit0YvYiIjBfHHr2IiBRR0IuIxFxsgt7M1pvZDjPrNrMbq7zuFWb2L2b2vJltN7MvRu03m9k+M3s6un2k6DlfjWrdYWYfnsXadpvZs9H6u6K2RWb2sJm9HN0vjNrNzG6P6nrGzC6cpZrOK9omT5vZUTP7k1psLzO728wOmtlzRW3T3j5mtila/mUz2zTZuipQ138zsxejdf/QzBZE7Z1mNli03b5d9JyLove/O6p9RlcBmaKuab9vlf57naKuB4pq2m1mT0ft1dxeU2VDdX/HPLrs1el8I/yytVeAs4E08BtgTRXXvxS4MJqeB7wErCG86tafTrL8mqjGDLAyqj2Ypdp2A+0lbX8J3BhN3wjcGk1/BPgJ4eUrLwaerNJ79wbwtlpsL+CDwIXAc291+wCLgJ3R/cJoeuEs1HUFkIymby2qq7N4uZLX+VVUq0W1b5iFuqb1vs3G3+tkdZXM/+/ATTXYXlNlQ1V/x+LSox+73KG7jwCFyx1Whbvvd/dt0fQx4AUmuZJWkbFLLLr7LqBwicVq2QjcE03fA3y8qP1eDz0BLDCzpbNcy2XAK+5+sk9Dz9r2cvdfEH7jaun6prN9Pgw87O697t4HPAysr3Rd7v5TD6/gBvAE4fUdphTV1uruT3iYFvcW/SwVq+skpnrfKv73erK6ol75HwB/d7LXmKXtNVU2VPV3LC5BX9YlC6vBzDqB9wBPRk03RP+C3V3494zq1uvAT83sKTO7Lmpb7O77o+k3gMU1qKvgasb/AdZ6e8H0t08tttu/Iez5Faw0s1+b2aNm9oGobVlUSzXqms77Vu3t9QHggLu/XNRW9e1Vkg1V/R2LS9DPCWbWAvwD8CfufhT4a+Ac4N3AfsJ/H6vtEne/ENgAXG9mHyyeGfVcanKOrYUXsrkS+PuoaS5sr3FquX2mYmZfI7y+w/ejpv3AWe7+HuBLwH1m1lrFkubc+1biGsZ3Jqq+vSbJhjHV+B2LS9DX/JKFZpYifCO/7+7/CODuB9w95+554G85MdxQtXrdfV90fxD4YVTDgcKQTHR/sNp1RTYA29z9QFRjzbdXZLrbp2r1mdkfAb8HfDYKCKKhkcPR9FOE49/nRjUUD+/MSl1v4X2r5vZKAp8AHiiqt6rba7JsoMq/Y3EJ+nIudzhrojHAu4AX3P2bRe3F49u/DxTOCKjKJRbNrNnM5hWmCQ/mPcf4Sz9uAn5UVNfnoiP/FwNHiv69nA3jelq13l5Fprt9HgKuMLOF0bDFFVFbRZnZeuArwJXuPlDU3mFmQTR9NuH22RnVdtTMLo5+Rz9X9LNUsq7pvm/V/Hv9XeBFdx8bkqnm9poqG6j279hMjijPpRvh0eqXCPfOX6vyui8h/NfrGeDp6PYR4LvAs1H7ZmBp0XO+FtW6gxke2T9JXWcTntHwG2B7YbsAbcA/Ay8DPwMWRe0G3BHV9Sywdha3WTPhBeTnF7VVfXsR7mj2A6OE457XvpXtQzhm3h3d/vUs1dVNOE5b+B37drTsJ6P392lgG/CxotdZSxi8rwDfIvo0fIXrmvb7Vum/18nqitr/F/CFkmWrub2myoaq/o7pKxBERGIuLkM3IiIyBQW9iEjMKehFRGJOQS8iEnMKehGRmFPQi4jEnIJeRCTm/j/GMDuIOUsz1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "T=np.linspace(0,M_epoch,M_epoch)\n",
    "gam=.1/(T/20+1)\n",
    "plt.plot(gam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
