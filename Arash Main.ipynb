{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate\n",
    "import control as ctrl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Perf_bound(RNN):\n",
    "    A,B,C,b,c=rnn.Weights()\n",
    "    At=np.transpose(A)\n",
    "    Q=np.transpose(C)@C\n",
    "    Bt=np.transpose(B)\n",
    "    P=ctrl.dlyap(A,Q)\n",
    "    return np.trace(Bt@P@B)\n",
    "\n",
    "def vdp1(t, x):\n",
    "    return np.array([-x[1], - .3*x[1] + x[0]])\n",
    "\n",
    "res=500\n",
    "seq_len=20\n",
    "t0, t1 = 0, 20                # start and end\n",
    "t = np.linspace(t0, t1, res+seq_len)  # the points of evaluation of solution\n",
    "x0 = [2, 0]                   # initial value\n",
    "x = np.zeros((len(t), len(x0)))   # arrax for solution\n",
    "x[0, :] = x0\n",
    "r = integrate.ode(vdp1).set_integrator(\"dopri5\")  # choice of method\n",
    "r.set_initial_value(x0, t0)   # initial values\n",
    "for i in range(1, t.size):\n",
    "    x[i, :] = r.integrate(t[i])+np.random.normal(0,0.03,(2,)) # get one more value, add it to the arrax\n",
    "    if not r.successful():\n",
    "        raise RuntimeError(\"Could not integrate\")\n",
    "plt.plot(t, x)\n",
    "plt.show()\n",
    "\n",
    "def createInputs(x,res,seq_len):\n",
    "    data_list=[]\n",
    "#     target_list=[]\n",
    "    for i in range(res-1):\n",
    "        data_list.append([x[i:seq_len+i,:],x[seq_len+i+1,:]])\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Rbstdiff(RNN):\n",
    "    A,B,C,b,c=rnn.Weights()\n",
    "    At=np.transpose(A)\n",
    "    Q=np.transpose(C)@C\n",
    "    Bt=np.transpose(B)\n",
    "    P=ctrl.dlyap(A,Q)\n",
    "    I=np.identity(len(A))\n",
    "    dB=2*P@B\n",
    "    dC=-2*C@B@Bt@ np.linalg.inv(I-At@A)\n",
    "    dA=2*B@Bt@P@A @np.linalg.inv(I-At@A)\n",
    "    return dA,dB,dC\n",
    "    \n",
    "    \n",
    "\n",
    "# ================================\n",
    "\n",
    "class RNN:\n",
    "  # A many-to-one Vanilla Recurrent Neural Network.\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        # Weights\n",
    "        self.Whh = random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wxh = random.randn(hidden_size, input_size) / 1000\n",
    "        self.Why = random.randn(output_size, hidden_size) / 1000\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "    \n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = { 0: h }\n",
    "\n",
    "        # Perform each step of the RNN\n",
    "        for i, xin in enumerate(inputs):\n",
    "            xin=xin[...,np.newaxis]\n",
    "            h = np.tanh(self.Wxh @ xin + self.Whh @ h + self.bh)\n",
    "            self.last_hs[i + 1] = h\n",
    "\n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "\n",
    "    def backprop(self, d_y,epoch, learn_rate=2e-2):\n",
    "        n = len(self.last_inputs)\n",
    "\n",
    "        # Calculate dL/dWhy and dL/dby.\n",
    "        d_Why = d_y @ self.last_hs[n].T\n",
    "        d_by = d_y\n",
    "\n",
    "        # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n",
    "        d_Whh = np.zeros(self.Whh.shape)\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "\n",
    "        # Calculate dL/dh for the last h.\n",
    "        # dL/dh = dL/dy * dy/dh\n",
    "        d_h = self.Why.T @ d_y\n",
    "\n",
    "        # Backpropagate through time.\n",
    "        for t in reversed(range(n)):\n",
    "            # An intermediate value: dL/dh * (1 - h^2)\n",
    "            temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n",
    "#             print(temp.shape)\n",
    "            # dL/db = dL/dh * (1 - h^2)\n",
    "            d_bh += temp\n",
    "\n",
    "            # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
    "            d_Whh += temp @ self.last_hs[t].T\n",
    "\n",
    "            # dL/dWxh = dL/dh * (1 - h^2) * x\n",
    "            last_x=self.last_inputs[t]\n",
    "            last_x=last_x[...,np.newaxis]\n",
    "            d_Wxh += temp @ last_x.T\n",
    "\n",
    "            # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "            d_h = self.Whh @ temp\n",
    "\n",
    "        # Clip to prevent exploding gradients.\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # Update weights and biases using gradient descent.\n",
    "        # adding the regulization terms -----------------------------------------\n",
    "        dA,dB,dC=Rbstdiff(self)\n",
    "        gamma=0\n",
    "#         gamma=.1/np.log(epoch/10+2)\n",
    "        learn_rate2=learn_rate*gamma\n",
    "        self.Whh -= learn_rate * d_Whh+learn_rate2 *dA\n",
    "        self.Wxh -= learn_rate * d_Wxh+learn_rate2 *dB\n",
    "        self.Why -= learn_rate * d_Why+learn_rate2 *dC\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by\n",
    "    def Weights(self):\n",
    "        return self.Whh,self.Wxh,self.Why,self.bh,self.by\n",
    "\n",
    "\n",
    "# ================================\n",
    "    \n",
    "rnn = RNN(2, 2, hidden_size=8)\n",
    "\n",
    "\n",
    "def processData(x, backprop=True,batch_size=500,res=500,seq_len=20):\n",
    "    X_train=createInputs(x,res,seq_len)\n",
    "    loss = 0\n",
    "    d_L_d_y=np.zeros([2,1])\n",
    "\n",
    "    random.shuffle(X_train)\n",
    "    for xd in X_train[0:batch_size]:\n",
    "        inputs = xd[0]\n",
    "        target = xd[1].reshape(-1,1)\n",
    "\n",
    "        out, _ = rnn.forward(inputs)\n",
    "        diff=(out - target)\n",
    "        loss += (diff**2).mean(axis=0)\n",
    "\n",
    "        if backprop:\n",
    "            d_L_d_y += 2*(diff)\n",
    "            rnn.backprop(d_L_d_y,epoch+1)\n",
    "        return loss \n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "M_epoch=5000\n",
    "for epoch in range(M_epoch):\n",
    "    \n",
    "    train_loss = processData(x,backprop=True)\n",
    "    \n",
    "    if epoch % (M_epoch/10) == ((M_epoch/10)- 1):\n",
    "        print('--- Epoch %d' % (epoch + 1))\n",
    "        print('Train:\\tLoss  ' , (train_loss))\n",
    "        print('Train:\\t Perf Bound',Perf_bound(rnn))\n",
    "        print('Train:\\t L2 norm of A',np.linalg.norm(rnn.Whh,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prediction(x,sig,res,seq_len):\n",
    "    y_pre=np.zeros([res-1,2])\n",
    "    i=0\n",
    "    x_n=x+np.random.normal(0,sig,x.shape)\n",
    "    X=createInputs(x_n,res,seq_len)\n",
    "    for xd in X:\n",
    "        inputs = xd[0]\n",
    "        target = xd[1].reshape(-1,1)\n",
    "        out, _ = rnn.forward(inputs)\n",
    "        y_pre[i]=out.reshape(-1,2)\n",
    "        i+=1\n",
    "    return y_pre\n",
    "\n",
    "\n",
    "\n",
    "perf,perfb=0,0\n",
    "for sigma in np.arange(0,.5,1/20):\n",
    "    yp=Prediction(x,0,res,seq_len)\n",
    "    yn=Prediction(x,sigma,res,seq_len)\n",
    "    perf=np.append(perf,((yp-yn)*(yp-yn)).mean(axis=None))\n",
    "    perfb=np.append(perfb,Perf_bound(RNN)*sigma**2)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(perf)\n",
    "plt.plot(perfb)\n",
    "A,B,C,b,c=rnn.Weights()\n",
    "# mng = plt.get_current_fig_manager()\n",
    "# mng.frame.Maximize(True)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "yp=Prediction(x,0,res,seq_len)\n",
    "yn=Prediction(x,sigma,res,seq_len)\n",
    "plt.plot(x[seq_len:])\n",
    "plt.plot(yp)\n",
    "plt.plot(yn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=np.linspace(0,M_epoch,M_epoch)\n",
    "gam=.1/(np.log(T/10+2))\n",
    "plt.plot(gam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
